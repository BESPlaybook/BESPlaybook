{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"BES Playbooks These playbooks are intended to support development and modernization efforts within the BES AFLCMC/HIA Logistic Division as it transforms its methodologies to procure, develop and implement specialized IT products using current best practices.","title":"Home"},{"location":"#bes-playbooks","text":"These playbooks are intended to support development and modernization efforts within the BES AFLCMC/HIA Logistic Division as it transforms its methodologies to procure, develop and implement specialized IT products using current best practices.","title":"BES Playbooks"},{"location":"search/","text":"BES Playbooks Search No content is added from this page. See search.html template in bes-theme.","title":"BES Playbooks Search"},{"location":"search/#bes-playbooks-search","text":"No content is added from this page. See search.html template in bes-theme.","title":"BES Playbooks Search"},{"location":"acquistions/1-1-acquistions/","text":"Agile Acquistions","title":"1. Overview"},{"location":"acquistions/1-1-acquistions/#agile-acquistions","text":"","title":"Agile Acquistions"},{"location":"agile/1-1-purpose/","text":"1.1 Purpose Agile Playbook Benefits Consistent use of BEST practices and templates Establishes AF Agile Body of Knowledge - continually improved by AF community Provides \"Quick Start\" for new programs adopting agile working within AF Provides non-technical professionals with an understanding of how to use agile Builds confidence and effective communications across Government-Industry team Identifies and reduces or eliminates barriers Transforms culture from one focused on traditional processes to one focused on timely customer capability delivery.","title":"1.1 Purpose"},{"location":"agile/1-1-purpose/#11-purpose","text":"","title":"1.1 Purpose"},{"location":"agile/1-1-purpose/#agile-playbook-benefits","text":"Consistent use of BEST practices and templates Establishes AF Agile Body of Knowledge - continually improved by AF community Provides \"Quick Start\" for new programs adopting agile working within AF Provides non-technical professionals with an understanding of how to use agile Builds confidence and effective communications across Government-Industry team Identifies and reduces or eliminates barriers Transforms culture from one focused on traditional processes to one focused on timely customer capability delivery.","title":"Agile Playbook Benefits"},{"location":"agile/1-2-audience/","text":"1.2 Audience While this Agile Playbook provides value to all personnel involved in a new agile project startup, the primary audience for this document is government project management teams determining how to enable an agile development environment. From that perspective, the Agile Playbook seeks to enable the government team for a startup or in-progress project to gain a better understanding of the steps necessary to proactively establish the regulatory and process guidelines that are conducive to an agile approach. To do this, the agile playbook will focus primarily on providing a better understanding of the following areas and how they enable the success of an agile project: Contracts Resources Communications","title":"1.2 Audience"},{"location":"agile/1-2-audience/#12-audience","text":"While this Agile Playbook provides value to all personnel involved in a new agile project startup, the primary audience for this document is government project management teams determining how to enable an agile development environment. From that perspective, the Agile Playbook seeks to enable the government team for a startup or in-progress project to gain a better understanding of the steps necessary to proactively establish the regulatory and process guidelines that are conducive to an agile approach. To do this, the agile playbook will focus primarily on providing a better understanding of the following areas and how they enable the success of an agile project: Contracts Resources Communications","title":"1.2 Audience"},{"location":"agile/1-3-references/","text":"1.1 References DoDI 5000.75, Business Systems Requirements and Acquisition, Feb 2, 2017 US Air Force Enterprise Logistics Flight Plan v2.0 (ELFP), April 2016 Enterprise Logistics Technology Annex v1.0 (ELFP), June 2016 AFGM2018-63-146-01 Rapid Acquisition Activities 13 June 2018 AFMAN 63-144 Business Capability Requirements, Compliance, and System Acquisition 25 July 2018 AFPAM 63-123 Product Support Business Case Analysis 1 July 2017","title":"1.3 References"},{"location":"agile/1-3-references/#11-references","text":"DoDI 5000.75, Business Systems Requirements and Acquisition, Feb 2, 2017 US Air Force Enterprise Logistics Flight Plan v2.0 (ELFP), April 2016 Enterprise Logistics Technology Annex v1.0 (ELFP), June 2016 AFGM2018-63-146-01 Rapid Acquisition Activities 13 June 2018 AFMAN 63-144 Business Capability Requirements, Compliance, and System Acquisition 25 July 2018 AFPAM 63-123 Product Support Business Case Analysis 1 July 2017","title":"1.1 References"},{"location":"agile/10-1-appendix-e/","text":"10.1 GLOSSARY","title":"10.1 Glossary"},{"location":"agile/10-1-appendix-e/#101-glossary","text":"","title":"10.1 GLOSSARY"},{"location":"agile/2-1-waterfall/","text":"2.1 Waterfall background (where is the reader coming from) It is challenging to use agile methods in the DoD and AF because the workforce has been trained in and has practiced the traditional waterfall methodology for 50+ years. In many ways, the waterfall methodology was implemented as a way to carefully control risk along the development path and accommodate the often stove piped organizations that supported development efforts. This means that the processes, procedures, acquisition rules, contracting approaches, testing requirements, and so on are all based on the traditional waterfall method.","title":"2.1 Waterfall Background"},{"location":"agile/2-1-waterfall/#21-waterfall-background-where-is-the-reader-coming-from","text":"It is challenging to use agile methods in the DoD and AF because the workforce has been trained in and has practiced the traditional waterfall methodology for 50+ years. In many ways, the waterfall methodology was implemented as a way to carefully control risk along the development path and accommodate the often stove piped organizations that supported development efforts. This means that the processes, procedures, acquisition rules, contracting approaches, testing requirements, and so on are all based on the traditional waterfall method.","title":"2.1 Waterfall background (where is the reader coming from)"},{"location":"agile/2-2-introduction/","text":"2.2 Agile Introduction 2.2.1 4 Agile Values The Agile Manifesto describes the overarching beliefs of Agile software development as follows (http://agilemanifesto.org): We are uncovering better ways of developing software by doing it and helping others do it. Through this work we have come to value: That is, while there is value in the items on the right, we value the items on the left more. 2.2.2 Benefits of Agile over Traditional Methods Higher quality product (incremental development, continuous integration and automated testing tools allow developers to fix issues quicker when they are fresh in their mind and have fewer secondary effects on code built on top of a bug). Ability to change dynamically to customer/user wants, needs, and/or requirements (value adaptation based on increased transparency, formal feedback events, and high degree of customer collaboration). Ability to balance workloads based on cross-functional teams (while team members often have areas of expertise, agile emphasizes cross-functionality which allows flexibility to surge resources as necessary). Decreases and eventually eliminates the \"throw over the wall\" approach, thereby allowing development, operations, and security to work together iteratively to swarm on a particular issue or bug, should one come up, during releases to lower environments (this decreases the risk of failures in production if all environments are aligned and mirrored-appropriately). Shorter implementation time to usable product (provides quicker return on investment, decreased risk of project failure, faster end-user feedback into the development cycle and increased customer satisfaction). Ability to balance technical debt and new functionality, thereby decreasing technical debt over time. 2.2.3 High-Level Differences 2.2.4 Friction Points with Waterfall Lack of Ownership - The traditional regulatory environment is based on establishment of fixed requirements, letting the external development team develop the product and then inspecting and validating the product to see if it meets the requirements (tennis match of throwing things back and forth between customers and developers with the assumption that both parties understand the requirements in the same way). -- Recommended Mitigation : The key to any agile methodology is collaboration. A dedicated PO (PO) (can be a government employee or contractor on the business level or on the technical team) with decision making authority or immediate reach-back for those decisions is necessary to support the velocity of agile methodologies in being able to provide value in an ever-changing environment. Lack of Collaboration - Traditional design reviews (Preliminary Design Review / Critical Design Review) currently focus on developers presenting their design results at a fixed point of time after exhaustive analysis. -- Recommended Mitigation : What is needed is mini-collaborative design reviews that are done in such a way that integration elements are identified early enough while allowing additional design elements to be refined as close to the work being done as necessary (based on changes happening constantly - re-work will be generated on designs which are done too early and no longer apply to the current situation). Traditional CDRL formats - The traditional government waterfall process intentionally incurs oversight costs as a risk mitigation strategy to ensure that progress was made by contracted agents. In terms of documentations, the government includes in their Contract Deliverable Requirements List (CDRL) a list of documents which are formatted and generated based on a waterfall development framework. The reviewers of these documents are familiar with what the old contents were, and when reviewing delivered documents expect the same waterfall content which are focused on large immovable designs versus less-detailed more agile enabling designs. -- Recommended Mitigation : Contracts need to be modified up front so that only the necessary CDRLs that provide actual value are included and document deliveries are based on an agile timeline (smaller iterative updates versus large updates for significant milestones). (NOTE: While adequate documentation is necessary - the definition of \"adequate\" needs to be modified to provide timely value as determined by the document owners). See recommended CDRL considerations attachment in Section 3.5. Team and team member performance - Agile is based on team dynamics which take time to coalesce (normally that means there is a start-up period where less physical value is produced in the short-term while establishing the foundation to more rapidly produce value in the long-term). This means that by the straight-line value metrics of traditional project management, the project will appear behind at first. The normal strategy here is to provide more \"waterfall\" elements into the equation to try and \"catch-up\" the project which actually impedes / restricts the capabilities of the team to add value. Then in the end if the blame is placed on the agile system versus on the waterfall regulatory restrictions that were placed on it in order to \"catch it up\". -- Recommended Mitigation : Establish trust and transparency (see Communication Management section) as the team learns together how to implement an agile-based process which is understood and accepted by the government and development team. Include more formalized process to \"ramp up\" new government and contractor team members and their understanding of the system and environments. Uncontrolled change/scope creep versus managed change to provide increased value. Waterfall methodologies focus on a formal change management process in order to manage scope creep. Agile is inherently flexible in nature - it welcomes change based on the need to provide value to an ever-changing reality versus provide the value determined during a snapshot in time planning event which could have taken place months or years previously. However, the issue with agile is this inherent flexibility leads to general requirements which allow for the customer to revise in an ad hoc manner which facilitates uncontrolled scope creep (which is okay unless there is a hard deadline in providing the product based on the initial general requirements). -- Recommended Mitigation : Establish a flexible change management system (this will be based on the amount of trust established by the team). It should be flexible enough to rapidly adapt to the changing reality of what the definition of value is within existing requirements while implementing formal steps to manage scope creep (an example is adding a new requirement / feature versus revising an existing one). Earned value reporting - EV is an attempt to monitor the progress of a project by linking contract to cost to contents - these fixed linkages established at the beginning of the contract are normally difficult to change as the work adapts to reality (I.e. we already reported these features and their value to our superiors and it is too hard to revise - so we would rather use the logic we reported instead of revising that logic to fit reality). This often causes a disconnect between the development team and the contracting team as the metrics based on past logic which EV is measuring does not reflect the evolved reality existing in the agile development environment. -- Recommended Mitigation : Link earned value costs to features (can be based on high-level estimates done during feature driven planning. Conduct final feature refinement (work / story breakdown) as close to the actual development effort as possible (as an example for Scrum - conduct feature refinement 2 sprints out to better reflect reality. Base the EV metrics on the percentage of the features decomposed work items / user stories completed. External testing requirements - external test organizations (including those involved with Development Testing and Evaluation (DT&E), Quality Testing and Evaluation (QT&E), etc) often require advanced information for their test events (examples include the release contents, developers test plan, test cases, conditions, etc. Sometimes up to 270 days prior to a test event). Also, these test events are scheduled at fixed milestones versus being iterative in nature. These two items constrain the flexibility of agile to both react to evolving requirements as well as receive timely feedback from testers to incorporate in current development efforts (waiting until the contents of 6 sprints are released before receiving any feedback on their results - see Extreme Programming (XP) Test Driven Development section for possible practices). -- Recommended Mitigation : External testers need to be integrated into the government / development team to derive the evolution of the requirements in preparation for their external testing. An additional step is to establish an iterative external testing cycle which coincides with the agile framework battle rhythm (rather than conduct annual large testing events). Pure agile theorists - unable to allow for regulatory requirements because they are \"not agile\". One of the final and most difficult problems is not the transition from waterfall to agile of the government, but the inability of agile implementers to allow for the regulatory requirements mandated by the government. Just as a waterfall purist will blame agile for all difficulties encountered, the agile purist will blame all issues on the waterfall regulations imposed. -- Recommended Mitigation : What is necessary is to find the correct hybrid framework between the two extremes that allow the process to work in the most efficient manner possible and also allows buy-in from all team members (the transition from a strictly waterfall to an appropriate hybrid framework may also be iterative in nature).","title":"2.2 Introduction"},{"location":"agile/2-2-introduction/#22-agile-introduction","text":"","title":"2.2 Agile Introduction"},{"location":"agile/2-2-introduction/#221-4-agile-values","text":"The Agile Manifesto describes the overarching beliefs of Agile software development as follows (http://agilemanifesto.org): We are uncovering better ways of developing software by doing it and helping others do it. Through this work we have come to value: That is, while there is value in the items on the right, we value the items on the left more.","title":"2.2.1 4 Agile Values"},{"location":"agile/2-2-introduction/#222-benefits-of-agile-over-traditional-methods","text":"Higher quality product (incremental development, continuous integration and automated testing tools allow developers to fix issues quicker when they are fresh in their mind and have fewer secondary effects on code built on top of a bug). Ability to change dynamically to customer/user wants, needs, and/or requirements (value adaptation based on increased transparency, formal feedback events, and high degree of customer collaboration). Ability to balance workloads based on cross-functional teams (while team members often have areas of expertise, agile emphasizes cross-functionality which allows flexibility to surge resources as necessary). Decreases and eventually eliminates the \"throw over the wall\" approach, thereby allowing development, operations, and security to work together iteratively to swarm on a particular issue or bug, should one come up, during releases to lower environments (this decreases the risk of failures in production if all environments are aligned and mirrored-appropriately). Shorter implementation time to usable product (provides quicker return on investment, decreased risk of project failure, faster end-user feedback into the development cycle and increased customer satisfaction). Ability to balance technical debt and new functionality, thereby decreasing technical debt over time.","title":"2.2.2 Benefits of Agile over Traditional Methods"},{"location":"agile/2-2-introduction/#223-high-level-differences","text":"","title":"2.2.3 High-Level Differences"},{"location":"agile/2-2-introduction/#224-friction-points-with-waterfall","text":"Lack of Ownership - The traditional regulatory environment is based on establishment of fixed requirements, letting the external development team develop the product and then inspecting and validating the product to see if it meets the requirements (tennis match of throwing things back and forth between customers and developers with the assumption that both parties understand the requirements in the same way). -- Recommended Mitigation : The key to any agile methodology is collaboration. A dedicated PO (PO) (can be a government employee or contractor on the business level or on the technical team) with decision making authority or immediate reach-back for those decisions is necessary to support the velocity of agile methodologies in being able to provide value in an ever-changing environment. Lack of Collaboration - Traditional design reviews (Preliminary Design Review / Critical Design Review) currently focus on developers presenting their design results at a fixed point of time after exhaustive analysis. -- Recommended Mitigation : What is needed is mini-collaborative design reviews that are done in such a way that integration elements are identified early enough while allowing additional design elements to be refined as close to the work being done as necessary (based on changes happening constantly - re-work will be generated on designs which are done too early and no longer apply to the current situation). Traditional CDRL formats - The traditional government waterfall process intentionally incurs oversight costs as a risk mitigation strategy to ensure that progress was made by contracted agents. In terms of documentations, the government includes in their Contract Deliverable Requirements List (CDRL) a list of documents which are formatted and generated based on a waterfall development framework. The reviewers of these documents are familiar with what the old contents were, and when reviewing delivered documents expect the same waterfall content which are focused on large immovable designs versus less-detailed more agile enabling designs. -- Recommended Mitigation : Contracts need to be modified up front so that only the necessary CDRLs that provide actual value are included and document deliveries are based on an agile timeline (smaller iterative updates versus large updates for significant milestones). (NOTE: While adequate documentation is necessary - the definition of \"adequate\" needs to be modified to provide timely value as determined by the document owners). See recommended CDRL considerations attachment in Section 3.5. Team and team member performance - Agile is based on team dynamics which take time to coalesce (normally that means there is a start-up period where less physical value is produced in the short-term while establishing the foundation to more rapidly produce value in the long-term). This means that by the straight-line value metrics of traditional project management, the project will appear behind at first. The normal strategy here is to provide more \"waterfall\" elements into the equation to try and \"catch-up\" the project which actually impedes / restricts the capabilities of the team to add value. Then in the end if the blame is placed on the agile system versus on the waterfall regulatory restrictions that were placed on it in order to \"catch it up\". -- Recommended Mitigation : Establish trust and transparency (see Communication Management section) as the team learns together how to implement an agile-based process which is understood and accepted by the government and development team. Include more formalized process to \"ramp up\" new government and contractor team members and their understanding of the system and environments. Uncontrolled change/scope creep versus managed change to provide increased value. Waterfall methodologies focus on a formal change management process in order to manage scope creep. Agile is inherently flexible in nature - it welcomes change based on the need to provide value to an ever-changing reality versus provide the value determined during a snapshot in time planning event which could have taken place months or years previously. However, the issue with agile is this inherent flexibility leads to general requirements which allow for the customer to revise in an ad hoc manner which facilitates uncontrolled scope creep (which is okay unless there is a hard deadline in providing the product based on the initial general requirements). -- Recommended Mitigation : Establish a flexible change management system (this will be based on the amount of trust established by the team). It should be flexible enough to rapidly adapt to the changing reality of what the definition of value is within existing requirements while implementing formal steps to manage scope creep (an example is adding a new requirement / feature versus revising an existing one). Earned value reporting - EV is an attempt to monitor the progress of a project by linking contract to cost to contents - these fixed linkages established at the beginning of the contract are normally difficult to change as the work adapts to reality (I.e. we already reported these features and their value to our superiors and it is too hard to revise - so we would rather use the logic we reported instead of revising that logic to fit reality). This often causes a disconnect between the development team and the contracting team as the metrics based on past logic which EV is measuring does not reflect the evolved reality existing in the agile development environment. -- Recommended Mitigation : Link earned value costs to features (can be based on high-level estimates done during feature driven planning. Conduct final feature refinement (work / story breakdown) as close to the actual development effort as possible (as an example for Scrum - conduct feature refinement 2 sprints out to better reflect reality. Base the EV metrics on the percentage of the features decomposed work items / user stories completed. External testing requirements - external test organizations (including those involved with Development Testing and Evaluation (DT&E), Quality Testing and Evaluation (QT&E), etc) often require advanced information for their test events (examples include the release contents, developers test plan, test cases, conditions, etc. Sometimes up to 270 days prior to a test event). Also, these test events are scheduled at fixed milestones versus being iterative in nature. These two items constrain the flexibility of agile to both react to evolving requirements as well as receive timely feedback from testers to incorporate in current development efforts (waiting until the contents of 6 sprints are released before receiving any feedback on their results - see Extreme Programming (XP) Test Driven Development section for possible practices). -- Recommended Mitigation : External testers need to be integrated into the government / development team to derive the evolution of the requirements in preparation for their external testing. An additional step is to establish an iterative external testing cycle which coincides with the agile framework battle rhythm (rather than conduct annual large testing events). Pure agile theorists - unable to allow for regulatory requirements because they are \"not agile\". One of the final and most difficult problems is not the transition from waterfall to agile of the government, but the inability of agile implementers to allow for the regulatory requirements mandated by the government. Just as a waterfall purist will blame agile for all difficulties encountered, the agile purist will blame all issues on the waterfall regulations imposed. -- Recommended Mitigation : What is necessary is to find the correct hybrid framework between the two extremes that allow the process to work in the most efficient manner possible and also allows buy-in from all team members (the transition from a strictly waterfall to an appropriate hybrid framework may also be iterative in nature).","title":"2.2.4 Friction Points with Waterfall"},{"location":"agile/2-transformation/","text":"2 Agile Transformation 2.1 Waterfall background (where is the reader coming from) It is challenging to use agile methods in the DoD and AF because the workforce has been trained in and has practiced the traditional waterfall methodology for 50+ years. In many ways, the waterfall methodology was implemented as a way to carefully control risk along the development path and accommodate the often stove piped organizations that supported development efforts. This means that the processes, procedures, acquisition rules, contracting approaches, testing requirements, and so on are all based on the traditional waterfall method.","title":"2 transformation"},{"location":"agile/2-transformation/#2-agile-transformation","text":"","title":"2 Agile Transformation"},{"location":"agile/2-transformation/#21-waterfall-background-where-is-the-reader-coming-from","text":"It is challenging to use agile methods in the DoD and AF because the workforce has been trained in and has practiced the traditional waterfall methodology for 50+ years. In many ways, the waterfall methodology was implemented as a way to carefully control risk along the development path and accommodate the often stove piped organizations that supported development efforts. This means that the processes, procedures, acquisition rules, contracting approaches, testing requirements, and so on are all based on the traditional waterfall method.","title":"2.1 Waterfall background (where is the reader coming from)"},{"location":"agile/3-0-overview/","text":"3 Applying Agile Methods and Mindset Within The Air Force At the inception of a software-based project, the detailed software requirements may be unknown or unknowable, and even if the requirements are known, they usually experience significant changes as the development progresses. To address these evolving requirements issues, agile or iterative development promotes enhanced collaboration between program managers, requirements analysts, testers, the end-user community, and, of course, the software developers. This approach develops software iteratively in short cycles (called \"sprints,\" \"spirals,\" or \"spins\"), and involves frequent testing, user feedback, rapid deliveries, and adaptation to changing requirements. While traditional Waterfall software development was approached via rigorous preplanning to fully specify requirements before building an entire computer program application, Agile software development breaks the project down to provide iterative improvements which also adapt to the evolving environment. This allows government Program Managers to incorporate changed or new requirements in accordance with user needs, thus promoting modular IT contracting. This section focuses on how to incorporate Agile practices into the Air Force acquisition organizations, contracting and the PMO organizations to pro-actively enable an Agile development framework rather than have the framework adapt to a pre-defined waterfall based contracting approach. As this section is predominantly focused on the mindset shift, it will focus on presenting considerations versus prescribing a specific methodology.","title":"3.0 Overview"},{"location":"agile/3-0-overview/#3-applying-agile-methods-and-mindset-within-the-air-force","text":"At the inception of a software-based project, the detailed software requirements may be unknown or unknowable, and even if the requirements are known, they usually experience significant changes as the development progresses. To address these evolving requirements issues, agile or iterative development promotes enhanced collaboration between program managers, requirements analysts, testers, the end-user community, and, of course, the software developers. This approach develops software iteratively in short cycles (called \"sprints,\" \"spirals,\" or \"spins\"), and involves frequent testing, user feedback, rapid deliveries, and adaptation to changing requirements. While traditional Waterfall software development was approached via rigorous preplanning to fully specify requirements before building an entire computer program application, Agile software development breaks the project down to provide iterative improvements which also adapt to the evolving environment. This allows government Program Managers to incorporate changed or new requirements in accordance with user needs, thus promoting modular IT contracting. This section focuses on how to incorporate Agile practices into the Air Force acquisition organizations, contracting and the PMO organizations to pro-actively enable an Agile development framework rather than have the framework adapt to a pre-defined waterfall based contracting approach. As this section is predominantly focused on the mindset shift, it will focus on presenting considerations versus prescribing a specific methodology.","title":"3 Applying Agile Methods and Mindset Within The Air Force"},{"location":"agile/3-1-contracting/","text":"3.1 Agile Contracting Before jumping into the Agile development, PMOs should take time to consider how Agile can benefit their program, what the issues will be, and if perhaps a hybrid approach (combination of Waterfall and Agile) is the best approach. Some of the concepts that need to be considered when embarking on the use of Agile are discussed below. The discussion assumes the government will be contracting with a firm to actually do the development. Since the contractor will be creating the Agile organization structure, it is important the government understands the contractors' Agile organization and how the government interacts within that structure. The better the understanding, the less likely there will be inadvertent roadblocks or obstacles created to impede the progress of the Agile team(s). If the government is doing the development internally, some of the actions may differ and would be accomplished by the government. The following establish some of the key variables which must be considered in the context of enabling an Agile development framework. Acquisition life cycle Team environment End-user access Training and coaching Oversight including milestone reviews, documentation, evaluation (metrics) Rewards and incentives Culture These concepts were actual issues that programs deal with during their use of Agile methods. The concepts discussed here overlap and are intertwined. In many cases, the concepts are mutually reinforcing. 3.1.1 Acquisition Life Cycle The acquisition life cycle consists of multiple phases: Materiel Solution Analysis, Technology Development, Engineering and Manufacturing Development, Production & Deployment and Operations & Support. Each of these phases presents unique challenges and opportunities. Some phases lend themselves to the use of Agile better than others. The PMO should determine how to best employ Agile in their program depending on their specific situation. The following paragraphs propose questions to ask and identify issues to consider in building an Agile program. If the PMO is doing a Request for Proposal (RFP), no matter which phase, ensure that the RFP contains language that allows the use of Agile. In many instances, the traditional RFP language makes it difficult, if not impossible, to propose an Agile-based solution. One consideration is the types of reviews and documents required. If the PMO wants to employ Agile, be prepared to allow for Agile style document development, i.e., incremental development of documents and data for reviews that result from the individual iterations and/or releases. This might not seem much different from what the traditional waterfall methods provide but consider the level of detail may be sparser using Agile in the earlier versions of the documents. Even final documents might not contain the amount of detail provided in traditional documents. The key here is not the volume, but the content. A necessary and sufficient criterion is that all important information required for operation and maintenance of the system are supplied. 3.1.2 Team Environment Due to the size and complexity of most Air force programs, multiple agile iteration teams will be needed. The number is dependent upon the program and in some instances the locations of the contractor team. The larger the number of teams, the more complicated the communications and the greater the need for more users to be involved. In an ideal situation, each agile iteration team would have access to their own dedicated Product Owner. However, that is not practical in the DoD environment so alternatives need to be employed. PMO can consider the use of Product Owner proxies, rotating personnel every x weeks (x usually is two-four weeks), or perhaps a separate - team of subject matter experts (SMEs) accessible by the agile iteration teams as needed. The structure of the overall program team-especially the contractor team-is dependent upon which Agile method is chosen. Agile Scrum, Kanban and XP are just three examples of management practices within Agile methods. Typically, the contractor determines the flavor of Agile. However, the government PMO team needs to be responsive and supportive of that method. Otherwise, using Agile will have less than optimal results. The Agile team also must exhibit behavior reflecting the approach. Seven Extreme Programming (XP) engineering practices have been observed to scale up to enterprise-level Agile development projects and will serve as a foundation for the discussion of Agile contractin The Define/Build/Test Component. Three basic workflows are combined in the component team: define, build, and test, operating cooperatively within a pre-defined period, known as a time box. The juxtaposition of these skill sets into one team tends to run counter to some conventional methods employed in DoD programs, where these players are often separated by intent. Two-Level Planning. Two-level planning is portrayed as providing both guidance of how software is to be inserted into the operational environment as well as allowing some flexibility to accommodate what is learned during development: The top level of the planning cycle is termed release level planning. This cycle of planning defines series of releases that broadly define capability to be contained in each release. This could be done at the feature set level. The second level of the planning cycle is termed iteration or flow level planning, where work is made ready for development within either a time-boxed iteration or rhythmic workflow approach. Mastering the flow / Iteration. The ability of a team to reliably execute a process flow (Kanban) or sequence of iterations (Scrum / XP) may well be the key behavior that distinguishes a team capable of exploiting Agile techniques in a large organization. If this capability is not present, the likelihood of success is minimal at best. The development iteration or workflow consists of the following key activities: creation of complete, tested, working code implementing a set of features and integration of the developed code into the working baseline. The result is potentially releasable to the user. Producing Smaller and More Frequent Releases. One goal of an agile development framework is the desire for more frequent feedback from the customer and/or stakeholders to avoid large-scale course corrections. The shorter duration of iterations or workflow lead time will help to maintain more or less continuous feedback from the customer. Concurrent Testing. Concurrent testing practices are based upon thorough testing of code both during development and during integration. The goal is that all code is tested. One primary methodology for this is the application of a Test-Driven-Development Approach where the unit tests for software are created prior to the actual code development. DevOps Continuous Integration (CI)/Continuous Delivery (CD) Pipelines. DevOps CI/CD pipelines may well be the most useful and controversial practice advocated in the Agile community. The DevOps CI/CD model diverges from the usual V-shaped model advocated by traditional systems engineering practice employed in DoD programs. In the V-shaped model, requirements synthesis, allocation, and development are carried out in a top-down fashion. This is followed by a bottom-up sequence of integration and verification activities, leading to a product ready for production. DevOps CI/CD pipeline processes are contingent upon the ability to concurrently execute two crucial activities: (1) collect incremental changes from multiple developers on a regular basis, ideally on a daily basis on code check-in, and (2) perform the nightly build discipline, where all changes are brought together in an incremental software baseline, which is in turn compiled and tested with the available automated unit, security, functional and regression test tools. Regular Reflection and Adaptation. Reflection and adaptation (called the Retrospective in Scrum) is the Agile version of continuous process improvement that is highlighted in other quality practices such as CMMI-DEV processes. In keeping with the bottom-up discipline of Agile approaches, this introspection is driven down to the individual team level. 3.1.3 Contracting Consideration Checklist Procure the repeatable process for the delivery of functional products Contractual Requirements should be the scope, period of performance, and price. The technical execution of the project should be at the discretion of the Product Owner Enhancement and fixes should be owned by the same team Contract Types: Fixed Price per iteration is good for the procurement of the process for an entire team but the current DoD acquisition process does not support short-term contract changes Time and Materials is ideal for the procurement of time of required skill sets but the risk is entirely on the government A preferred type which enables agile development is a \"Rent the Factory\" type contract: Establish contract to resource (\"rent\") a team for a specified time period from a contractor Control change through PMO management of the Product Backlog Implement within contract incremental options for extension to decrease government risk (off-ramp for lack of performance) Provide reward incentives for excellence in performance","title":"3.1 Agile Contracting"},{"location":"agile/3-1-contracting/#31-agile-contracting","text":"Before jumping into the Agile development, PMOs should take time to consider how Agile can benefit their program, what the issues will be, and if perhaps a hybrid approach (combination of Waterfall and Agile) is the best approach. Some of the concepts that need to be considered when embarking on the use of Agile are discussed below. The discussion assumes the government will be contracting with a firm to actually do the development. Since the contractor will be creating the Agile organization structure, it is important the government understands the contractors' Agile organization and how the government interacts within that structure. The better the understanding, the less likely there will be inadvertent roadblocks or obstacles created to impede the progress of the Agile team(s). If the government is doing the development internally, some of the actions may differ and would be accomplished by the government. The following establish some of the key variables which must be considered in the context of enabling an Agile development framework. Acquisition life cycle Team environment End-user access Training and coaching Oversight including milestone reviews, documentation, evaluation (metrics) Rewards and incentives Culture These concepts were actual issues that programs deal with during their use of Agile methods. The concepts discussed here overlap and are intertwined. In many cases, the concepts are mutually reinforcing.","title":"3.1 Agile Contracting"},{"location":"agile/3-1-contracting/#311-acquisition-life-cycle","text":"The acquisition life cycle consists of multiple phases: Materiel Solution Analysis, Technology Development, Engineering and Manufacturing Development, Production & Deployment and Operations & Support. Each of these phases presents unique challenges and opportunities. Some phases lend themselves to the use of Agile better than others. The PMO should determine how to best employ Agile in their program depending on their specific situation. The following paragraphs propose questions to ask and identify issues to consider in building an Agile program. If the PMO is doing a Request for Proposal (RFP), no matter which phase, ensure that the RFP contains language that allows the use of Agile. In many instances, the traditional RFP language makes it difficult, if not impossible, to propose an Agile-based solution. One consideration is the types of reviews and documents required. If the PMO wants to employ Agile, be prepared to allow for Agile style document development, i.e., incremental development of documents and data for reviews that result from the individual iterations and/or releases. This might not seem much different from what the traditional waterfall methods provide but consider the level of detail may be sparser using Agile in the earlier versions of the documents. Even final documents might not contain the amount of detail provided in traditional documents. The key here is not the volume, but the content. A necessary and sufficient criterion is that all important information required for operation and maintenance of the system are supplied.","title":"3.1.1 Acquisition Life Cycle"},{"location":"agile/3-1-contracting/#312-team-environment","text":"Due to the size and complexity of most Air force programs, multiple agile iteration teams will be needed. The number is dependent upon the program and in some instances the locations of the contractor team. The larger the number of teams, the more complicated the communications and the greater the need for more users to be involved. In an ideal situation, each agile iteration team would have access to their own dedicated Product Owner. However, that is not practical in the DoD environment so alternatives need to be employed. PMO can consider the use of Product Owner proxies, rotating personnel every x weeks (x usually is two-four weeks), or perhaps a separate - team of subject matter experts (SMEs) accessible by the agile iteration teams as needed. The structure of the overall program team-especially the contractor team-is dependent upon which Agile method is chosen. Agile Scrum, Kanban and XP are just three examples of management practices within Agile methods. Typically, the contractor determines the flavor of Agile. However, the government PMO team needs to be responsive and supportive of that method. Otherwise, using Agile will have less than optimal results. The Agile team also must exhibit behavior reflecting the approach. Seven Extreme Programming (XP) engineering practices have been observed to scale up to enterprise-level Agile development projects and will serve as a foundation for the discussion of Agile contractin The Define/Build/Test Component. Three basic workflows are combined in the component team: define, build, and test, operating cooperatively within a pre-defined period, known as a time box. The juxtaposition of these skill sets into one team tends to run counter to some conventional methods employed in DoD programs, where these players are often separated by intent. Two-Level Planning. Two-level planning is portrayed as providing both guidance of how software is to be inserted into the operational environment as well as allowing some flexibility to accommodate what is learned during development: The top level of the planning cycle is termed release level planning. This cycle of planning defines series of releases that broadly define capability to be contained in each release. This could be done at the feature set level. The second level of the planning cycle is termed iteration or flow level planning, where work is made ready for development within either a time-boxed iteration or rhythmic workflow approach. Mastering the flow / Iteration. The ability of a team to reliably execute a process flow (Kanban) or sequence of iterations (Scrum / XP) may well be the key behavior that distinguishes a team capable of exploiting Agile techniques in a large organization. If this capability is not present, the likelihood of success is minimal at best. The development iteration or workflow consists of the following key activities: creation of complete, tested, working code implementing a set of features and integration of the developed code into the working baseline. The result is potentially releasable to the user. Producing Smaller and More Frequent Releases. One goal of an agile development framework is the desire for more frequent feedback from the customer and/or stakeholders to avoid large-scale course corrections. The shorter duration of iterations or workflow lead time will help to maintain more or less continuous feedback from the customer. Concurrent Testing. Concurrent testing practices are based upon thorough testing of code both during development and during integration. The goal is that all code is tested. One primary methodology for this is the application of a Test-Driven-Development Approach where the unit tests for software are created prior to the actual code development. DevOps Continuous Integration (CI)/Continuous Delivery (CD) Pipelines. DevOps CI/CD pipelines may well be the most useful and controversial practice advocated in the Agile community. The DevOps CI/CD model diverges from the usual V-shaped model advocated by traditional systems engineering practice employed in DoD programs. In the V-shaped model, requirements synthesis, allocation, and development are carried out in a top-down fashion. This is followed by a bottom-up sequence of integration and verification activities, leading to a product ready for production. DevOps CI/CD pipeline processes are contingent upon the ability to concurrently execute two crucial activities: (1) collect incremental changes from multiple developers on a regular basis, ideally on a daily basis on code check-in, and (2) perform the nightly build discipline, where all changes are brought together in an incremental software baseline, which is in turn compiled and tested with the available automated unit, security, functional and regression test tools. Regular Reflection and Adaptation. Reflection and adaptation (called the Retrospective in Scrum) is the Agile version of continuous process improvement that is highlighted in other quality practices such as CMMI-DEV processes. In keeping with the bottom-up discipline of Agile approaches, this introspection is driven down to the individual team level.","title":"3.1.2 Team Environment"},{"location":"agile/3-1-contracting/#313-contracting-consideration-checklist","text":"Procure the repeatable process for the delivery of functional products Contractual Requirements should be the scope, period of performance, and price. The technical execution of the project should be at the discretion of the Product Owner Enhancement and fixes should be owned by the same team Contract Types: Fixed Price per iteration is good for the procurement of the process for an entire team but the current DoD acquisition process does not support short-term contract changes Time and Materials is ideal for the procurement of time of required skill sets but the risk is entirely on the government A preferred type which enables agile development is a \"Rent the Factory\" type contract: Establish contract to resource (\"rent\") a team for a specified time period from a contractor Control change through PMO management of the Product Backlog Implement within contract incremental options for extension to decrease government risk (off-ramp for lack of performance) Provide reward incentives for excellence in performance","title":"3.1.3 Contracting Consideration Checklist"},{"location":"agile/3-2-stakeholder/","text":"3.2 Agile Organization, Roles, and Responsibilities - Stakeholder Level One addition to the typical traditional Air Force PMO organization is an Agile Coach. As described in the previous training and coaching section, the Agile Coach is someone who can provide real-time answers for the immediate Agile issue. Another addition to the typical PMO staff is an end-user representative, the PO, who is empowered to work with the contractors' agile development team and make decisions that are binding for the development. Given the nature of government contracting, care must be taken to ensure that the PO user representative has the legal authority to direct the contractor. We can envision a situation where constructive change could become an issue. Another addition to the PMO is a DevSecOps Lead who works with the with contractor engineering support teams that may be divided into two segments: Continuous Integration Team and Continuous Delivery/Deployment Team to implement configuration management, version control, automated build, automated security testing, automated functional testing and regression testing. The government needs skilled Agile personnel to review the documentation and understand how the Agile software development approach works. Many traditional PMO teams do not have software representatives experienced with modern software development approaches. That could be more problematic in an Agile environment, where any shortfalls quickly become more visible. Another challenge is keeping high-performing Agile teams together long enough for them to achieve peak performance. This is a challenge because developers can change at the end of a contractual period of performance. The continuity of an Agile team enhances the tacit knowledge of the program and this improves overall performance. One recommendation might be to look at is putting key Agile technical leads into the PMO under a separate contract vehicle or hire them to work for the government PMO organization itself. 3.2.1 Stakeholder Consideration Checklist: Empower the Product Owner to make technical decisions Provide regular feedback to stakeholders demonstrating progress Maintain a short feedback loop with users Align with external organizations Testing Configuration","title":"3.2 Agile Stakeholder"},{"location":"agile/3-2-stakeholder/#32-agile-organization-roles-and-responsibilities-stakeholder-level","text":"One addition to the typical traditional Air Force PMO organization is an Agile Coach. As described in the previous training and coaching section, the Agile Coach is someone who can provide real-time answers for the immediate Agile issue. Another addition to the typical PMO staff is an end-user representative, the PO, who is empowered to work with the contractors' agile development team and make decisions that are binding for the development. Given the nature of government contracting, care must be taken to ensure that the PO user representative has the legal authority to direct the contractor. We can envision a situation where constructive change could become an issue. Another addition to the PMO is a DevSecOps Lead who works with the with contractor engineering support teams that may be divided into two segments: Continuous Integration Team and Continuous Delivery/Deployment Team to implement configuration management, version control, automated build, automated security testing, automated functional testing and regression testing. The government needs skilled Agile personnel to review the documentation and understand how the Agile software development approach works. Many traditional PMO teams do not have software representatives experienced with modern software development approaches. That could be more problematic in an Agile environment, where any shortfalls quickly become more visible. Another challenge is keeping high-performing Agile teams together long enough for them to achieve peak performance. This is a challenge because developers can change at the end of a contractual period of performance. The continuity of an Agile team enhances the tacit knowledge of the program and this improves overall performance. One recommendation might be to look at is putting key Agile technical leads into the PMO under a separate contract vehicle or hire them to work for the government PMO organization itself.","title":"3.2 Agile Organization, Roles, and Responsibilities - Stakeholder Level"},{"location":"agile/3-2-stakeholder/#321-stakeholder-consideration-checklist","text":"Empower the Product Owner to make technical decisions Provide regular feedback to stakeholders demonstrating progress Maintain a short feedback loop with users Align with external organizations Testing Configuration","title":"3.2.1 Stakeholder Consideration Checklist:"},{"location":"agile/3-3-projectmanagement/","text":"3.3 Project Management This section describes how project management practices need to be adjusted in support of Agile projects by first identifying common Agile practices and then describing how these management practices work in terms of scope, schedule and cost baselines for the project work. 3.3.1 Planning An integrated project management plan (PMP) is developed for the Agile project to define the basis of all project work and how the work will be performed. It describes how the project will be executed, monitored, control and closed. From the Agile perspective, the performance measurement baseline is an integrated scope, schedule, and cost baseline for the software release project work maintained in the Product Backlog against which project execution metrics are used to measure and manage performance. The PMP describes the series of phases (themes, initiatives, and epics) the project passes through from initiation to closure. The PMP also describes the Agile development management approach; i.e. Agile iteration-based (Scrum/XP), flow-based (Kanban) or a hybrid model. Figure 1 describes the hierarchal structure of the Agile project work effort. Notice that this view of the project work hierarchy is similar to the WBS in predictive (Waterfall) projects. One additional consideration is that the above hierarchy promotes a multi-team or \"scaled\" agile approach. While methodologies exist to support the scaling of agile (i.e. Scaled Agile Framework (SAFe), Scrum of Scrums, Disciplined Agile Delivery (DAD), etc), these will not be presented in this playbook but are a topic for further elaboration as an organization's agile process matures. 3.3.2 Scope In Agile projects the requirements are defined by the organization's stakeholders and Product Owner with support from the Agile Team in the form of Epics, Features, user stories or PBIs that are maintained in the Product Backlog. Therefore, the project scope for an Agile project begins with the organization's governance process which commonly consists of a Configuration Control Board (CCB) that produces a high-level product backlog for a release consisting of a list of approved requirements defined in the form of epics, features and sometimes high-level PBIs. The product owner then works with the Agile Inception Team to prioritize these items (epics, features, PBIs if applicable). The PMO can initially use the MoSCoW (Must Have, Should Have, Could Have, Won't Have this time) method to prioritize requirements. 3.3.3 Forecasting Schedule and Cost Once the user stories and PBIs in the product backlog are defined, prioritized and the MVP determined, Agile estimating techniques can be applied to estimate the effort for each feature, sum up the effort for all the features in a project as well as determine which features would be part of which release. The PMO and development team are then able to forecast a schedule and cost for the release project. There are several gross-level estimation techniques used by teams using agile approaches such as Scrum, Kanban, and eXtreme Programming which include T-shirt Sizes (for Features), and Affinity Mapping. T-Shirt Sizes . This estimation technique can be applied when providing a quick and rough estimation to a project feature. Here, the features are estimated in T-shirt sizes, ranging from XS to XL, which would be later converted to numbers, as per requirements. In this type of estimation, the estimators assign a size to each of the features. Points are assigned to the each of the T-Shirt Sizes using the Fibonacci-like format: 0, 1, 2, 3, 5, 8, 13, 20, 40, 100. These points are summed up and based on a rough estimate of how many feature points can get done within a time period by a normal agile team (note, the more detailed the refinement, the better the estimates). 3.3.4 Cost The cost for the release is estimated by using the team's iteration average cost and multiplying it by the number of iterations estimated to complete the backlog. For example, the following formula to determine budgeted cost can demonstrate this estimation for the above example: (Team monthly cost (example: $15,000.00 per month) multiplied by the number of months/iterations (example: 5 months) = $75,000.00 + other expenses = forecast budgeted cost. The above example is utterly simplistic and does not take into account the following factors: The Inception phase at the beginning that is required to develop the product backlog, estimate the size in story points and develop an architecture vision; Changes in scope during the Construction iterations caused by adding new features driven by urgent business needs or Cybersecurity issues; A Transition phase at the end of development for Government Acceptance Tests and Security Tests required to achieve customer acceptance and Authority to Operate (AtO) for deployment; Many other variables such as Cloud Migration and implementation of a DevSecOps reference model. 3.3.5 Project Management Consideration Checklist The project manager removes ensures funding, organizes stakeholder interactions and keeps the team from being distracted Work in increments. Buy, build, and fail small. Make proceed and pivot decisions regularly. Learn from mistakes but don't punish the people Leverage the efficiency of commercial contracting methods. If and when possible, use services and tools sold by private sector vendors. Use living roadmaps not fixed Integrated master schedules","title":"3.3 Project Management"},{"location":"agile/3-3-projectmanagement/#33-project-management","text":"This section describes how project management practices need to be adjusted in support of Agile projects by first identifying common Agile practices and then describing how these management practices work in terms of scope, schedule and cost baselines for the project work.","title":"3.3 Project Management"},{"location":"agile/3-3-projectmanagement/#331-planning","text":"An integrated project management plan (PMP) is developed for the Agile project to define the basis of all project work and how the work will be performed. It describes how the project will be executed, monitored, control and closed. From the Agile perspective, the performance measurement baseline is an integrated scope, schedule, and cost baseline for the software release project work maintained in the Product Backlog against which project execution metrics are used to measure and manage performance. The PMP describes the series of phases (themes, initiatives, and epics) the project passes through from initiation to closure. The PMP also describes the Agile development management approach; i.e. Agile iteration-based (Scrum/XP), flow-based (Kanban) or a hybrid model. Figure 1 describes the hierarchal structure of the Agile project work effort. Notice that this view of the project work hierarchy is similar to the WBS in predictive (Waterfall) projects. One additional consideration is that the above hierarchy promotes a multi-team or \"scaled\" agile approach. While methodologies exist to support the scaling of agile (i.e. Scaled Agile Framework (SAFe), Scrum of Scrums, Disciplined Agile Delivery (DAD), etc), these will not be presented in this playbook but are a topic for further elaboration as an organization's agile process matures.","title":"3.3.1 Planning"},{"location":"agile/3-3-projectmanagement/#332-scope","text":"In Agile projects the requirements are defined by the organization's stakeholders and Product Owner with support from the Agile Team in the form of Epics, Features, user stories or PBIs that are maintained in the Product Backlog. Therefore, the project scope for an Agile project begins with the organization's governance process which commonly consists of a Configuration Control Board (CCB) that produces a high-level product backlog for a release consisting of a list of approved requirements defined in the form of epics, features and sometimes high-level PBIs. The product owner then works with the Agile Inception Team to prioritize these items (epics, features, PBIs if applicable). The PMO can initially use the MoSCoW (Must Have, Should Have, Could Have, Won't Have this time) method to prioritize requirements.","title":"3.3.2 Scope"},{"location":"agile/3-3-projectmanagement/#333-forecasting-schedule-and-cost","text":"Once the user stories and PBIs in the product backlog are defined, prioritized and the MVP determined, Agile estimating techniques can be applied to estimate the effort for each feature, sum up the effort for all the features in a project as well as determine which features would be part of which release. The PMO and development team are then able to forecast a schedule and cost for the release project. There are several gross-level estimation techniques used by teams using agile approaches such as Scrum, Kanban, and eXtreme Programming which include T-shirt Sizes (for Features), and Affinity Mapping. T-Shirt Sizes . This estimation technique can be applied when providing a quick and rough estimation to a project feature. Here, the features are estimated in T-shirt sizes, ranging from XS to XL, which would be later converted to numbers, as per requirements. In this type of estimation, the estimators assign a size to each of the features. Points are assigned to the each of the T-Shirt Sizes using the Fibonacci-like format: 0, 1, 2, 3, 5, 8, 13, 20, 40, 100. These points are summed up and based on a rough estimate of how many feature points can get done within a time period by a normal agile team (note, the more detailed the refinement, the better the estimates).","title":"3.3.3 Forecasting Schedule and Cost"},{"location":"agile/3-3-projectmanagement/#334-cost","text":"The cost for the release is estimated by using the team's iteration average cost and multiplying it by the number of iterations estimated to complete the backlog. For example, the following formula to determine budgeted cost can demonstrate this estimation for the above example: (Team monthly cost (example: $15,000.00 per month) multiplied by the number of months/iterations (example: 5 months) = $75,000.00 + other expenses = forecast budgeted cost. The above example is utterly simplistic and does not take into account the following factors: The Inception phase at the beginning that is required to develop the product backlog, estimate the size in story points and develop an architecture vision; Changes in scope during the Construction iterations caused by adding new features driven by urgent business needs or Cybersecurity issues; A Transition phase at the end of development for Government Acceptance Tests and Security Tests required to achieve customer acceptance and Authority to Operate (AtO) for deployment; Many other variables such as Cloud Migration and implementation of a DevSecOps reference model.","title":"3.3.4 Cost"},{"location":"agile/3-3-projectmanagement/#335-project-management-consideration-checklist","text":"The project manager removes ensures funding, organizes stakeholder interactions and keeps the team from being distracted Work in increments. Buy, build, and fail small. Make proceed and pivot decisions regularly. Learn from mistakes but don't punish the people Leverage the efficiency of commercial contracting methods. If and when possible, use services and tools sold by private sector vendors. Use living roadmaps not fixed Integrated master schedules","title":"3.3.5 Project Management Consideration Checklist"},{"location":"agile/3-4-delivery/","text":"3.4 Agile CDRLs and Delivery 3.4.1 Overview One of the four agile manifesto values is that \"working products are valued over comprehensive documentation\". Many times this is viewed by the agile purist as a justification for not doing documentation. However, even in agile, there is a value for doing documentation. Documentation exists to support the development teams work in creating the product and supporting the product after release. Prior to looking deeper at agile content and delivery recommendations for CDRLs, here are some general considerations to keep in mind when determining the format, content, and delivery schedule for CDRLs within an agile framework - \"Just in time\" a. Document late (based on design completion) - Consolidate deliverable design documentation as late as possible (though can be iteratively updated) - better to have the stable concepts versus speculative ideas which are constantly changing as part of the agile framework and would require constant document revisions and submissions. b. Document continuously (based on iteration) - iteratively update development related documentation (i.e. user guides) in parallel with development efforts to not lose critical ideas. A key concept here is the idea of a living document, which is discussed below. \"Just sufficient\" - Sufficiency is defined by the document owner (provide the necessary useful documentation elements). Additional thoughts: a. Provide the fewest CDRLs possible with the least amount of overlap (I.e. considering combining the Interface Requirements Specification (IRS) with the Interface Design Description IDD). b. Better communication means less documentation (collaboration is key to agile - often a conversation between engineers can eliminate the need for a staffing document). c. Working software does not eliminate the need for documentation - the software delivered still needs to be improved, operated and maintained in the future - documentation's value is transferring product knowledge gained in development to users, operators and maintainers or to new development personnel when contracts change. \"NOT Just Because\" - Treat documentation as any other requirement that needs to be justified by the government document owner (since resources will need to be allocated to produce it). Documentation work efforts can then be prioritized within the product backlog based on the value it provides. 3.4.2 Agile CDRL Content Considerations Agile is built to be fast and flexible, and the contents of the CDRLs must be able to keep up with this development framework. CDRLs should not be an after-thought - they must be incorporated into the process. In other words, we don't want big CDRLs that are out of date by the time they are published. We need documents which can be frequently updated based on the ongoing iterative development efforts. Updates for CDRLs should be provided incrementally by the team when it is fresh in their mind, versus producing documentation at the end of the release when much of the valuable information has already been forgotten. In the military, this is a mindset switch. 3.4.3 Agile CDRL Delivery Consideration In order to maintain the current value of CDRLs, these documents should be flexible enough to keep up with an iterative update approach (versus long periods of time between updates). In that case, the best methodology is to establish a system to enable CDRLs as \"living documents\". This can best be enabled by re-thinking the methodology of delivery for CDRLs. By considering alternate digital delivery methodologies, CDRLs can be more quickly updated and maintain their relevance throughout the agile development process. One final note to this section is while Sharepoint or a shared drive may fulfill the CDRL requirements above, a further shift from the traditional mindset is to provide appropriate dashboards or reports within an existing system to provide the CDRL information requirements. An example of this is the Test Descriptions / Scripts. Executable test cases are normally already stored in a digital format within the test management software. 3.4.4 CDRL Recommended Modifications Attachment The following link is to an attachment which provides a more detailed list of CDRLs normally associated with a software development project. The list contains the associated DID, waterfall description, agile recommended modifications, and normal delivery timelines (i.e. are the documents delivered one time for the project, at specified design reviews, with a delivery, or on an as needed basis. CDRL List","title":"3.4 Agile CDRLs and Delivery"},{"location":"agile/3-4-delivery/#34-agile-cdrls-and-delivery","text":"","title":"3.4 Agile CDRLs and Delivery"},{"location":"agile/3-4-delivery/#341-overview","text":"One of the four agile manifesto values is that \"working products are valued over comprehensive documentation\". Many times this is viewed by the agile purist as a justification for not doing documentation. However, even in agile, there is a value for doing documentation. Documentation exists to support the development teams work in creating the product and supporting the product after release. Prior to looking deeper at agile content and delivery recommendations for CDRLs, here are some general considerations to keep in mind when determining the format, content, and delivery schedule for CDRLs within an agile framework - \"Just in time\" a. Document late (based on design completion) - Consolidate deliverable design documentation as late as possible (though can be iteratively updated) - better to have the stable concepts versus speculative ideas which are constantly changing as part of the agile framework and would require constant document revisions and submissions. b. Document continuously (based on iteration) - iteratively update development related documentation (i.e. user guides) in parallel with development efforts to not lose critical ideas. A key concept here is the idea of a living document, which is discussed below. \"Just sufficient\" - Sufficiency is defined by the document owner (provide the necessary useful documentation elements). Additional thoughts: a. Provide the fewest CDRLs possible with the least amount of overlap (I.e. considering combining the Interface Requirements Specification (IRS) with the Interface Design Description IDD). b. Better communication means less documentation (collaboration is key to agile - often a conversation between engineers can eliminate the need for a staffing document). c. Working software does not eliminate the need for documentation - the software delivered still needs to be improved, operated and maintained in the future - documentation's value is transferring product knowledge gained in development to users, operators and maintainers or to new development personnel when contracts change. \"NOT Just Because\" - Treat documentation as any other requirement that needs to be justified by the government document owner (since resources will need to be allocated to produce it). Documentation work efforts can then be prioritized within the product backlog based on the value it provides.","title":"3.4.1 Overview"},{"location":"agile/3-4-delivery/#342-agile-cdrl-content-considerations","text":"Agile is built to be fast and flexible, and the contents of the CDRLs must be able to keep up with this development framework. CDRLs should not be an after-thought - they must be incorporated into the process. In other words, we don't want big CDRLs that are out of date by the time they are published. We need documents which can be frequently updated based on the ongoing iterative development efforts. Updates for CDRLs should be provided incrementally by the team when it is fresh in their mind, versus producing documentation at the end of the release when much of the valuable information has already been forgotten. In the military, this is a mindset switch.","title":"3.4.2 Agile CDRL Content Considerations"},{"location":"agile/3-4-delivery/#343-agile-cdrl-delivery-consideration","text":"In order to maintain the current value of CDRLs, these documents should be flexible enough to keep up with an iterative update approach (versus long periods of time between updates). In that case, the best methodology is to establish a system to enable CDRLs as \"living documents\". This can best be enabled by re-thinking the methodology of delivery for CDRLs. By considering alternate digital delivery methodologies, CDRLs can be more quickly updated and maintain their relevance throughout the agile development process. One final note to this section is while Sharepoint or a shared drive may fulfill the CDRL requirements above, a further shift from the traditional mindset is to provide appropriate dashboards or reports within an existing system to provide the CDRL information requirements. An example of this is the Test Descriptions / Scripts. Executable test cases are normally already stored in a digital format within the test management software.","title":"3.4.3 Agile CDRL Delivery Consideration"},{"location":"agile/3-4-delivery/#344-cdrl-recommended-modifications-attachment","text":"The following link is to an attachment which provides a more detailed list of CDRLs normally associated with a software development project. The list contains the associated DID, waterfall description, agile recommended modifications, and normal delivery timelines (i.e. are the documents delivered one time for the project, at specified design reviews, with a delivery, or on an as needed basis. CDRL List","title":"3.4.4 CDRL Recommended Modifications Attachment"},{"location":"agile/3-5-metrics/","text":"3.5 Measuring Agile Delivery, KPIs, and Metrics - Status Reporting In Agile, the system always runs, thus Agile metrics are empirical and business value-based measurements instead of predictive measurements such as the performance measurement baseline and earned value that are used in traditional Waterfall. Agile metrics measure what the Agile Team delivers, not what the team predicts it will deliver. Project teams use this data for improved schedule and cost forecasts as well as for surfacing problems and issues that the Agile Team can diagnose and address. The metrics described below address Team Metrics, Program Metrics and Portfolio Metrics. These metrics were derived from the Project Management Institute, Inc. Agile Practice Guide, SAFe Metrics, DAD, and Atlassian web sites. 3.5.1 Team Iteration Metrics The Agile team metrics discussed below focus on the delivery of software. Whether the project team is a Scrum or Kanban team, each of these agile metrics will help the team better understand their development process, making releasing software easier. Scrum Metrics Sprint burndown. Scrum teams organize development into time-boxed sprint iterations. At the outset of the sprint, the team forecasts how many story points they can finish during a sprint. A sprint burndown report (Figure 2) then tracks the completion of work during the sprint. The x-axis represents time, and the y-axis refers to the amount of story points left to complete. The goal is to have all the forecasted work completed by the end of the sprint. A team that consistently meets its forecast is a compelling advertisement for Agile in their organization, however, it may be too good to be true if the team is inflating the numbers by declaring an item complete before it really is. In the long run cheating hampers learning and improvement. There are several anti-patterns to watch for in team performance: The team finishes early sprint after sprint because they aren't committing to enough work in the sprint backlog. The team misses their forecast sprint after sprint because they're committing to too much work. The burndown line makes steep drops rather than a more gradual burndown because the work hasn't been broken down into granular user stories or PBIs. The product owner adds PBIs or changes the scope mid-sprint. Velocity Velocity is the average amount of work a Scrum team completes during a sprint, measured in story points and we used it in the example from the prior section to forecast a release schedule. The product owner can use velocity to predict how quickly a team can work through the product backlog, since the velocity chart report tracks the forecasted and completed work over several iteration-the more iterations, the more accurate the forecast. Each team's velocity is unique. If team A has a velocity of 25 story points and team B has a velocity of 50 story points, it doesn't mean that team B has higher throughput. Because each team's story point estimation technique is unique, their sprint velocity will be as well. Organizations should resist the temptation to compare velocity across teams. Instead, Program Management should measure the level of effort and output of work based on each team's unique interpretation of story points. Kanban Metrics Team Kanban Board Flow-based Agile Teams using Kanban methods and Kanban Boards need to use different measurements like work in progress, lead time for delivery of a feature to customer, cycle time for completion of a task on the Kanban Board, and response time - the amount of time the item waits until work begins. Figure 4 shows an example of an Agile team's initial Kanban board, which captures their current workflow states: analyze, review, build, and integrate and test. After defining the initial process and Work in Process (WIP) limits and executing for a while, the Kanban team's bottlenecks should surface. If this is the case, the Kanban Team refines the workflow process step where the bottleneck occurred or reduces some WIP limits until it becomes evident that a workflow state is 'starving' or is too full. In this manner the Kanban Team continually adjusts the process workflows to optimize their flow. For example, changing WIP limits and merging, splitting, or redefining workflow states. Cumulative Flow Diagram The cumulative flow diagram is a key resource for Kanban teams, helping them ensure the flow of work across the team is consistent. With number of issues on the Y axis, time on the X axis, and colors to indicate the various workflow states, it visually points out shortages and bottlenecks and works in conjunction with Work in Process (WIP) limits. The cumulative flow diagram should look smooth(ish) from left to right. Bubbles or gaps in any one color indicate shortages and bottlenecks, so when the Agile Team sees one, they should look for ways to smooth out color bands across the chart. Anti-patterns to look for are: Blocking issues create large backups in some parts of the process and starvation in others. Unchecked backlog growth over time. This results from product owners not closing issues that are obsolete or simply too low in priority to ever be pulled in.","title":"3.5 Status Reporting"},{"location":"agile/3-5-metrics/#35-measuring-agile-delivery-kpis-and-metrics-status-reporting","text":"In Agile, the system always runs, thus Agile metrics are empirical and business value-based measurements instead of predictive measurements such as the performance measurement baseline and earned value that are used in traditional Waterfall. Agile metrics measure what the Agile Team delivers, not what the team predicts it will deliver. Project teams use this data for improved schedule and cost forecasts as well as for surfacing problems and issues that the Agile Team can diagnose and address. The metrics described below address Team Metrics, Program Metrics and Portfolio Metrics. These metrics were derived from the Project Management Institute, Inc. Agile Practice Guide, SAFe Metrics, DAD, and Atlassian web sites.","title":"3.5 Measuring Agile Delivery, KPIs, and Metrics - Status Reporting"},{"location":"agile/3-5-metrics/#351-team-iteration-metrics","text":"The Agile team metrics discussed below focus on the delivery of software. Whether the project team is a Scrum or Kanban team, each of these agile metrics will help the team better understand their development process, making releasing software easier.","title":"3.5.1 Team Iteration Metrics"},{"location":"agile/3-5-metrics/#scrum-metrics","text":"Sprint burndown. Scrum teams organize development into time-boxed sprint iterations. At the outset of the sprint, the team forecasts how many story points they can finish during a sprint. A sprint burndown report (Figure 2) then tracks the completion of work during the sprint. The x-axis represents time, and the y-axis refers to the amount of story points left to complete. The goal is to have all the forecasted work completed by the end of the sprint. A team that consistently meets its forecast is a compelling advertisement for Agile in their organization, however, it may be too good to be true if the team is inflating the numbers by declaring an item complete before it really is. In the long run cheating hampers learning and improvement. There are several anti-patterns to watch for in team performance: The team finishes early sprint after sprint because they aren't committing to enough work in the sprint backlog. The team misses their forecast sprint after sprint because they're committing to too much work. The burndown line makes steep drops rather than a more gradual burndown because the work hasn't been broken down into granular user stories or PBIs. The product owner adds PBIs or changes the scope mid-sprint. Velocity Velocity is the average amount of work a Scrum team completes during a sprint, measured in story points and we used it in the example from the prior section to forecast a release schedule. The product owner can use velocity to predict how quickly a team can work through the product backlog, since the velocity chart report tracks the forecasted and completed work over several iteration-the more iterations, the more accurate the forecast. Each team's velocity is unique. If team A has a velocity of 25 story points and team B has a velocity of 50 story points, it doesn't mean that team B has higher throughput. Because each team's story point estimation technique is unique, their sprint velocity will be as well. Organizations should resist the temptation to compare velocity across teams. Instead, Program Management should measure the level of effort and output of work based on each team's unique interpretation of story points.","title":"Scrum Metrics"},{"location":"agile/3-5-metrics/#kanban-metrics","text":"Team Kanban Board Flow-based Agile Teams using Kanban methods and Kanban Boards need to use different measurements like work in progress, lead time for delivery of a feature to customer, cycle time for completion of a task on the Kanban Board, and response time - the amount of time the item waits until work begins. Figure 4 shows an example of an Agile team's initial Kanban board, which captures their current workflow states: analyze, review, build, and integrate and test. After defining the initial process and Work in Process (WIP) limits and executing for a while, the Kanban team's bottlenecks should surface. If this is the case, the Kanban Team refines the workflow process step where the bottleneck occurred or reduces some WIP limits until it becomes evident that a workflow state is 'starving' or is too full. In this manner the Kanban Team continually adjusts the process workflows to optimize their flow. For example, changing WIP limits and merging, splitting, or redefining workflow states. Cumulative Flow Diagram The cumulative flow diagram is a key resource for Kanban teams, helping them ensure the flow of work across the team is consistent. With number of issues on the Y axis, time on the X axis, and colors to indicate the various workflow states, it visually points out shortages and bottlenecks and works in conjunction with Work in Process (WIP) limits. The cumulative flow diagram should look smooth(ish) from left to right. Bubbles or gaps in any one color indicate shortages and bottlenecks, so when the Agile Team sees one, they should look for ways to smooth out color bands across the chart. Anti-patterns to look for are: Blocking issues create large backups in some parts of the process and starvation in others. Unchecked backlog growth over time. This results from product owners not closing issues that are obsolete or simply too low in priority to ever be pulled in.","title":"Kanban Metrics"},{"location":"agile/3-mindset/","text":"","title":"3 mindset"},{"location":"agile/4-1-architecture/","text":"4.1 Agile Management Tools (AMT) 4.1.1 Purpose: While the environment is established by stakeholder management and contractual obligations, it is also necessary to establish the physical infrastructure (toolset) necessary to enable agile. With that in mind, this section will focus on the provisioning of an infrastructure that supplies 4 general functions: Backlog Management - the tools necessary to capture and refine requirements as well as allows the PO to prioritize the different work efforts to provide the most value. This is the tool that maintains a prioritized and organized listing of the work items which need to be done for the project. Work management - the tools necessary to execute the specific methodology which will be employed by the development team (Scrum, Kanban, XP, hybrid). This is the tool which provides the team the ability to collaborate on the development of their work items (Scrum Board, Kanban Board, etc). Communications Management - the tools necessary to communicate the status of the development effort to internal and external stakeholders. It includes the ability to create reports, provide metrics (does not define the metrics themselves), and implement collaborative dashboards with information which is relevant (i.e. work items complete, work items remaining, identified risks, identified issues, test status, etc). The actual capabilities of the tool which will be used will be based on the communications needs of the stakeholders involved (the evolution of a dashboard or report is often iterative in nature as communication needs are better refined). Continuous Integration - While not a necessity, a continuous integration tool makes agile run much more efficiently. The concept of iterative deliveries to the customer requires a mechanism which allows for continuous inputs by the developers to the code to provide smaller increments versus the big bang development approach of waiting till everything is done. With continuous integration, you get an iterative product of better quality based on the integrated automated testing functionality built into the tool (will cover automated testing methodologies and benefits in a companion playbook). Each section below will include the recommended capabilities required of the enabling tools (general in nature - not tied to specific methodology) One note - the infrastructure tools used by the HIA community are on the Atlassian set of agile products as well as Team Foundation Server (TFS) / Visual Studios Team Server (VSTS). The table below shows a quick overview of the differences and similarities between the two in reference to enabling an agile framework. Links describing how to enable the management systems below within the VSTS/TFS and the Atlassian Products are found in Appendix C - Key Links. 4.1.2 Backlog Management When considering which tool to use, the following functionalities should be provided: Capture and refine requirements into nested Product Backlog Items (PBIs) - Epics, Features, User Stories Organize and prioritize those requirements Provide a reporting capability to show necessary details to understand the status of those Product Backlog Items 4.1.3 Work Management (i.e. Release Roadmap/Scrum Boards/Kanban Boards) The following functionalities should be provided: Capability to establish an agile execution board: Scrum - Sprint Board Kanban - Kanban Board XP / Hybrid - Agile Board Ability to refine the work in progress (add and update the status details of the different work items - including tasks, user stories, etc.) Ability to assign responsible parties to the different work items (while agile emphasizes the establishment of responsibility when capacity is available, there needs to be a method to monitor which work items have a responsible resource and which ones are still available to be worked on) Capability to report necessary details to understand the status of work in progress (metrics analysis will be provided later in the discussion of communication tools - here we are looking at status of individual versus aggregate work items). 4.1.4 Communication Management Recommend functions include the ability to produce the following: Dashboards Metric Analysis Reports: Release Roadmaps - These will be discussed later in the methodology, but the release roadmap provides an overview of when features are expected to be complete. In terms of project management, the release roadmap can be used to establish the schedule for the project. 4.1.5 Continuous Integration Architecture and Management Recommended functionality for enabling continuous integration in any toolset: Version control tool (code repository management) Instrumented or scripted build process Trigger capability for implementing a build and test cycle based on code check-in Automated testing implementation capability Automated alerts, in case of a failed test, back to the developers so that they can resolve issue immediately (provide feedback on issues)","title":"4.1 Agile Management Tools"},{"location":"agile/4-1-architecture/#41-agile-management-tools-amt","text":"","title":"4.1 Agile Management Tools (AMT)"},{"location":"agile/4-1-architecture/#411-purpose","text":"While the environment is established by stakeholder management and contractual obligations, it is also necessary to establish the physical infrastructure (toolset) necessary to enable agile. With that in mind, this section will focus on the provisioning of an infrastructure that supplies 4 general functions: Backlog Management - the tools necessary to capture and refine requirements as well as allows the PO to prioritize the different work efforts to provide the most value. This is the tool that maintains a prioritized and organized listing of the work items which need to be done for the project. Work management - the tools necessary to execute the specific methodology which will be employed by the development team (Scrum, Kanban, XP, hybrid). This is the tool which provides the team the ability to collaborate on the development of their work items (Scrum Board, Kanban Board, etc). Communications Management - the tools necessary to communicate the status of the development effort to internal and external stakeholders. It includes the ability to create reports, provide metrics (does not define the metrics themselves), and implement collaborative dashboards with information which is relevant (i.e. work items complete, work items remaining, identified risks, identified issues, test status, etc). The actual capabilities of the tool which will be used will be based on the communications needs of the stakeholders involved (the evolution of a dashboard or report is often iterative in nature as communication needs are better refined). Continuous Integration - While not a necessity, a continuous integration tool makes agile run much more efficiently. The concept of iterative deliveries to the customer requires a mechanism which allows for continuous inputs by the developers to the code to provide smaller increments versus the big bang development approach of waiting till everything is done. With continuous integration, you get an iterative product of better quality based on the integrated automated testing functionality built into the tool (will cover automated testing methodologies and benefits in a companion playbook). Each section below will include the recommended capabilities required of the enabling tools (general in nature - not tied to specific methodology) One note - the infrastructure tools used by the HIA community are on the Atlassian set of agile products as well as Team Foundation Server (TFS) / Visual Studios Team Server (VSTS). The table below shows a quick overview of the differences and similarities between the two in reference to enabling an agile framework. Links describing how to enable the management systems below within the VSTS/TFS and the Atlassian Products are found in Appendix C - Key Links.","title":"4.1.1 Purpose:"},{"location":"agile/4-1-architecture/#412-backlog-management","text":"When considering which tool to use, the following functionalities should be provided: Capture and refine requirements into nested Product Backlog Items (PBIs) - Epics, Features, User Stories Organize and prioritize those requirements Provide a reporting capability to show necessary details to understand the status of those Product Backlog Items","title":"4.1.2 Backlog Management"},{"location":"agile/4-1-architecture/#413-work-management-ie-release-roadmapscrum-boardskanban-boards","text":"The following functionalities should be provided: Capability to establish an agile execution board: Scrum - Sprint Board Kanban - Kanban Board XP / Hybrid - Agile Board Ability to refine the work in progress (add and update the status details of the different work items - including tasks, user stories, etc.) Ability to assign responsible parties to the different work items (while agile emphasizes the establishment of responsibility when capacity is available, there needs to be a method to monitor which work items have a responsible resource and which ones are still available to be worked on) Capability to report necessary details to understand the status of work in progress (metrics analysis will be provided later in the discussion of communication tools - here we are looking at status of individual versus aggregate work items).","title":"4.1.3 Work Management (i.e. Release Roadmap/Scrum Boards/Kanban Boards)"},{"location":"agile/4-1-architecture/#414-communication-management","text":"Recommend functions include the ability to produce the following: Dashboards Metric Analysis Reports: Release Roadmaps - These will be discussed later in the methodology, but the release roadmap provides an overview of when features are expected to be complete. In terms of project management, the release roadmap can be used to establish the schedule for the project.","title":"4.1.4 Communication Management"},{"location":"agile/4-1-architecture/#415-continuous-integration-architecture-and-management","text":"Recommended functionality for enabling continuous integration in any toolset: Version control tool (code repository management) Instrumented or scripted build process Trigger capability for implementing a build and test cycle based on code check-in Automated testing implementation capability Automated alerts, in case of a failed test, back to the developers so that they can resolve issue immediately (provide feedback on issues)","title":"4.1.5 Continuous Integration Architecture and Management"},{"location":"agile/5-1-backlog/","text":"5.1 Part I - Establish the Product Backlog (PB) and Constraints: 5.1.1 Establish the Product Backlog For a government project the initial list of requirements is established within the contract and will be used as the foundation for the initial PB development (and captured within the appropriate Backlog Management tool). The establishment of the PB is comparable to the establishment of a language - in this case it is the language of \"value\" that provides for understanding between the government and the development team. For this \"value\" language - the individual PB components provide the words and grammar (think of Epics as Paragraphs, Features as sentences and User Stories/Cards as words), while the order of the backlog based on prioritization of \"value\" provides the context of the product story. This initial section focuses on the creation of the Product Backlog. Later sections will discuss how the PB is prioritized to provide the most value up front as well as how it evolves based on a changing environment to continue to remain value relevant. As the PB will generally follow a rolling-wave planning process with more detail provided as it becomes available or necessary, it can initially be setup with larger organizational elements (epics and features) which will be iteratively refined later. A recommended nested organization for these components include: Epics - These include functionality which can take one or more releases to complete. For government contracts, these can be linked to a Contract Line Item Number for easier reference to cost tracking and earned value metrics. Features - These include functionality components that will generally take more than one iteration to complete and provide the functionality to the users through releases. Product Backlog Items (PBI) - These include the product increments which consolidate to provide the functionality of the feature and generally can be accomplished within a single iteration. PBIs can be decomposed into specific tasks with hours estimated for completion. The main component of the PB is the PBI that may be named something different depending on the framework (for Scrum and XP can be a user story, for Kanban can be a simple work item or card). In translating a requirement to a PBI, it is necessary to understand that the PBI can contain 1 or more requirements as it is based on the customer functional need (can be captured in government use case documents). However, while functional requirements will most likely include BES Process Directory requirements terms including \"shall\" which dictates the provision of a functional capability, they may also include conditional requirements indicated by the \"must\", \"must not\" and \"required\" requirements descriptions indicating additional performance requirements or constraints). Additional non-functional requirements to be considered include: Preparation and conduct of design reviews Hardware setup (Different environments) Tool setup (Software tools) Network Setup (establishing connectivity - access requirements, ports and protocols) UAT, Pre-Prod, Prod transitions (can be facilitated by continuous integration and Devops) Documentation (User Guides / CDRL's / Training Materials Regression testing (can be facilitated by automated testing) Cybersecurity testing The typical PBI contains a minimum of 3 elements - Title: Ensure this ties to the logic of the work (many times the titles of a PBI can cause confusion between the PO and development team if improperly stated). Description: Explains the customers need and the functionality required. Acceptance Criteria: Explains the conditions under which the PO will accept the work item as complete (note that there are additional criteria which will be discussed in presenting a \"Definition of Done\"). These criteria should be testable (I.e. should be specific enough to be able to qualify as complete or not complete). A typical PBI descriptions contains the following elements: \"As a...\" - Defines the perspective of the user who needs the functionality. \"I need...\" - Defines the functionality needed. \"So that...\" - Defines the why of the functionality (enables the developers' ingenuity as they may have a better solution which still meets the \"why\" of the user's request though the how may be something different). Example: As a maintenance scheduler I need to be able to review the Daily Schedule So that I can review the events scheduled for the day and the status of their completion Additional information can be included within the description. One recommendation is to include the requirement number(s) from the original contract in this section to facilitate the creation of the Requirement's Traceability Matrix necessary for design reviews and deliveries. While the PO is the primary owner of the backlog, the initial creation of the backlog is most effective when completed in a collaborative manner. While the term Sprint 0 is not an official Scrum term, it does provide a context for doing the preparation work necessary to initiate an agile methodology (without the PBI's it is not possible to do the work). Whatever it is called, there is a preparation session which is necessary to prime the pump of the agile framework by building the initial PB. Here is a recommended approach to doing this which ends with the government System Functional Review (SFR) which established the PB as the Functional Baseline: Preparation Phases (Phases are provided here as timeline will have to adjust based on size and complexity of requirements as well as whether preliminary planning has been done...i.e. feature driven planning): Phase 1: (Government and Development Team collaboration preparation) Government provides initial Requirements Traceability Matrix (RTM). Government provides initial feature backlog linked to RTM to development team (with minimum of description filled out and rough draft of acceptance criteria). Development team reviews feature descriptions and provides feature backlog feedback to government at end of the phase with recommended revised acceptance criteria and questions of clarification for review prior to physical collaboration. Phase 2 (Physical Collaboration - government and development team) Collaboratively refine features with end-state of initial Title, Description, and Acceptance Criteria agreed on by government and development team Conduct breakout sessions as necessary to provide additional details to development team necessary to make initial estimates of feature sizes Development team makes initial estimates of feature sizes (T-Shirt sizing). Development team prepares for SFR. 5.1.2 System Functional Review (SFR) The SFR is a multi-disciplined system-level technical review ensures that the functional baseline is established and has a reasonable expectation of satisfying the requirements of draft capability development document (CDD) within the current budget and schedule. It completes the process of defining the system-level technical requirements that are documented in the system performance specification. According to the BPD, a successful completion of SFR provides a sound technical basis for proceeding into preliminary design. If the above collaboration is done correctly, this review becomes a \"value language\" confirmation brief ensuring that the PB correctly reflects the requirements of the contract and are consistent with cost (program budget), risk and other system constraints. Audience : Functional Review Board (FRB) members Objectives : Reconfirm that Capability Package Requirements are linked to PB Features derived during the preparation event above (Sprint 0) Initial identification of system-level document changes which will need to be addressed during design reviews (see CDRL section (link within main document)) Establish Functional Baseline. Add Feature list to the Program Product Backlog to be considered in Release Planning for development based on PMO priorities and value determination. Determine based on Release Planning when each Feature should be considered within either the Preliminary Design Review or the Critical Design Review. Outputs : Required inputs for a Configuration Control Board (CCB). Approved PB and Functional Baseline based on Configuration Control Directive (CCD) from CCB. Updated system release roadmap with new features. Updated Requirement Traceability Matrix with links from requirements to PB. Identified system-level CDRLs to update for design reviews based on PB contents. Identified features for design reviews (Preliminary Design Review [PDR] and Critical Design Review [CDR]) 5.1.3 Release Management Release management is about determining what is expected to be in each release and what comes next for the development team as well as identifying PBI dependencies early enough so that they can be rectified prior to becoming impediments during development. The PB includes those features approved in the functional baseline by the PMO during the System Functional Release (SFR) conducted at the end of Sprint 0. The features are prioritized within the PB by the PMO and considered by a Release Planning committee for inclusion in upcoming releases. Those features which are selected for inclusion in upcoming releases are updated on the Release Roadmap based on their tentative estimate of completion. The Release Roadmap is a high-level view of the PB features, with a loose time frame for when the team will refine and develop those features. Proposed agenda for Release Planning meeting: Objective Overview Review Current Release Roadmap (Starting point) Review Current Status of Development Streams (Establish current status which may have changed since last release planning session) Determine dates for: Next Release (Feature focus for the CDR) Next Release (+1) (Feature focus for the PDR) Review Product Backlog Feature Priority Lists (Confirm priorities) Confirm Features for Next Release Identify Features for Next Release (+1) Update Release Roadmap (Use information from session to update the release roadmap) Discuss any release process improvement steps (Continuous Improvement) 5.1.4 System Engineering Design Reviews Link to DAU System Engineering Overview (Add to links appendix - will not be contained in base document): https://www.dau.mil/guidebooks/Shared%20Documents%20HTML/Chapter%203%20Systems%20Engineering.aspx#toc83 A secondary advantage of implementing a pro-active release management system is the ability to establish what features will be reviewed at what level for Agile-based design review gates (here we are talking about Preliminary and Critical Design Reviews but not in the level of detail that was expected within traditional development engineering management plans as Agile is about doing architectural design \"just in time\" and iteratively \"just enough\"). The Design Reviews can be synchronized with the above release management process to fulfill their primary objectives based on two different perspectives: Preliminary Design Review (PDR): Focus : The features included in the release after the upcoming release (long-term perspective). Objectives : Provide sufficient confidence in the preliminary design's integration with necessary system components to serve as the starting point for Agile incremental design during development. Provide technical confidence that the capability need can be satisfied within cost goals based on the schedule included in the release roadmap. Confirms that high-level design decisions are consistent with the user's performance, schedule needs, and the validated Capability Development Document (CDD). Establishes the allocated baseline (based on features), which is placed under formal configuration control. Agile Revisions : Identify dependencies on external requirements or other features to ensure proper sequencing and identify issues for resolution prior to them becoming impediments (identifies feature risks early). Determine required infrastructure requirements so they can be acquired prior to development. Agenda Template : Review Release Roadmap Review Features for the release after the immediate upcoming release. Review Feature description Review Feature acceptance criteria Review Features system-level design integration (focused on necessary design integration of feature capability) Review Feature dependencies (I.e. external interface requirements, additional software requirements) - these will become an initial set of assumptions for risk management (the goal is to mitigate the risks so that the assumption will not impede our development efforts later) Review Feature CDRL requirements (what CDRLs will need to be updated based on the completion of the feature development) Review Feature size estimates (establishes metric for estimating duration) Revise Release Roadmap as necessary based on updated information Critical Design Review (CDR): Focus : The features included in the next (upcoming) release (short-term perspective). Objectives : Establishes the initial product baseline (based on features), which is placed under formal configuration control. Confirms the high-level system design is expected to meet system performance requirements Confirms the system is on track to achieve affordability and should-cost goals as evidenced by the design documentation Establishes requirements and system interfaces for enabling system elements such as support equipment, training system, maintenance and data systems. Agile Revisions : Confirms that dependencies identified in the PDR have a resolution strategy that will be completed in time to enable development of the dependent features (will not become impediments during development). Confirms that features are ready for further refinement into work items for the development team. Agenda Template : Review Release Roadmap Review Features for the upcoming release. Review Feature description Review Feature acceptance criteria Review Features system-level design integration and system requirements: UI change requirements (mock-ups if available) Middle tier requirements (services) Data structure requirements Review Feature dependencies to ensure they have been resolved or a feasible resolution strategy is in place prior to beginning work on the feature (these address the initial assumptions determined during the PDR) Review Feature CDRL requirements - update as required Review Feature size estimates (establishes metric for estimating duration) - update based on additional information Revise Release Roadmap as necessary based on updated information 5.1.5 Refine the Product Backlog (Features -> Initial PBIs) In general, the refinement process establishes the \"development language\" between the Development Team and the Government through their designated PO. Feature refinement should be conducted about 2 months prior to working on the first PBI of the feature. The objectives of feature refinement are to: Break down feature to individual PBIs (user stories, cards, work items, etc) with, at a minimum, a description and acceptance criteria; Provide an initial estimate of the complexity of the work item (sizing estimates can be associated with established costs and used for EV metrics); Gain the formal approval of the PO to include the refined work items into the Product Backlog. Feature refinement in preparation for development work conducted between the government PO and Development Team: Normally takes 1-2 hours per feature if done correctly (the more discussion, the better clarity of understanding between the government and the development team increasing the likelihood that the development product will meet government expectations). Refine initial title, description, acceptance criteria at the feature - level based on knowledge acquired since design review - this is the focus of the PMO input - provides the WHAT of the feature. Identify the logic of the work. Identify the initial user stories. Ensure that the stories have a description and acceptance criteria which provide a common reference for the team to produce an initial sizing estimate (provides metric for EV - can be updated later). Identify infrastructure requirements (i.e. suites) necessary for development. Three questions to guide Feature Refinement Discussion and what they are attempting to elicit: What do you want to see at the end? (Acceptance Criteria) What are the high-level steps to get there? (Logic) What are the sub-steps for each of the high-level steps? (PBIs) 5.1.6 Prioritize the Product Backlog Agile is about providing value to the government customer as quickly as possible. Less valuable work is ranked lower in the priority scale. Thus, after refinement of the features into PBIs within the PB, it is necessary to rank them based on the value they provide. There are multiple considerations in evaluating the value of each individual PBI including: What provides most value functionally to the government user? What impact does the PBI have on other items within the backlog (i.e. are their dependencies between work items)? Cost / impact assessment of each PBI. Learning - Are their efficiencies to be gained for the remaining work items by completing a specific story earlier (i.e. taking a smaller item which is easier to do to establish the templates and processes for doing more complex work items of a similar nature later but at a more rapid pace)? Are their negative impacts to other PBIs by not completing one earlier? Regulatory deadlines (not considered agile, but is a consideration for prioritization) It is the responsibility of the PO, as the representative of the government to continuously reprioritize the PB to provide the most value to the government based on the current environment (especially as new PBIs are added or removed from the PB). Prioritization considerations to avoid (as they are normally short-term perspectives which may reduce long-term value): Highest Ranking Person in the Room Flavor of the Day 5.1.7 Constraints Analysis The above sections focus on refining requirements into work items, but there are several; other areas of regulatory constraints within the governmental regulatory environment that should be addressed prior to determining as well as implementing an Agile development methodology. The following is a non-exhaustive list of items that may impact the execution of development based on scope (these provide a basis for discussion between the Government and Development Team to determine which will be enforced and to what level): External testing requirements such as DT&E and QT&E (capacity may be required to prepare and support testing - need to delineate what falls to the development team and what will be done by the government) Additional BPD Design, Test, Functional Readiness Reviews (all come with a preparation and documentation overhead - will need to discuss expectations and agile modifications based on iterative versus up front design) Development environment limitations (discussion around whether the contractor will control the environment or whether it is an externally administered environment - I.e. the Capabilities Integration Environment (CIE)) Access requirements (security requirements may dictate which personnel resources can be allocated to a team) Re-usability and compatibility requirements (establishes the boundaries of initiative for the developers - \"can be innovative within these limits\") Cybersecurity requirements (the pre-requisite cybersecurity requirements may limit development options) 5.1.8 Change Management While there is an inherent flexibility within Agile methodologies to accept change specifically through backlog grooming and PBI refinement, there is still the challenge of differentiating between contractual scope additions versus simple requirement refinement. The differentiation of what will be left within the authority of the PO to approve through backlog refinement and what requirement changes need formal approval in the form of a Configuration Change Directive (CCD) should be established prior to the beginning of development to ensure proper steps are taken within the established constraints to adapt to change. When a formal configuration change is necessary, the government Configuration Control Board (CCB) with input from the Functional Review Board (FRB) should: Review the list of any new user requests, deficiency reports and change requests received in the project space; Assess the impact of a change; Assign a priority to the change; Assign a business value to the change; and Implement a CCD to insert an approved work item for the change into the Product Backlog (upon completion of any contractual modification requirements to accept in the new work)","title":"5.1 Part I - Establish the Product Backlog (PB) and Constraints"},{"location":"agile/5-1-backlog/#51-part-i-establish-the-product-backlog-pb-and-constraints","text":"","title":"5.1 Part I - Establish the Product Backlog (PB) and Constraints:"},{"location":"agile/5-1-backlog/#511-establish-the-product-backlog","text":"For a government project the initial list of requirements is established within the contract and will be used as the foundation for the initial PB development (and captured within the appropriate Backlog Management tool). The establishment of the PB is comparable to the establishment of a language - in this case it is the language of \"value\" that provides for understanding between the government and the development team. For this \"value\" language - the individual PB components provide the words and grammar (think of Epics as Paragraphs, Features as sentences and User Stories/Cards as words), while the order of the backlog based on prioritization of \"value\" provides the context of the product story. This initial section focuses on the creation of the Product Backlog. Later sections will discuss how the PB is prioritized to provide the most value up front as well as how it evolves based on a changing environment to continue to remain value relevant. As the PB will generally follow a rolling-wave planning process with more detail provided as it becomes available or necessary, it can initially be setup with larger organizational elements (epics and features) which will be iteratively refined later. A recommended nested organization for these components include: Epics - These include functionality which can take one or more releases to complete. For government contracts, these can be linked to a Contract Line Item Number for easier reference to cost tracking and earned value metrics. Features - These include functionality components that will generally take more than one iteration to complete and provide the functionality to the users through releases. Product Backlog Items (PBI) - These include the product increments which consolidate to provide the functionality of the feature and generally can be accomplished within a single iteration. PBIs can be decomposed into specific tasks with hours estimated for completion. The main component of the PB is the PBI that may be named something different depending on the framework (for Scrum and XP can be a user story, for Kanban can be a simple work item or card). In translating a requirement to a PBI, it is necessary to understand that the PBI can contain 1 or more requirements as it is based on the customer functional need (can be captured in government use case documents). However, while functional requirements will most likely include BES Process Directory requirements terms including \"shall\" which dictates the provision of a functional capability, they may also include conditional requirements indicated by the \"must\", \"must not\" and \"required\" requirements descriptions indicating additional performance requirements or constraints). Additional non-functional requirements to be considered include: Preparation and conduct of design reviews Hardware setup (Different environments) Tool setup (Software tools) Network Setup (establishing connectivity - access requirements, ports and protocols) UAT, Pre-Prod, Prod transitions (can be facilitated by continuous integration and Devops) Documentation (User Guides / CDRL's / Training Materials Regression testing (can be facilitated by automated testing) Cybersecurity testing The typical PBI contains a minimum of 3 elements - Title: Ensure this ties to the logic of the work (many times the titles of a PBI can cause confusion between the PO and development team if improperly stated). Description: Explains the customers need and the functionality required. Acceptance Criteria: Explains the conditions under which the PO will accept the work item as complete (note that there are additional criteria which will be discussed in presenting a \"Definition of Done\"). These criteria should be testable (I.e. should be specific enough to be able to qualify as complete or not complete). A typical PBI descriptions contains the following elements: \"As a...\" - Defines the perspective of the user who needs the functionality. \"I need...\" - Defines the functionality needed. \"So that...\" - Defines the why of the functionality (enables the developers' ingenuity as they may have a better solution which still meets the \"why\" of the user's request though the how may be something different). Example: As a maintenance scheduler I need to be able to review the Daily Schedule So that I can review the events scheduled for the day and the status of their completion Additional information can be included within the description. One recommendation is to include the requirement number(s) from the original contract in this section to facilitate the creation of the Requirement's Traceability Matrix necessary for design reviews and deliveries. While the PO is the primary owner of the backlog, the initial creation of the backlog is most effective when completed in a collaborative manner. While the term Sprint 0 is not an official Scrum term, it does provide a context for doing the preparation work necessary to initiate an agile methodology (without the PBI's it is not possible to do the work). Whatever it is called, there is a preparation session which is necessary to prime the pump of the agile framework by building the initial PB. Here is a recommended approach to doing this which ends with the government System Functional Review (SFR) which established the PB as the Functional Baseline:","title":"5.1.1 Establish the Product Backlog"},{"location":"agile/5-1-backlog/#preparation-phases","text":"(Phases are provided here as timeline will have to adjust based on size and complexity of requirements as well as whether preliminary planning has been done...i.e. feature driven planning): Phase 1: (Government and Development Team collaboration preparation) Government provides initial Requirements Traceability Matrix (RTM). Government provides initial feature backlog linked to RTM to development team (with minimum of description filled out and rough draft of acceptance criteria). Development team reviews feature descriptions and provides feature backlog feedback to government at end of the phase with recommended revised acceptance criteria and questions of clarification for review prior to physical collaboration. Phase 2 (Physical Collaboration - government and development team) Collaboratively refine features with end-state of initial Title, Description, and Acceptance Criteria agreed on by government and development team Conduct breakout sessions as necessary to provide additional details to development team necessary to make initial estimates of feature sizes Development team makes initial estimates of feature sizes (T-Shirt sizing). Development team prepares for SFR.","title":"Preparation Phases"},{"location":"agile/5-1-backlog/#512-system-functional-review-sfr","text":"The SFR is a multi-disciplined system-level technical review ensures that the functional baseline is established and has a reasonable expectation of satisfying the requirements of draft capability development document (CDD) within the current budget and schedule. It completes the process of defining the system-level technical requirements that are documented in the system performance specification. According to the BPD, a successful completion of SFR provides a sound technical basis for proceeding into preliminary design. If the above collaboration is done correctly, this review becomes a \"value language\" confirmation brief ensuring that the PB correctly reflects the requirements of the contract and are consistent with cost (program budget), risk and other system constraints. Audience : Functional Review Board (FRB) members Objectives : Reconfirm that Capability Package Requirements are linked to PB Features derived during the preparation event above (Sprint 0) Initial identification of system-level document changes which will need to be addressed during design reviews (see CDRL section (link within main document)) Establish Functional Baseline. Add Feature list to the Program Product Backlog to be considered in Release Planning for development based on PMO priorities and value determination. Determine based on Release Planning when each Feature should be considered within either the Preliminary Design Review or the Critical Design Review. Outputs : Required inputs for a Configuration Control Board (CCB). Approved PB and Functional Baseline based on Configuration Control Directive (CCD) from CCB. Updated system release roadmap with new features. Updated Requirement Traceability Matrix with links from requirements to PB. Identified system-level CDRLs to update for design reviews based on PB contents. Identified features for design reviews (Preliminary Design Review [PDR] and Critical Design Review [CDR])","title":"5.1.2 System Functional Review (SFR)"},{"location":"agile/5-1-backlog/#513-release-management","text":"Release management is about determining what is expected to be in each release and what comes next for the development team as well as identifying PBI dependencies early enough so that they can be rectified prior to becoming impediments during development. The PB includes those features approved in the functional baseline by the PMO during the System Functional Release (SFR) conducted at the end of Sprint 0. The features are prioritized within the PB by the PMO and considered by a Release Planning committee for inclusion in upcoming releases. Those features which are selected for inclusion in upcoming releases are updated on the Release Roadmap based on their tentative estimate of completion. The Release Roadmap is a high-level view of the PB features, with a loose time frame for when the team will refine and develop those features. Proposed agenda for Release Planning meeting: Objective Overview Review Current Release Roadmap (Starting point) Review Current Status of Development Streams (Establish current status which may have changed since last release planning session) Determine dates for: Next Release (Feature focus for the CDR) Next Release (+1) (Feature focus for the PDR) Review Product Backlog Feature Priority Lists (Confirm priorities) Confirm Features for Next Release Identify Features for Next Release (+1) Update Release Roadmap (Use information from session to update the release roadmap) Discuss any release process improvement steps (Continuous Improvement)","title":"5.1.3 Release Management"},{"location":"agile/5-1-backlog/#514-system-engineering-design-reviews","text":"Link to DAU System Engineering Overview (Add to links appendix - will not be contained in base document): https://www.dau.mil/guidebooks/Shared%20Documents%20HTML/Chapter%203%20Systems%20Engineering.aspx#toc83 A secondary advantage of implementing a pro-active release management system is the ability to establish what features will be reviewed at what level for Agile-based design review gates (here we are talking about Preliminary and Critical Design Reviews but not in the level of detail that was expected within traditional development engineering management plans as Agile is about doing architectural design \"just in time\" and iteratively \"just enough\"). The Design Reviews can be synchronized with the above release management process to fulfill their primary objectives based on two different perspectives: Preliminary Design Review (PDR): Focus : The features included in the release after the upcoming release (long-term perspective). Objectives : Provide sufficient confidence in the preliminary design's integration with necessary system components to serve as the starting point for Agile incremental design during development. Provide technical confidence that the capability need can be satisfied within cost goals based on the schedule included in the release roadmap. Confirms that high-level design decisions are consistent with the user's performance, schedule needs, and the validated Capability Development Document (CDD). Establishes the allocated baseline (based on features), which is placed under formal configuration control. Agile Revisions : Identify dependencies on external requirements or other features to ensure proper sequencing and identify issues for resolution prior to them becoming impediments (identifies feature risks early). Determine required infrastructure requirements so they can be acquired prior to development. Agenda Template : Review Release Roadmap Review Features for the release after the immediate upcoming release. Review Feature description Review Feature acceptance criteria Review Features system-level design integration (focused on necessary design integration of feature capability) Review Feature dependencies (I.e. external interface requirements, additional software requirements) - these will become an initial set of assumptions for risk management (the goal is to mitigate the risks so that the assumption will not impede our development efforts later) Review Feature CDRL requirements (what CDRLs will need to be updated based on the completion of the feature development) Review Feature size estimates (establishes metric for estimating duration) Revise Release Roadmap as necessary based on updated information Critical Design Review (CDR): Focus : The features included in the next (upcoming) release (short-term perspective). Objectives : Establishes the initial product baseline (based on features), which is placed under formal configuration control. Confirms the high-level system design is expected to meet system performance requirements Confirms the system is on track to achieve affordability and should-cost goals as evidenced by the design documentation Establishes requirements and system interfaces for enabling system elements such as support equipment, training system, maintenance and data systems. Agile Revisions : Confirms that dependencies identified in the PDR have a resolution strategy that will be completed in time to enable development of the dependent features (will not become impediments during development). Confirms that features are ready for further refinement into work items for the development team. Agenda Template : Review Release Roadmap Review Features for the upcoming release. Review Feature description Review Feature acceptance criteria Review Features system-level design integration and system requirements: UI change requirements (mock-ups if available) Middle tier requirements (services) Data structure requirements Review Feature dependencies to ensure they have been resolved or a feasible resolution strategy is in place prior to beginning work on the feature (these address the initial assumptions determined during the PDR) Review Feature CDRL requirements - update as required Review Feature size estimates (establishes metric for estimating duration) - update based on additional information Revise Release Roadmap as necessary based on updated information","title":"5.1.4 System Engineering Design Reviews"},{"location":"agile/5-1-backlog/#515-refine-the-product-backlog-features-initial-pbis","text":"In general, the refinement process establishes the \"development language\" between the Development Team and the Government through their designated PO. Feature refinement should be conducted about 2 months prior to working on the first PBI of the feature. The objectives of feature refinement are to: Break down feature to individual PBIs (user stories, cards, work items, etc) with, at a minimum, a description and acceptance criteria; Provide an initial estimate of the complexity of the work item (sizing estimates can be associated with established costs and used for EV metrics); Gain the formal approval of the PO to include the refined work items into the Product Backlog. Feature refinement in preparation for development work conducted between the government PO and Development Team: Normally takes 1-2 hours per feature if done correctly (the more discussion, the better clarity of understanding between the government and the development team increasing the likelihood that the development product will meet government expectations). Refine initial title, description, acceptance criteria at the feature - level based on knowledge acquired since design review - this is the focus of the PMO input - provides the WHAT of the feature. Identify the logic of the work. Identify the initial user stories. Ensure that the stories have a description and acceptance criteria which provide a common reference for the team to produce an initial sizing estimate (provides metric for EV - can be updated later). Identify infrastructure requirements (i.e. suites) necessary for development. Three questions to guide Feature Refinement Discussion and what they are attempting to elicit: What do you want to see at the end? (Acceptance Criteria) What are the high-level steps to get there? (Logic) What are the sub-steps for each of the high-level steps? (PBIs)","title":"5.1.5 Refine the Product Backlog (Features -&gt; Initial PBIs)"},{"location":"agile/5-1-backlog/#516-prioritize-the-product-backlog","text":"Agile is about providing value to the government customer as quickly as possible. Less valuable work is ranked lower in the priority scale. Thus, after refinement of the features into PBIs within the PB, it is necessary to rank them based on the value they provide. There are multiple considerations in evaluating the value of each individual PBI including: What provides most value functionally to the government user? What impact does the PBI have on other items within the backlog (i.e. are their dependencies between work items)? Cost / impact assessment of each PBI. Learning - Are their efficiencies to be gained for the remaining work items by completing a specific story earlier (i.e. taking a smaller item which is easier to do to establish the templates and processes for doing more complex work items of a similar nature later but at a more rapid pace)? Are their negative impacts to other PBIs by not completing one earlier? Regulatory deadlines (not considered agile, but is a consideration for prioritization) It is the responsibility of the PO, as the representative of the government to continuously reprioritize the PB to provide the most value to the government based on the current environment (especially as new PBIs are added or removed from the PB). Prioritization considerations to avoid (as they are normally short-term perspectives which may reduce long-term value): Highest Ranking Person in the Room Flavor of the Day","title":"5.1.6 Prioritize the Product Backlog"},{"location":"agile/5-1-backlog/#517-constraints-analysis","text":"The above sections focus on refining requirements into work items, but there are several; other areas of regulatory constraints within the governmental regulatory environment that should be addressed prior to determining as well as implementing an Agile development methodology. The following is a non-exhaustive list of items that may impact the execution of development based on scope (these provide a basis for discussion between the Government and Development Team to determine which will be enforced and to what level): External testing requirements such as DT&E and QT&E (capacity may be required to prepare and support testing - need to delineate what falls to the development team and what will be done by the government) Additional BPD Design, Test, Functional Readiness Reviews (all come with a preparation and documentation overhead - will need to discuss expectations and agile modifications based on iterative versus up front design) Development environment limitations (discussion around whether the contractor will control the environment or whether it is an externally administered environment - I.e. the Capabilities Integration Environment (CIE)) Access requirements (security requirements may dictate which personnel resources can be allocated to a team) Re-usability and compatibility requirements (establishes the boundaries of initiative for the developers - \"can be innovative within these limits\") Cybersecurity requirements (the pre-requisite cybersecurity requirements may limit development options)","title":"5.1.7 Constraints Analysis"},{"location":"agile/5-1-backlog/#518-change-management","text":"While there is an inherent flexibility within Agile methodologies to accept change specifically through backlog grooming and PBI refinement, there is still the challenge of differentiating between contractual scope additions versus simple requirement refinement. The differentiation of what will be left within the authority of the PO to approve through backlog refinement and what requirement changes need formal approval in the form of a Configuration Change Directive (CCD) should be established prior to the beginning of development to ensure proper steps are taken within the established constraints to adapt to change. When a formal configuration change is necessary, the government Configuration Control Board (CCB) with input from the Functional Review Board (FRB) should: Review the list of any new user requests, deficiency reports and change requests received in the project space; Assess the impact of a change; Assign a priority to the change; Assign a business value to the change; and Implement a CCD to insert an approved work item for the change into the Product Backlog (upon completion of any contractual modification requirements to accept in the new work)","title":"5.1.8 Change Management"},{"location":"agile/5-2-methodology/","text":"5.2 Part II - Establish the Methodology: 5.2.1 Definition of \"Ready\" and \"done\" Prior to implementing a specific development methodology, the definitions for the development entry and exit criteria need to be defined. Thus, prior to beginning development through any of the methodologies, two definitions should be agreed upon between the PO and development team in reference to individual PBIs: Deifinition of Ready Purpose Describes materials and topics that must be included and addressed in well written backlog items (I.e. Scrum / XP User Stories, Kanban cards) Used to evaluate whether or not a PBI has been appropriately elaborated and is ready for development Helps ensure that backlog items are complete and understandable before being scheduled for an iteration. EXAMPLE DEFINITION OF READY CHECKLIST Description clearly states the who (\"As a\"), what (\"I want to\"), and why (\"so that\") are for the backlog item PO has approved the backlog item Acceptance criteria is clearly defined Scenarios and expected outcomes are clearly defined Artifacts defining user interface requirements are included (if applicable) Business rules are referenced or included (if applicable) Dependencies are identified Development Team has reviewed and confirmed they understand the backlog item Backlog item is appropriately estimated (I.e. sized for scrum) \"Definition of Done\" for the PBI is understood Definition of Done Purpose The \"Definition of Done\" describes criterion that must be met by each committed User Story or Defect to development before the PO can validate that the PBI is accepted. Definition is used to help ensure that the developed product is consistent with the associated backlog item, is high quality, and is ready for production testing. \"Done\" establishes quality norms and assures the PO that major defects are not likely to be identified during production testing EXAMPLE DEFINITION OF DONE CHECKLIST Unit testing and module testing are complete All acceptance criteria defined within the PBI are met Test results have been reviewed with the PO The PO concurs with the adequacy of the testing The backlog item has passed acceptance testing by the PO The development product has been demonstrated and accepted by the PO 5.2.2 Scrum This section provides a template for the scrum process. As it is a template, government and development teams are encouraged to adapt the process based on team structure, culture and regulatory constraints to make it more efficient. To that effect, this section is prescriptive versus directive in nature. Also note that we have transitioned from the general agile term of PBI to user stories as they relate to Scrum. Scrum Methodology: Scrum Events The five key Scrum events are: Product Backlog Refinement - Based on the project release roadmap and PO priorities, there are two components to refinement: Feature Refinement - Breaking down features and setting priorities (this process was explained in Section 5.1.5 above under feature refinement) User Story Refinement - Grooming user stories so that they are ready to be accepted into a sprint Sprint Planning: Determining the goals and related stories for a sprint Sprint Execution - Developing and testing stories so they are done Sprint Review - Providing feedback to improve the product Sprint Retrospective - Determining how to improve the process Product Backlog Refinement Feature Refinement Session (2 Sprints early) : Coordinated by the Scrum Master with the Solution Architect and Customer System Engineer (and POs if available) to: Ensure that the feature acceptance criteria are still correct for upcoming features originally determined during Sprint 0 activities. Ensure that all dependencies (possible future impediments) have a resolution strategy prior to development. Ensure that design tenets are understood within the context of the feature work. User Story Refinement Collaboration (Between Feature and User Story Refinement Sessions) : Team designated Business Analyst works collaboratively with the PO to refine story priority and the acceptance criteria necessary to accomplish the associated feature functionality. Team testers can also add necessary positive test cases based on draft acceptance criteria to the story (facilitates test driven development). Refinement Session - User Stories (1 Sprint early) : Business Analyst discusses each story with the development team to ensure understanding of delivery requirements. The PO is included in this refinement session, but it is the development team that is being introduced to the stories. The PO is there to confirm through listening to the team's conversations with the Business Analyst that his/her intent is understood within the context of the story. This refinement session is done to ensure that each story meets the established Definition of Ready prior to being accepted into the sprint for development. Meeting the Definition of Ready : The acronym INVEST provides a high-level cross-check of whether a story is Ready for development. It is normally part of the last check done on a story during either Refinement Day activities or as part of Sprint Planning for priority stories that have been updated since Refinement Day. While it is presented here under the Scrum section, it can be used with modification (I.e. Kanban work flow estimates versus sprint estimates) within the other agile frameworks as a final check for each PBI). The acronym represents the following criteria: (I) Independent - Story contains no dependencies on unavailable external resources or preparatory user story completion (N) Negotiable - Three perspectives must be represented in refinement of the user story to meet the PO's intent - Business Analyst, Developer, Tester - if they have not been involved in refinement, a key perspective on the complexity of the story is missing from the preparatory analysis. (V) Valuable - Established by the prioritization given by the POs. (E) Estimable - Estimates provide a gage for the acceptance of future work levels into a sprint as well as for forecasting future work timelines - they gain value over time as the team evolves. (S) Small - User stories should be small enough to complete within a sprint while large enough to provide observable value to the PO. (T) Testable - Positive manual test cases will be added to appropriate user stories prior to being taken into the sprint to account for successful achievement of acceptance criteria. As necessary, these tests will be augmented during the sprint with additional negative manual and automated tests. Based on the high-level of interaction between the team and the PO in this option, each story should have as a minimum 3 touch points with the PO to confirm that PO's intent is maintained throughout refinement. At a minimum, these touch points are the verification of the acceptance criteria in the documentation, the answering of team questions during team refinement of stories, and the final acceptance of the story as ready for development At this point the story is considered \"ready\" for acceptance into a sprint during sprint planning. Sprint Planning Sprint Planning (Appendix A) Planning centers around the presence of the PO within the Planning meeting. During this meeting, the PO will review the current list of Ready user stories to ensure that they are still valuable (relevant), as well as reprioritize the Backlog based on existing value as necessary (this facilitates the team's acceptance of work into the sprint as they simply take user stories from the top of the list within their established sprint capacity. Sprint Execution Sprint Execution (Appendix A) While developers and testers work to complete and validate the work required for each user story, Business Analysts and, if necessary, POs are available for clarification of requirements to ensure that the work produced meets the intent of the customer. If refinement has been done correctly, sprint execution consists of developers completing their work, testers executing the tests developed prior to the sprint (with the addition of any necessary negative test cases), and BAs and testers reviewing the completed work with the PO in order to receive early feedback or formal closure of the story as meeting the teams \"Definition of Done\". The user story acceptance criteria is the foundation for this definition. The story points assigned to sprint user stories not designated as \"Done\" are not counted when measuring sprint completion velocity. Sprint Review Sprint Review (Appendix A) Story acceptance as Done is completed by the PO only (the same person who approves the acceptance criteria must be the person who approves work against the approved acceptance criteria). If the PO believes that the work does not satisfy the customer requirements, they have to make a decision: if the work does not meet the acceptance criteria, they can reject the work as not done and provide additional clarification; if the work meets the acceptance criteria, but the PO wants additional revisions, additional user stories are created and prioritized within the existing backlog. The key to the review is that it is interactive between the PO and Development Team - the focus being to improve the product. Sprint Retrospective Sprint Retrospective (Appendix A) This is where the POs and the Development Team focus on improving the Agile process. Critical to understanding the concept of the retrospective is that learning always occurs during these events (Successful sprints can indicate items to sustain; failed sprints provide opportunities to learn where the team can improve). One note to remember in the retrospective is that the process should facilitate the development efforts - not impede them (hence why this document is presented as a template - it needs to be adjusted to fit the needs of the agile environment in which it exists which is a combination of the people, processes, tools, relationships and levels of trust involved in the project).","title":"5.2 Part II - Establish the Methodology"},{"location":"agile/5-2-methodology/#52-part-ii-establish-the-methodology","text":"","title":"5.2 Part II - Establish the Methodology:"},{"location":"agile/5-2-methodology/#521-definition-of-ready-and-done","text":"Prior to implementing a specific development methodology, the definitions for the development entry and exit criteria need to be defined. Thus, prior to beginning development through any of the methodologies, two definitions should be agreed upon between the PO and development team in reference to individual PBIs: Deifinition of Ready Purpose Describes materials and topics that must be included and addressed in well written backlog items (I.e. Scrum / XP User Stories, Kanban cards) Used to evaluate whether or not a PBI has been appropriately elaborated and is ready for development Helps ensure that backlog items are complete and understandable before being scheduled for an iteration. EXAMPLE DEFINITION OF READY CHECKLIST Description clearly states the who (\"As a\"), what (\"I want to\"), and why (\"so that\") are for the backlog item PO has approved the backlog item Acceptance criteria is clearly defined Scenarios and expected outcomes are clearly defined Artifacts defining user interface requirements are included (if applicable) Business rules are referenced or included (if applicable) Dependencies are identified Development Team has reviewed and confirmed they understand the backlog item Backlog item is appropriately estimated (I.e. sized for scrum) \"Definition of Done\" for the PBI is understood Definition of Done Purpose The \"Definition of Done\" describes criterion that must be met by each committed User Story or Defect to development before the PO can validate that the PBI is accepted. Definition is used to help ensure that the developed product is consistent with the associated backlog item, is high quality, and is ready for production testing. \"Done\" establishes quality norms and assures the PO that major defects are not likely to be identified during production testing EXAMPLE DEFINITION OF DONE CHECKLIST Unit testing and module testing are complete All acceptance criteria defined within the PBI are met Test results have been reviewed with the PO The PO concurs with the adequacy of the testing The backlog item has passed acceptance testing by the PO The development product has been demonstrated and accepted by the PO","title":"5.2.1 Definition of \"Ready\" and \"done\""},{"location":"agile/5-2-methodology/#522-scrum","text":"This section provides a template for the scrum process. As it is a template, government and development teams are encouraged to adapt the process based on team structure, culture and regulatory constraints to make it more efficient. To that effect, this section is prescriptive versus directive in nature. Also note that we have transitioned from the general agile term of PBI to user stories as they relate to Scrum. Scrum Methodology: Scrum Events The five key Scrum events are: Product Backlog Refinement - Based on the project release roadmap and PO priorities, there are two components to refinement: Feature Refinement - Breaking down features and setting priorities (this process was explained in Section 5.1.5 above under feature refinement) User Story Refinement - Grooming user stories so that they are ready to be accepted into a sprint Sprint Planning: Determining the goals and related stories for a sprint Sprint Execution - Developing and testing stories so they are done Sprint Review - Providing feedback to improve the product Sprint Retrospective - Determining how to improve the process Product Backlog Refinement Feature Refinement Session (2 Sprints early) : Coordinated by the Scrum Master with the Solution Architect and Customer System Engineer (and POs if available) to: Ensure that the feature acceptance criteria are still correct for upcoming features originally determined during Sprint 0 activities. Ensure that all dependencies (possible future impediments) have a resolution strategy prior to development. Ensure that design tenets are understood within the context of the feature work. User Story Refinement Collaboration (Between Feature and User Story Refinement Sessions) : Team designated Business Analyst works collaboratively with the PO to refine story priority and the acceptance criteria necessary to accomplish the associated feature functionality. Team testers can also add necessary positive test cases based on draft acceptance criteria to the story (facilitates test driven development). Refinement Session - User Stories (1 Sprint early) : Business Analyst discusses each story with the development team to ensure understanding of delivery requirements. The PO is included in this refinement session, but it is the development team that is being introduced to the stories. The PO is there to confirm through listening to the team's conversations with the Business Analyst that his/her intent is understood within the context of the story. This refinement session is done to ensure that each story meets the established Definition of Ready prior to being accepted into the sprint for development. Meeting the Definition of Ready : The acronym INVEST provides a high-level cross-check of whether a story is Ready for development. It is normally part of the last check done on a story during either Refinement Day activities or as part of Sprint Planning for priority stories that have been updated since Refinement Day. While it is presented here under the Scrum section, it can be used with modification (I.e. Kanban work flow estimates versus sprint estimates) within the other agile frameworks as a final check for each PBI). The acronym represents the following criteria: (I) Independent - Story contains no dependencies on unavailable external resources or preparatory user story completion (N) Negotiable - Three perspectives must be represented in refinement of the user story to meet the PO's intent - Business Analyst, Developer, Tester - if they have not been involved in refinement, a key perspective on the complexity of the story is missing from the preparatory analysis. (V) Valuable - Established by the prioritization given by the POs. (E) Estimable - Estimates provide a gage for the acceptance of future work levels into a sprint as well as for forecasting future work timelines - they gain value over time as the team evolves. (S) Small - User stories should be small enough to complete within a sprint while large enough to provide observable value to the PO. (T) Testable - Positive manual test cases will be added to appropriate user stories prior to being taken into the sprint to account for successful achievement of acceptance criteria. As necessary, these tests will be augmented during the sprint with additional negative manual and automated tests. Based on the high-level of interaction between the team and the PO in this option, each story should have as a minimum 3 touch points with the PO to confirm that PO's intent is maintained throughout refinement. At a minimum, these touch points are the verification of the acceptance criteria in the documentation, the answering of team questions during team refinement of stories, and the final acceptance of the story as ready for development At this point the story is considered \"ready\" for acceptance into a sprint during sprint planning. Sprint Planning Sprint Planning (Appendix A) Planning centers around the presence of the PO within the Planning meeting. During this meeting, the PO will review the current list of Ready user stories to ensure that they are still valuable (relevant), as well as reprioritize the Backlog based on existing value as necessary (this facilitates the team's acceptance of work into the sprint as they simply take user stories from the top of the list within their established sprint capacity. Sprint Execution Sprint Execution (Appendix A) While developers and testers work to complete and validate the work required for each user story, Business Analysts and, if necessary, POs are available for clarification of requirements to ensure that the work produced meets the intent of the customer. If refinement has been done correctly, sprint execution consists of developers completing their work, testers executing the tests developed prior to the sprint (with the addition of any necessary negative test cases), and BAs and testers reviewing the completed work with the PO in order to receive early feedback or formal closure of the story as meeting the teams \"Definition of Done\". The user story acceptance criteria is the foundation for this definition. The story points assigned to sprint user stories not designated as \"Done\" are not counted when measuring sprint completion velocity. Sprint Review Sprint Review (Appendix A) Story acceptance as Done is completed by the PO only (the same person who approves the acceptance criteria must be the person who approves work against the approved acceptance criteria). If the PO believes that the work does not satisfy the customer requirements, they have to make a decision: if the work does not meet the acceptance criteria, they can reject the work as not done and provide additional clarification; if the work meets the acceptance criteria, but the PO wants additional revisions, additional user stories are created and prioritized within the existing backlog. The key to the review is that it is interactive between the PO and Development Team - the focus being to improve the product. Sprint Retrospective Sprint Retrospective (Appendix A) This is where the POs and the Development Team focus on improving the Agile process. Critical to understanding the concept of the retrospective is that learning always occurs during these events (Successful sprints can indicate items to sustain; failed sprints provide opportunities to learn where the team can improve). One note to remember in the retrospective is that the process should facilitate the development efforts - not impede them (hence why this document is presented as a template - it needs to be adjusted to fit the needs of the agile environment in which it exists which is a combination of the people, processes, tools, relationships and levels of trust involved in the project).","title":"5.2.2 Scrum"},{"location":"agile/5-3-kanban/","text":"5.3 Kanban Considerations for establishing a Kanban process: Establish the Kanban Board - Visualize the workflow to organize, optimize, and track the work item development progress (allows for transparency of status) Limit Work in Progress (WIP) - Kanban is about flow. The more items in progress, the more risk of nothing getting done. It is better to have a limited WIP that emphasizes the Lean pull system. That is, when a resource has completed work on a work item and moved it to the next column, s/he pulls the next work item from the column before it. A starting rule for each policy column is limit WIP to amount of column resources + 1 (i.e. for a development column - the WIP would be the number of developers on the team plus one - that allows for a developer who hits an impediment to stop work on that work item and pull another one from those completed in the previous column while the Agile Lead works to remove the impediment. This allows for the flow to continue). Key points in Kanban: Minimize Lead Time (the time a work item is in any specific stage / column) Minimize Cycle Time (the time for the entire work flow cycle to complete for a single work item) - since establishing a cycle time allows for estimation of work items, but not all work items are the same complexity - one methodology for differentiating the work item complexity is to provide multiple swim lanes for tracking work progress - One for work items which have an expected cycle time of 1 week One for 2-week items One for 1-month items Optional - an expedited swim lane (similar to the express lane at a supermarket) for high-priority work items that should take precedence over all others (this will be mentioned again as a strategy for using Kanban to manage Operations and Maintenance work items) Manage flow - The Agile Lead works with the development team to ensure that workflow issues are identified and resolved (i.e. if there is a backup in the workflow, an additional resource may be needed to relieve pressure on the bottleneck column) Make process policies explicit - Each column on the Kanban Board should have an explicit policy which indicates when the work in that column is complete (policy = that column's \"Definition of Done\") Improve the process - In collaboration between the development team and PMO - there should be a focus on continuous improvement of the process (re-define the policies, determine whether columns should be added, determine whether columns should be deleted) While Scrum focuses on a cross-functional team, Kanban focuses more on aligning the resources with the policies. A simple representation of a Kanban Board (this also provides a quick start-up template for HIA products) is: Templated steps / policies for an initial Development Team Kanban board: Backlog : PMO PO prioritizes the Work Items in the Backlog Identify : Development Team Business Analyst in coordination with the PMO PO will identify / refine the description and acceptance criteria for the work item PO Approved : PMO PO will approve the description and acceptance criteria (Confirms that the work meets functional requirements - think mini-PDR) Design : Development Team's Senior Developer will complete draft solution design Cyber Assessment : Developer's cybersecurity representative (ICW PMO Cybersecurity) will complete security assessment of solution PMO Engineer Approved : PMO Engineer will approve design of solution (Confirm that solution meets technical requirements - think mini-CDR) - work item now \"ready\" for development. Develop / Implement : Developers implement the solution Test : Development team testers and cybersecurity will conduct testing and update documentation as needed Pending Acceptance : Development team will demonstrate completed work item functionality to PMO Done : PMO will accept work item as complete (meets the Definition of Done) An Operations and Maintenance team provides an example of a project built for Kanban. The backlog would contain the ticket items requiring resolution with the priority of work established by a government Product Owner. This approach allows refinement of last minute prioritized work items identified by system users through the workflow's column policies identified above (the process of making the work item \"ready\" is captured within the workflow). Also, if the Kanban board swim lanes are setup as described above, the board will contain an expedite lane for flowing through critical fixes requiring short duration resolution. (Note: Another example would be the cybersecurity team handling the implementation of required TCNO/IAVAs.) 5.3.1 Kanban Cadences While Scrum Events focus on execution of the time-boxed process, the key Kanban events (cadences - named for their formal rhythm) focus on organizing, optimizing, and tracking the development process. The cadences indicated below are interdependent as represented in the figure above - some meetings inform others, while feedback from other meetings drive changes to the process. Strategy Review (Release Management) - Are the Backlog priorities still correct? Is the Release Roadmap up to date? Play: Link to Play below in the Appendix Overview: A Strategy Review session is held to periodically review the current status of the development effort as well as revise the Release Roadmap to ensure that it correctly reflects current value and priorities. The question answered during this meeting is: Are we doing the right thing? The Release Management process described before represents an example of the events and timelines reflected within this cadence. Example cadence : Quarterly or based on release period - should look at current and next release and how the PB reflects that plan / strategy. Stand-ups - Focused on the Kanban board and managing workflow Play: Link to Play below in the Appendix Overview: While the focus of the Scrum stand-up meeting is on the individual work and how it relates to the team's ability to meet sprint (time-boxed) goals (What did I do yesterday? What will I do today? What is impeding my work?), the focus of the stand-up for Kanban is managing the flow of work items among the Development Team resources to make it as efficient as possible (managing workflow to minimize the time any piece of work is in any stage of development). While the stand-up can be done daily, it is not a requirement of Kanban as the trigger for work to be pulled into the next column is automatic (completion of the column policy for that work item and next column resource availability). The cadence recommendation of a week is a goal - the team will most likely start out in a daily cadence until the workflow is better understood. Example cadence : Initially daily. Can evolve to once a week to coordinate the efforts of the team and resolve work-in-progress bottlenecks. Replenishment Meeting - Similar to the Sprint planning sessions - these meetings ensure that the work items which will be brought into the development stream are prioritized and \"ready\" to be worked on. Play: Link to Play below in the Appendix Overview: The replenishment meeting ensures that there is \"ready\" work prioritized in the backlog to be brought into the workflow. The battle rhythm for when these meetings are necessary are dependent on the workflow (the recommendation is to start by having this monthly and then determine whether it should happen more or less often). It should take the work that the government has identified (refined down to the work item level) and ensure that the work is \"ready\" to enter into the workflow. It is the last step in determining which prioritized work is up next and ensuring that the work still adds value to the government. Example cadence : Once every month (or as necessary to ensure that there is a \"ready\" backlog of work to feed the development stream). Delivery Planning Meeting - This meeting coordinates product hand-off based on current work delivered between multiple entities: Developers, Product Owners, Testers, Trainers, Users (can be facilitated with establishment of Continuous Delivery process and structure). Play: Link to Play below in the Appendix Overview: The Delivery Planning meeting is a discussion between multiple entities of the results of the actual workflow (while the Strategy Meeting establishes a plan, the Delivery Planning Meeting looks at the reality). An assessment is made of the Kanban board to determine what work in progress will be completed by a specific time to pass it on to the next entity in the chain. It is about establishing expectation management so that external agencies can plan for events which are dependent on the contents of the delivery. These include: - PMO ensuring that reviewers are ready to conduct the Physical Configuration Audits and appropriate document reviews, - The LDTO agency coordinating testing events based on specific RALOT. - Users providing timely feedback; - Functionals coordinating training events so that released functionality into production can be used. Example cadence : Once every month to update downfield receivers of the product (highly dependent on the release timeline - i.e. a release once a quarter might only require this meeting once a quarter). Operations Review - How do we improve the process from the organization's point of view Play: Link to Play below in the Appendix Overview: This review is similar to a Scrum retrospective (how to improve the process) but looks at the release management system from an organizational point of view. It looks at improving the entire system as it relates to incorporating the Kanban team workflow methodology into the government review and release system. While the Delivery Planning Meeting focuses on coordinating the efficient release of the product, this meeting is about improving the release process based on lessons learned. It looks back on each release from the perspective of all stakeholders to continuously improve the system for the next release. Example cadence : Once every 3 months (or by exception) following the Release Delivery Service Delivery Review - Review of the release product from the user's point of view. Play: Link to Play below in the Appendix Overview: This review is conducted to determine if the items being produced are actually meeting the user's needs. The Service Delivery Review focuses on what was produced from the end-user's point of view and whether it fulfills their functional needs as well as quality standards. It provides a direct feedback chain from the end user to allow the PMO PO insight into requirement and prioritization changes, as well as the development team to understand better how their product is being viewed by the users of the system (including their satisfaction and concerns). Example cadence: Once every 3 months (or by exception) following the Release Delivery. Frequency will vary based on user first look events or actual releases to the field - need to provide end-user enough time to review delivered product. Risk Review - Done as necessary to review all identified risks to determine the status of risk mitigation measures and issue / impediment resolution. Play: Link to Play below in the Appendix Overview: The Risk Review discusses the probability and impact of planning assumptions being wrong and what steps the PMO and development team are taking to minimize those risks. It focuses on what steps are being taken to minimize the probability of false assumptions impacting delivery cost or schedule. This review focuses on reduces uncertainties in the system in order to establish predictability and reliability in the system which enables trust to form within the different organizational entities including the PMO, development team, external testers, users, etc. Example cadence : Once every 2 weeks (or combined with PMO Risk Management Meeting)","title":"5.3 Kanban"},{"location":"agile/5-3-kanban/#53-kanban","text":"Considerations for establishing a Kanban process: Establish the Kanban Board - Visualize the workflow to organize, optimize, and track the work item development progress (allows for transparency of status) Limit Work in Progress (WIP) - Kanban is about flow. The more items in progress, the more risk of nothing getting done. It is better to have a limited WIP that emphasizes the Lean pull system. That is, when a resource has completed work on a work item and moved it to the next column, s/he pulls the next work item from the column before it. A starting rule for each policy column is limit WIP to amount of column resources + 1 (i.e. for a development column - the WIP would be the number of developers on the team plus one - that allows for a developer who hits an impediment to stop work on that work item and pull another one from those completed in the previous column while the Agile Lead works to remove the impediment. This allows for the flow to continue). Key points in Kanban: Minimize Lead Time (the time a work item is in any specific stage / column) Minimize Cycle Time (the time for the entire work flow cycle to complete for a single work item) - since establishing a cycle time allows for estimation of work items, but not all work items are the same complexity - one methodology for differentiating the work item complexity is to provide multiple swim lanes for tracking work progress - One for work items which have an expected cycle time of 1 week One for 2-week items One for 1-month items Optional - an expedited swim lane (similar to the express lane at a supermarket) for high-priority work items that should take precedence over all others (this will be mentioned again as a strategy for using Kanban to manage Operations and Maintenance work items) Manage flow - The Agile Lead works with the development team to ensure that workflow issues are identified and resolved (i.e. if there is a backup in the workflow, an additional resource may be needed to relieve pressure on the bottleneck column) Make process policies explicit - Each column on the Kanban Board should have an explicit policy which indicates when the work in that column is complete (policy = that column's \"Definition of Done\") Improve the process - In collaboration between the development team and PMO - there should be a focus on continuous improvement of the process (re-define the policies, determine whether columns should be added, determine whether columns should be deleted) While Scrum focuses on a cross-functional team, Kanban focuses more on aligning the resources with the policies. A simple representation of a Kanban Board (this also provides a quick start-up template for HIA products) is: Templated steps / policies for an initial Development Team Kanban board: Backlog : PMO PO prioritizes the Work Items in the Backlog Identify : Development Team Business Analyst in coordination with the PMO PO will identify / refine the description and acceptance criteria for the work item PO Approved : PMO PO will approve the description and acceptance criteria (Confirms that the work meets functional requirements - think mini-PDR) Design : Development Team's Senior Developer will complete draft solution design Cyber Assessment : Developer's cybersecurity representative (ICW PMO Cybersecurity) will complete security assessment of solution PMO Engineer Approved : PMO Engineer will approve design of solution (Confirm that solution meets technical requirements - think mini-CDR) - work item now \"ready\" for development. Develop / Implement : Developers implement the solution Test : Development team testers and cybersecurity will conduct testing and update documentation as needed Pending Acceptance : Development team will demonstrate completed work item functionality to PMO Done : PMO will accept work item as complete (meets the Definition of Done) An Operations and Maintenance team provides an example of a project built for Kanban. The backlog would contain the ticket items requiring resolution with the priority of work established by a government Product Owner. This approach allows refinement of last minute prioritized work items identified by system users through the workflow's column policies identified above (the process of making the work item \"ready\" is captured within the workflow). Also, if the Kanban board swim lanes are setup as described above, the board will contain an expedite lane for flowing through critical fixes requiring short duration resolution. (Note: Another example would be the cybersecurity team handling the implementation of required TCNO/IAVAs.)","title":"5.3 Kanban"},{"location":"agile/5-3-kanban/#531-kanban-cadences","text":"While Scrum Events focus on execution of the time-boxed process, the key Kanban events (cadences - named for their formal rhythm) focus on organizing, optimizing, and tracking the development process. The cadences indicated below are interdependent as represented in the figure above - some meetings inform others, while feedback from other meetings drive changes to the process. Strategy Review (Release Management) - Are the Backlog priorities still correct? Is the Release Roadmap up to date? Play: Link to Play below in the Appendix Overview: A Strategy Review session is held to periodically review the current status of the development effort as well as revise the Release Roadmap to ensure that it correctly reflects current value and priorities. The question answered during this meeting is: Are we doing the right thing? The Release Management process described before represents an example of the events and timelines reflected within this cadence. Example cadence : Quarterly or based on release period - should look at current and next release and how the PB reflects that plan / strategy. Stand-ups - Focused on the Kanban board and managing workflow Play: Link to Play below in the Appendix Overview: While the focus of the Scrum stand-up meeting is on the individual work and how it relates to the team's ability to meet sprint (time-boxed) goals (What did I do yesterday? What will I do today? What is impeding my work?), the focus of the stand-up for Kanban is managing the flow of work items among the Development Team resources to make it as efficient as possible (managing workflow to minimize the time any piece of work is in any stage of development). While the stand-up can be done daily, it is not a requirement of Kanban as the trigger for work to be pulled into the next column is automatic (completion of the column policy for that work item and next column resource availability). The cadence recommendation of a week is a goal - the team will most likely start out in a daily cadence until the workflow is better understood. Example cadence : Initially daily. Can evolve to once a week to coordinate the efforts of the team and resolve work-in-progress bottlenecks. Replenishment Meeting - Similar to the Sprint planning sessions - these meetings ensure that the work items which will be brought into the development stream are prioritized and \"ready\" to be worked on. Play: Link to Play below in the Appendix Overview: The replenishment meeting ensures that there is \"ready\" work prioritized in the backlog to be brought into the workflow. The battle rhythm for when these meetings are necessary are dependent on the workflow (the recommendation is to start by having this monthly and then determine whether it should happen more or less often). It should take the work that the government has identified (refined down to the work item level) and ensure that the work is \"ready\" to enter into the workflow. It is the last step in determining which prioritized work is up next and ensuring that the work still adds value to the government. Example cadence : Once every month (or as necessary to ensure that there is a \"ready\" backlog of work to feed the development stream). Delivery Planning Meeting - This meeting coordinates product hand-off based on current work delivered between multiple entities: Developers, Product Owners, Testers, Trainers, Users (can be facilitated with establishment of Continuous Delivery process and structure). Play: Link to Play below in the Appendix Overview: The Delivery Planning meeting is a discussion between multiple entities of the results of the actual workflow (while the Strategy Meeting establishes a plan, the Delivery Planning Meeting looks at the reality). An assessment is made of the Kanban board to determine what work in progress will be completed by a specific time to pass it on to the next entity in the chain. It is about establishing expectation management so that external agencies can plan for events which are dependent on the contents of the delivery. These include: - PMO ensuring that reviewers are ready to conduct the Physical Configuration Audits and appropriate document reviews, - The LDTO agency coordinating testing events based on specific RALOT. - Users providing timely feedback; - Functionals coordinating training events so that released functionality into production can be used. Example cadence : Once every month to update downfield receivers of the product (highly dependent on the release timeline - i.e. a release once a quarter might only require this meeting once a quarter). Operations Review - How do we improve the process from the organization's point of view Play: Link to Play below in the Appendix Overview: This review is similar to a Scrum retrospective (how to improve the process) but looks at the release management system from an organizational point of view. It looks at improving the entire system as it relates to incorporating the Kanban team workflow methodology into the government review and release system. While the Delivery Planning Meeting focuses on coordinating the efficient release of the product, this meeting is about improving the release process based on lessons learned. It looks back on each release from the perspective of all stakeholders to continuously improve the system for the next release. Example cadence : Once every 3 months (or by exception) following the Release Delivery Service Delivery Review - Review of the release product from the user's point of view. Play: Link to Play below in the Appendix Overview: This review is conducted to determine if the items being produced are actually meeting the user's needs. The Service Delivery Review focuses on what was produced from the end-user's point of view and whether it fulfills their functional needs as well as quality standards. It provides a direct feedback chain from the end user to allow the PMO PO insight into requirement and prioritization changes, as well as the development team to understand better how their product is being viewed by the users of the system (including their satisfaction and concerns). Example cadence: Once every 3 months (or by exception) following the Release Delivery. Frequency will vary based on user first look events or actual releases to the field - need to provide end-user enough time to review delivered product. Risk Review - Done as necessary to review all identified risks to determine the status of risk mitigation measures and issue / impediment resolution. Play: Link to Play below in the Appendix Overview: The Risk Review discusses the probability and impact of planning assumptions being wrong and what steps the PMO and development team are taking to minimize those risks. It focuses on what steps are being taken to minimize the probability of false assumptions impacting delivery cost or schedule. This review focuses on reduces uncertainties in the system in order to establish predictability and reliability in the system which enables trust to form within the different organizational entities including the PMO, development team, external testers, users, etc. Example cadence : Once every 2 weeks (or combined with PMO Risk Management Meeting)","title":"5.3.1 Kanban Cadences"},{"location":"agile/5-4-xp/","text":"5.4 Extreme Programming (XP) XP's primary contribution to the software development world is an interdependent collection of engineering practices that teams can use to be more effective and produce higher quality code. Many teams adopting agile start by using a different framework and when they identify the need for more?disciplined engineering practices they adopt several if not all of the engineering practices espoused by XP. The recommended methodology here is to begin with the Scrum Framework Events listed before and then adopt as necessary the XP engineering practices which enhance the process. General differences between Scrum and XP: Scrum uses sprints which are normally 2-4 weeks long. XP uses iterations which are normally 1-2 weeks long. Sprint backlogs are normally sacred while XP embraces more flexibility if a higher priority work item comes in (understanding that it will replace an equivalent work item already accepted into the iteration). If the team identifies some stories that they are unable to estimate because they don't understand all of the technical considerations involved, they can introduce a spike to do some focused research on that particular story or a common aspect of multiple stories. Spikes are short, time-boxed time frames set aside for the purposes of doing research on a particular aspect of the project. Spikes can occur before regular iterations start or alongside ongoing iterations. XP focuses on practice excellence. The method prescribes a small number of absolutely essential practices and encourages teams to perform those practices as good as they possibly can, almost to the extreme. This is where the name comes from. Not because the practices themselves are necessarily radical rather that teams continuously focus so intently on continuously improving their ability to perform those few practices. With that in mind, the initial recommended XP methodology will focus on the following initial events with selected engineering practices being employed during execution of these events: Product Backlog Refinement Iteration Planning Iteration Iteration Review Iteration Retrospective The core of XP is the interconnected set of software development practices listed below. While it is possible to do these practices in isolation, many teams have found some practices reinforce the others and should be done in conjunction to fully eliminate the risks you often face in software development. The XP Practices have changed a bit since they were initially introduced. The original twelve practices are listed below. If you would like more information about how these practices were originally described, you can visit? http://ronjeffries.com/xprog/what-is-extreme-programming/. Planning Games Small Releases Metaphor Simple Design Testing Refactoring Pair Programming Collective Ownership Continuous Integration 40-hour week On-site Customer Coding Standard Below are the descriptions of the practices as described in the second edition of Extreme Programming Explained - Embrace?Change.?These descriptions include refinements based on experiences of many who practice extreme programming and reflect a more practical set of practices. Sit Together . Since communication is one of the five values of XP, and most people agree that face to face conversation is the best form of communication, have the team sit together in the same space without barriers to communication, such as cubicle walls (note that there are always exception to this rule for remote workers - but these exceptions should be minimized). Whole Team . A cross functional group of people with the necessary roles for a product form a single team. This means people with a need as well as all the people who play some part in satisfying that need all work together on a daily basis to accomplish a specific outcome. Informative Workspace . Set up the team space to facilitate face to face communication. Allow people to have some privacy when they need it, and make the work of the team transparent to each other and to interested parties outside the team. Utilize Information Radiators to actively communicate up-to-date information. Energized Work . Teams are most effective at software development when they are focused and free from distractions.?Energized work means taking steps to make sure the team is able physically and mentally to get into a focused state. This means do not overwork the team (surges should be minimized). Pair Programming . Pair Programming means all production software is developed by two people sitting at the same machine. The idea behind this practice is that two brains and four eyes are better than one brain and two eyes. This practice effectively provides a continuous code review and quicker response to nagging problems that may impede one person doing it on their own.?Teams that have used pair programming have found that it improves quality and does not actually take twice as long because they are able to work through problems quicker, and they stay more focused on the task at hand, thereby creating less code to accomplish the same thing. Stories . Describe what the product should do in terms meaningful to customers and users. These?stories?are intended to be short descriptions of things users want to be able to do with the product that can be used for planning and serve as reminders for more detailed conversations when the team gets around to realizing that particular story. Weekly Cycle . The Weekly Cycle is synonymous to?an?iteration. In the case of XP, the team meets on the first day of the week to reflect on progress to date, the customer picks the stories they would like delivered in that week, and the team determines how they will approach those stories. The goal by the end of the week is to have running tested features that realize the selected stories. The intent behind the time boxed delivery period is to produce something to show to the PMO for feedback. Quarterly Cycle . The Quarterly Cycle is synonymous to a release. The purpose is to keep the detailed work of each weekly cycle in context of the overall project. The Customer lays out the overall plan for the team in terms of features desired within a particular quarter, which provides the team with a view of the forest while they are in the trees, and it also helps the customer to work with other stakeholders who may need some idea of when features will be available. Remember when planning a quarterly cycle the information about any particular story is at a relatively high level, the order of story delivery within a quarterly cycle can change and the stories included in the quarterly cycle may change. Revisiting the plan following each iteration provides an opportunity to keep everyone informed as soon as those changes become apparent to keep surprises to a minimum. Slack . The idea behind slack in XP terms is to add some low priority tasks or stories in your weekly and quarterly cycles that can be dropped if the team gets behind on more important tasks or stories. Put another way, account for the inherent variability in estimates to make sure you leave yourself a good chance of meeting your forecasts. Ten-Minute Build . The goal with the Ten-Minute Build is to automatically build the whole system and run all of the tests in ten minutes. The founders of XP suggested a 10 minute time frame because if a team has a build that takes longer than that, it is less likely to be run on a frequent basis, thus introducing longer time between errors. This practice encourages the team to automate the build and test process to run on a regular basis. This practice supports the practice of Continuous Integration and is supported by the practice of Test First Development. Continuous Integration . Continuous Integration?is a practice where code changes are immediately tested when they are added to a larger code base. The benefit of this practice is the development team can catch and fix integration issues sooner. Most teams dread the code integration step because of the inherent discovery of conflicts and issues that result. Most teams take the approach \"If it hurts, avoid it as long as possible\". Practitioners of XP suggest \"if it hurts, do it more often\". The reasoning behind that approach is that if the development team experiences problems every time they integrate code, the more frequently they integrate, the smaller the changes and the easier to determine the source of the problem. This practice requires is highly dependent on Ten Minute Build and Test First Development. Test-First Programming . Instead of following the normal path of: Develop code -> write tests -> run tests The practice of Test-First Programming follows the path of Test Driven Development (TDD): Write failing automated test -> run failing test -> develop code to make test pass -> run test -> repeat As with Continuous Integration, Test-First Programming reduces the feedback cycle for developers to identify and resolve issues, thereby decreasing the number of bugs that get introduced into production. Incremental Design . The practice of?Incremental Design?suggests that the team does a little bit of work up front to understand the proper breadth-wise perspective of the system design, and then dives into the details of a particular aspect of that design when it delivers specific features. This approach reduces the cost of changes and allows the team to make design decisions when necessary based on the most current information available. The practice of Refactoring was originally listed among the 12 core, but was incorporated into the practice of Incremental Design. Refactoring is an excellent practice to use to keep the design simple, and one of the most recommended uses of refactoring is to remove duplication of processes. The biggest impact on instituting this practice is determining the scope of the governments formal design reviews (PDR/CDR) as this practice provides the inputs to these activities during the actual development iteration. 5.4.1 XP Process Cycle 5.4.2 XP Events (Similar to Scrum - minor revisions, associated engineering practices) Using the Scrum framework for the baseline of events to start the XP process, the following events are summarized here to avoid redundancy with the Scrum section above. Only key differences will be presented here along with a recommendation of which engineering processes could be incorporated into these events (note that many of the engineering practices can be employed in multiple events - below is only one recommendation): a. Product Backlog Refinement - Are the Backlog priorities still correct? Is the Release Roadmap up to date? Play: Link to Play below in the Appendix Overview : Backlog refinement focuses on providing a prioritized set of \"ready\" (see Definition of Ready above) user stories for the development team to accept into the next iteration as well as a \"ready\" reserve set of user stories which are available to the team during the iteration in case additional capacity becomes available. XP differences : Introduction of Spike to do focused research on stories which require more clarity. Applicable Engineering Practices : Quarterly Cycle, Stories, Incremental Design b. Iteration Planning - Are the Backlog priorities still correct? Is the Release Roadmap up to date? Play: Link to Play below in the Appendix Overview : During this meeting, the Product Owner will review the current list of Ready user stories to ensure that they are still valuable (relevant), as well as reprioritize the Backlog based on existing value as necessary (this facilitates the team's acceptance of work into the iteration as they simply take user stories from the top of the list within their established iteration capacity. XP differences : XP uses iterations which are normally 1-2 weeks long. Applicable Engineering Practices : Weekly Cycle, Slack c. Iteration - Are the Backlog priorities still correct? Is the Release Roadmap up to date? Play: Link to Play below in the Appendix Overview : Iteration execution consists of developers completing their work, testers executing the tests developed prior to the iteration (with the addition of any necessary negative test cases), and BAs and testers reviewing the completed work with the Product Owner in order to receive early feedback or formal closure of the story as meeting the teams \"Definition of Done\". XP differences : Focus of XP is on Test-First Programming and Pair Programming in the execution stage (tests fail at the beginning because the code is notdeveloped - working in pairs, the developers develop the code to pass the tests, when development testing is done - the team testers do a final verification of the initial tests along with any additional negative tests they have written. This process can also be facilitated by implementing a Continuous Integration system to maximize the use of automated testing. Applicable Engineering Practices : Sit Together, Informative Workspace, Energized Work, Pair Programming, Test-First Programming, Continuous Integration, Ten-Minute Build d. Iteration Review - Are the Backlog priorities still correct? Is the Release Roadmap up to date? Play: Link to Play below in the Appendix Overview : The iteration review is a presentation to Stakeholders of the completed increment based on acceptance by the Product Owner. The key to the review is that it is interactive between the Stakeholders and Development Team - the focus being to provide a feedback mechanism to improve the product. XP differences : None. Applicable Engineering Practices : None e. Iteration Retrospective - Are the Backlog priorities still correct? Is the Release Roadmap up to date? Play: Link to Play below in the Appendix Overview : This is where the Product Owners and the Development Team focus on improving the Agile process. Critical to understanding the concept of the retrospective is that learning always occurs during these events whether the iteration was successful or not. XP differences : Focus will not only be on events - it will include the team's selected engineering practices. Applicable Engineering Practices : Whole Team","title":"5.4 Extreme Programming"},{"location":"agile/5-4-xp/#54-extreme-programming-xp","text":"XP's primary contribution to the software development world is an interdependent collection of engineering practices that teams can use to be more effective and produce higher quality code. Many teams adopting agile start by using a different framework and when they identify the need for more?disciplined engineering practices they adopt several if not all of the engineering practices espoused by XP. The recommended methodology here is to begin with the Scrum Framework Events listed before and then adopt as necessary the XP engineering practices which enhance the process. General differences between Scrum and XP: Scrum uses sprints which are normally 2-4 weeks long. XP uses iterations which are normally 1-2 weeks long. Sprint backlogs are normally sacred while XP embraces more flexibility if a higher priority work item comes in (understanding that it will replace an equivalent work item already accepted into the iteration). If the team identifies some stories that they are unable to estimate because they don't understand all of the technical considerations involved, they can introduce a spike to do some focused research on that particular story or a common aspect of multiple stories. Spikes are short, time-boxed time frames set aside for the purposes of doing research on a particular aspect of the project. Spikes can occur before regular iterations start or alongside ongoing iterations. XP focuses on practice excellence. The method prescribes a small number of absolutely essential practices and encourages teams to perform those practices as good as they possibly can, almost to the extreme. This is where the name comes from. Not because the practices themselves are necessarily radical rather that teams continuously focus so intently on continuously improving their ability to perform those few practices. With that in mind, the initial recommended XP methodology will focus on the following initial events with selected engineering practices being employed during execution of these events: Product Backlog Refinement Iteration Planning Iteration Iteration Review Iteration Retrospective The core of XP is the interconnected set of software development practices listed below. While it is possible to do these practices in isolation, many teams have found some practices reinforce the others and should be done in conjunction to fully eliminate the risks you often face in software development. The XP Practices have changed a bit since they were initially introduced. The original twelve practices are listed below. If you would like more information about how these practices were originally described, you can visit? http://ronjeffries.com/xprog/what-is-extreme-programming/. Planning Games Small Releases Metaphor Simple Design Testing Refactoring Pair Programming Collective Ownership Continuous Integration 40-hour week On-site Customer Coding Standard Below are the descriptions of the practices as described in the second edition of Extreme Programming Explained - Embrace?Change.?These descriptions include refinements based on experiences of many who practice extreme programming and reflect a more practical set of practices. Sit Together . Since communication is one of the five values of XP, and most people agree that face to face conversation is the best form of communication, have the team sit together in the same space without barriers to communication, such as cubicle walls (note that there are always exception to this rule for remote workers - but these exceptions should be minimized). Whole Team . A cross functional group of people with the necessary roles for a product form a single team. This means people with a need as well as all the people who play some part in satisfying that need all work together on a daily basis to accomplish a specific outcome. Informative Workspace . Set up the team space to facilitate face to face communication. Allow people to have some privacy when they need it, and make the work of the team transparent to each other and to interested parties outside the team. Utilize Information Radiators to actively communicate up-to-date information. Energized Work . Teams are most effective at software development when they are focused and free from distractions.?Energized work means taking steps to make sure the team is able physically and mentally to get into a focused state. This means do not overwork the team (surges should be minimized). Pair Programming . Pair Programming means all production software is developed by two people sitting at the same machine. The idea behind this practice is that two brains and four eyes are better than one brain and two eyes. This practice effectively provides a continuous code review and quicker response to nagging problems that may impede one person doing it on their own.?Teams that have used pair programming have found that it improves quality and does not actually take twice as long because they are able to work through problems quicker, and they stay more focused on the task at hand, thereby creating less code to accomplish the same thing. Stories . Describe what the product should do in terms meaningful to customers and users. These?stories?are intended to be short descriptions of things users want to be able to do with the product that can be used for planning and serve as reminders for more detailed conversations when the team gets around to realizing that particular story. Weekly Cycle . The Weekly Cycle is synonymous to?an?iteration. In the case of XP, the team meets on the first day of the week to reflect on progress to date, the customer picks the stories they would like delivered in that week, and the team determines how they will approach those stories. The goal by the end of the week is to have running tested features that realize the selected stories. The intent behind the time boxed delivery period is to produce something to show to the PMO for feedback. Quarterly Cycle . The Quarterly Cycle is synonymous to a release. The purpose is to keep the detailed work of each weekly cycle in context of the overall project. The Customer lays out the overall plan for the team in terms of features desired within a particular quarter, which provides the team with a view of the forest while they are in the trees, and it also helps the customer to work with other stakeholders who may need some idea of when features will be available. Remember when planning a quarterly cycle the information about any particular story is at a relatively high level, the order of story delivery within a quarterly cycle can change and the stories included in the quarterly cycle may change. Revisiting the plan following each iteration provides an opportunity to keep everyone informed as soon as those changes become apparent to keep surprises to a minimum. Slack . The idea behind slack in XP terms is to add some low priority tasks or stories in your weekly and quarterly cycles that can be dropped if the team gets behind on more important tasks or stories. Put another way, account for the inherent variability in estimates to make sure you leave yourself a good chance of meeting your forecasts. Ten-Minute Build . The goal with the Ten-Minute Build is to automatically build the whole system and run all of the tests in ten minutes. The founders of XP suggested a 10 minute time frame because if a team has a build that takes longer than that, it is less likely to be run on a frequent basis, thus introducing longer time between errors. This practice encourages the team to automate the build and test process to run on a regular basis. This practice supports the practice of Continuous Integration and is supported by the practice of Test First Development. Continuous Integration . Continuous Integration?is a practice where code changes are immediately tested when they are added to a larger code base. The benefit of this practice is the development team can catch and fix integration issues sooner. Most teams dread the code integration step because of the inherent discovery of conflicts and issues that result. Most teams take the approach \"If it hurts, avoid it as long as possible\". Practitioners of XP suggest \"if it hurts, do it more often\". The reasoning behind that approach is that if the development team experiences problems every time they integrate code, the more frequently they integrate, the smaller the changes and the easier to determine the source of the problem. This practice requires is highly dependent on Ten Minute Build and Test First Development. Test-First Programming . Instead of following the normal path of: Develop code -> write tests -> run tests The practice of Test-First Programming follows the path of Test Driven Development (TDD): Write failing automated test -> run failing test -> develop code to make test pass -> run test -> repeat As with Continuous Integration, Test-First Programming reduces the feedback cycle for developers to identify and resolve issues, thereby decreasing the number of bugs that get introduced into production. Incremental Design . The practice of?Incremental Design?suggests that the team does a little bit of work up front to understand the proper breadth-wise perspective of the system design, and then dives into the details of a particular aspect of that design when it delivers specific features. This approach reduces the cost of changes and allows the team to make design decisions when necessary based on the most current information available. The practice of Refactoring was originally listed among the 12 core, but was incorporated into the practice of Incremental Design. Refactoring is an excellent practice to use to keep the design simple, and one of the most recommended uses of refactoring is to remove duplication of processes. The biggest impact on instituting this practice is determining the scope of the governments formal design reviews (PDR/CDR) as this practice provides the inputs to these activities during the actual development iteration.","title":"5.4 Extreme Programming (XP)"},{"location":"agile/5-4-xp/#541-xp-process-cycle","text":"","title":"5.4.1 XP Process Cycle"},{"location":"agile/5-4-xp/#542-xp-events-similar-to-scrum-minor-revisions-associated-engineering-practices","text":"Using the Scrum framework for the baseline of events to start the XP process, the following events are summarized here to avoid redundancy with the Scrum section above. Only key differences will be presented here along with a recommendation of which engineering processes could be incorporated into these events (note that many of the engineering practices can be employed in multiple events - below is only one recommendation): a. Product Backlog Refinement - Are the Backlog priorities still correct? Is the Release Roadmap up to date? Play: Link to Play below in the Appendix Overview : Backlog refinement focuses on providing a prioritized set of \"ready\" (see Definition of Ready above) user stories for the development team to accept into the next iteration as well as a \"ready\" reserve set of user stories which are available to the team during the iteration in case additional capacity becomes available. XP differences : Introduction of Spike to do focused research on stories which require more clarity. Applicable Engineering Practices : Quarterly Cycle, Stories, Incremental Design b. Iteration Planning - Are the Backlog priorities still correct? Is the Release Roadmap up to date? Play: Link to Play below in the Appendix Overview : During this meeting, the Product Owner will review the current list of Ready user stories to ensure that they are still valuable (relevant), as well as reprioritize the Backlog based on existing value as necessary (this facilitates the team's acceptance of work into the iteration as they simply take user stories from the top of the list within their established iteration capacity. XP differences : XP uses iterations which are normally 1-2 weeks long. Applicable Engineering Practices : Weekly Cycle, Slack c. Iteration - Are the Backlog priorities still correct? Is the Release Roadmap up to date? Play: Link to Play below in the Appendix Overview : Iteration execution consists of developers completing their work, testers executing the tests developed prior to the iteration (with the addition of any necessary negative test cases), and BAs and testers reviewing the completed work with the Product Owner in order to receive early feedback or formal closure of the story as meeting the teams \"Definition of Done\". XP differences : Focus of XP is on Test-First Programming and Pair Programming in the execution stage (tests fail at the beginning because the code is notdeveloped - working in pairs, the developers develop the code to pass the tests, when development testing is done - the team testers do a final verification of the initial tests along with any additional negative tests they have written. This process can also be facilitated by implementing a Continuous Integration system to maximize the use of automated testing. Applicable Engineering Practices : Sit Together, Informative Workspace, Energized Work, Pair Programming, Test-First Programming, Continuous Integration, Ten-Minute Build d. Iteration Review - Are the Backlog priorities still correct? Is the Release Roadmap up to date? Play: Link to Play below in the Appendix Overview : The iteration review is a presentation to Stakeholders of the completed increment based on acceptance by the Product Owner. The key to the review is that it is interactive between the Stakeholders and Development Team - the focus being to provide a feedback mechanism to improve the product. XP differences : None. Applicable Engineering Practices : None e. Iteration Retrospective - Are the Backlog priorities still correct? Is the Release Roadmap up to date? Play: Link to Play below in the Appendix Overview : This is where the Product Owners and the Development Team focus on improving the Agile process. Critical to understanding the concept of the retrospective is that learning always occurs during these events whether the iteration was successful or not. XP differences : Focus will not only be on events - it will include the team's selected engineering practices. Applicable Engineering Practices : Whole Team","title":"5.4.2 XP Events (Similar to Scrum - minor revisions, associated engineering practices)"},{"location":"agile/5-5-team/","text":"5.5 Part III - Establish the Team The following section provides a general overview of the roles required for effective execution of the different agile methodologies presented in this playbook. Note that these are \"roles\", and, based on the team size, can be implemented by one or more people. Also, the same person can have multiple roles (i.e. a BA and a Scrum Master) though this might not lend to optimal effectiveness. 5.5.1 Scrum Roles and Responsibilities The relationship between development team members and the PMO is critical as understanding between the two will breed the trust necessary for successful execution of an agile process. Product Owners : These are representatives of the PMO with the necessary functional knowledge of the system to be able to collaborate effectively with the development team. One of the key aspects within the government environment is that often, the Product Owner is a collection of individuals representing different perspective (Program Management, Functionality, Engineering, and Cybersecurity are examples of these perspectives). Whether there are 1 or more people representing this position, individual or group must have the ability to make decisions on the requirements to effectively collaborate with the business analyst and the development team. Note that a government Project Manager may fulfill the role of the Product Owner, but they must have the functional knowledge to allow for effective clarification of requirements to the development team. POs are responsible for establishing: a. Requirements / Acceptance Criteria b. Priorities c. Clarification on Requirements for the Development Team d. Approval of \"Done\" for stories. Development Teams : Formed around the specific projects, their purpose is to create quality code that produces value for the government. They have the following components (the size and composition of each team will vary based on the development requirements within a release): a. Scrum Master - They are the Process Advisor to the PO. b. Business Analyst(s) - Responsible for gathering the requirements and translating them into documentation which not only reflects the intent of the PObut are also understandable and executable by the developers within the team. c. Developers (General) - Responsible for - focus is functionality to the users. e. Tester / Quality Assurance (QA) - Responsible for implementation of quality within the team. 5.5.2 Kanban Roles and Responsibilities For Scrum, the Product Owner, Scrum Master and Development team roles must be assigned. In Kanban, the workflow dictates the role requirements. The refinement work can be done prior to entering the workflow (thus requiring a government Product Owner and a team Business Analyst) or the work can be part of the workflow (i.e. a column for refinement). Depending on the policies of the Kanban board workflow, resources will be allocated which can work on the work items within the column. Optimally, there would be specialists as necessary for the different workflow aspects and a couple of generalists could work to ease bottlenecks within the workflow. Additionally, while an Agile Lead familiar with Kanban would maintain the discipline within the system and would also enable faster optimization of the system, a Project Manager could also serve as the team lead for managing the workflow. However, to provide a checklist to begin with in implementing a Kanban team (and to provide flexibility to adapt to or adopt other methodologies), the recommended roles for a Kanban Team startup would be: Agile Lead / Coach - Similar to Scrum Master above but experienced in the execution of Kanban. Product Owner - Same as Product Owner in Scrum. Development Team Members - Determined as necessary to enable the workflow (i.e. business analyst, developers, testers, cybersecurity personnel as necessary based on the different policies of the Kanban Board). Key here is the team is not required to be cross-functional so can be formed around specialists and generalists as necessary. 5.5.3 XP Roles and Responsibilities Although Extreme Programming specifies engineering practices for the development team to follow, it does not really establish specific roles for the people on the team.?Depending on the reference material on roles in XP, there is either no guidance, or there is a description of how roles typically found in more traditional projects behave on Extreme Programming projects. Here are four most common roles associated with Extreme Programming:? ? The Customer - The Customer role in XP is almost exactly the same as the Product Owner Role in Scrum (same responsibilities stated above). ? ? The Developer - Because XP does not have much need for role definition, everyone on the team (with the exception of the customer and a couple of secondary roles listed below) is labeled a developer. ? The Coach - This role is similar to the Scrum Master role for Scrum. The fundamental difference is that this Agile lead should have experience with XP Practices.? The main value of the coach is that they have gone through it before and can help the team maintain practice discipline and avoid process mistakes.? 5.5.4 Roles and Responsibilities Plays Since the core roles are essentially the same between the different agile frameworks, there will only be one play for each of the roles common to all frameworks. We will not be discussing developers and testers / quality assurance here as these are roles common to IT development environment. However, when drafting job requirements, the following should be added in the preferred qualifications: Experience in agile development methodologies (Scrum, Kanban, and/or XP) The following agile related roles have plays associated with them: Agile Lead (includes Coach and Scrum Master) Product Owner Business Analyst The contents of each role play is the following: Recommended skill sets Recommended qualifications Recommended certifications General Responsibility Overview Link: Appendix B: Key Personnel Plays","title":"5.5 Part III - Establish the Team"},{"location":"agile/5-5-team/#55-part-iii-establish-the-team","text":"The following section provides a general overview of the roles required for effective execution of the different agile methodologies presented in this playbook. Note that these are \"roles\", and, based on the team size, can be implemented by one or more people. Also, the same person can have multiple roles (i.e. a BA and a Scrum Master) though this might not lend to optimal effectiveness.","title":"5.5 Part III - Establish the Team"},{"location":"agile/5-5-team/#551-scrum-roles-and-responsibilities","text":"The relationship between development team members and the PMO is critical as understanding between the two will breed the trust necessary for successful execution of an agile process. Product Owners : These are representatives of the PMO with the necessary functional knowledge of the system to be able to collaborate effectively with the development team. One of the key aspects within the government environment is that often, the Product Owner is a collection of individuals representing different perspective (Program Management, Functionality, Engineering, and Cybersecurity are examples of these perspectives). Whether there are 1 or more people representing this position, individual or group must have the ability to make decisions on the requirements to effectively collaborate with the business analyst and the development team. Note that a government Project Manager may fulfill the role of the Product Owner, but they must have the functional knowledge to allow for effective clarification of requirements to the development team. POs are responsible for establishing: a. Requirements / Acceptance Criteria b. Priorities c. Clarification on Requirements for the Development Team d. Approval of \"Done\" for stories. Development Teams : Formed around the specific projects, their purpose is to create quality code that produces value for the government. They have the following components (the size and composition of each team will vary based on the development requirements within a release): a. Scrum Master - They are the Process Advisor to the PO. b. Business Analyst(s) - Responsible for gathering the requirements and translating them into documentation which not only reflects the intent of the PObut are also understandable and executable by the developers within the team. c. Developers (General) - Responsible for - focus is functionality to the users. e. Tester / Quality Assurance (QA) - Responsible for implementation of quality within the team.","title":"5.5.1 Scrum Roles and Responsibilities"},{"location":"agile/5-5-team/#552-kanban-roles-and-responsibilities","text":"For Scrum, the Product Owner, Scrum Master and Development team roles must be assigned. In Kanban, the workflow dictates the role requirements. The refinement work can be done prior to entering the workflow (thus requiring a government Product Owner and a team Business Analyst) or the work can be part of the workflow (i.e. a column for refinement). Depending on the policies of the Kanban board workflow, resources will be allocated which can work on the work items within the column. Optimally, there would be specialists as necessary for the different workflow aspects and a couple of generalists could work to ease bottlenecks within the workflow. Additionally, while an Agile Lead familiar with Kanban would maintain the discipline within the system and would also enable faster optimization of the system, a Project Manager could also serve as the team lead for managing the workflow. However, to provide a checklist to begin with in implementing a Kanban team (and to provide flexibility to adapt to or adopt other methodologies), the recommended roles for a Kanban Team startup would be: Agile Lead / Coach - Similar to Scrum Master above but experienced in the execution of Kanban. Product Owner - Same as Product Owner in Scrum. Development Team Members - Determined as necessary to enable the workflow (i.e. business analyst, developers, testers, cybersecurity personnel as necessary based on the different policies of the Kanban Board). Key here is the team is not required to be cross-functional so can be formed around specialists and generalists as necessary.","title":"5.5.2 Kanban Roles and Responsibilities"},{"location":"agile/5-5-team/#553-xp-roles-and-responsibilities","text":"Although Extreme Programming specifies engineering practices for the development team to follow, it does not really establish specific roles for the people on the team.?Depending on the reference material on roles in XP, there is either no guidance, or there is a description of how roles typically found in more traditional projects behave on Extreme Programming projects. Here are four most common roles associated with Extreme Programming:? ? The Customer - The Customer role in XP is almost exactly the same as the Product Owner Role in Scrum (same responsibilities stated above). ? ? The Developer - Because XP does not have much need for role definition, everyone on the team (with the exception of the customer and a couple of secondary roles listed below) is labeled a developer. ? The Coach - This role is similar to the Scrum Master role for Scrum. The fundamental difference is that this Agile lead should have experience with XP Practices.? The main value of the coach is that they have gone through it before and can help the team maintain practice discipline and avoid process mistakes.?","title":"5.5.3 XP Roles and Responsibilities"},{"location":"agile/5-5-team/#554-roles-and-responsibilities-plays","text":"Since the core roles are essentially the same between the different agile frameworks, there will only be one play for each of the roles common to all frameworks. We will not be discussing developers and testers / quality assurance here as these are roles common to IT development environment. However, when drafting job requirements, the following should be added in the preferred qualifications: Experience in agile development methodologies (Scrum, Kanban, and/or XP) The following agile related roles have plays associated with them: Agile Lead (includes Coach and Scrum Master) Product Owner Business Analyst The contents of each role play is the following: Recommended skill sets Recommended qualifications Recommended certifications General Responsibility Overview Link: Appendix B: Key Personnel Plays","title":"5.5.4 Roles and Responsibilities Plays"},{"location":"agile/6-1-scrum/","text":"6.1 Scrum 6.1.1 Backlog Refinement Formal Collaboration Session 6.1.2 Sprint Planning 6.1.3 Sprint Execution 6.1.4 Sprint Review 6.1.5 Sprint Retrospective","title":"6.1 Scrum"},{"location":"agile/6-1-scrum/#61-scrum","text":"","title":"6.1 Scrum"},{"location":"agile/6-1-scrum/#611-backlog-refinement-formal-collaboration-session","text":"","title":"6.1.1 Backlog Refinement Formal Collaboration Session"},{"location":"agile/6-1-scrum/#612-sprint-planning","text":"","title":"6.1.2 Sprint Planning"},{"location":"agile/6-1-scrum/#613-sprint-execution","text":"","title":"6.1.3 Sprint Execution"},{"location":"agile/6-1-scrum/#614-sprint-review","text":"","title":"6.1.4 Sprint Review"},{"location":"agile/6-1-scrum/#615-sprint-retrospective","text":"","title":"6.1.5 Sprint Retrospective"},{"location":"agile/6-2-kanban/","text":"6.2 Kanban 6.2.1 Strategy Review/Release Management 6.2.2 Stand-up 6.2.3 Replenishment Meeting 6.2.4 Delivery Planning Meeting 6.2.5 Operations Review 6.2.6 Service Delivery Review 6.2.7 Risk Review","title":"6.2 Kanban"},{"location":"agile/6-2-kanban/#62-kanban","text":"","title":"6.2 Kanban"},{"location":"agile/6-2-kanban/#621-strategy-reviewrelease-management","text":"","title":"6.2.1 Strategy Review/Release Management"},{"location":"agile/6-2-kanban/#622-stand-up","text":"","title":"6.2.2 Stand-up"},{"location":"agile/6-2-kanban/#623-replenishment-meeting","text":"","title":"6.2.3 Replenishment Meeting"},{"location":"agile/6-2-kanban/#624-delivery-planning-meeting","text":"","title":"6.2.4 Delivery Planning Meeting"},{"location":"agile/6-2-kanban/#625-operations-review","text":"","title":"6.2.5 Operations Review"},{"location":"agile/6-2-kanban/#626-service-delivery-review","text":"","title":"6.2.6 Service Delivery Review"},{"location":"agile/6-2-kanban/#627-risk-review","text":"","title":"6.2.7 Risk Review"},{"location":"agile/6-3-xp/","text":"6.3 Extreme Programming 6.3.1 Backlog Refinement Formal Collaboration Session 6.3.2 Iteration Planning 6.3.3 Iteration 6.3.4 Iteration Review 6.3.5 Iteration Retrospective","title":"6.3 Extreme Programming"},{"location":"agile/6-3-xp/#63-extreme-programming","text":"","title":"6.3 Extreme Programming"},{"location":"agile/6-3-xp/#631-backlog-refinement-formal-collaboration-session","text":"","title":"6.3.1 Backlog Refinement Formal Collaboration Session"},{"location":"agile/6-3-xp/#632-iteration-planning","text":"","title":"6.3.2 Iteration Planning"},{"location":"agile/6-3-xp/#633-iteration","text":"","title":"6.3.3 Iteration"},{"location":"agile/6-3-xp/#634-iteration-review","text":"","title":"6.3.4 Iteration Review"},{"location":"agile/6-3-xp/#635-iteration-retrospective","text":"","title":"6.3.5 Iteration Retrospective"},{"location":"agile/7-1-lead/","text":"7.1 Agile Lead","title":"7.1 Agile Lead"},{"location":"agile/7-1-lead/#71-agile-lead","text":"","title":"7.1 Agile Lead"},{"location":"agile/7-2-owner/","text":"7.2 Product Owner","title":"7.2 Product Owner"},{"location":"agile/7-2-owner/#72-product-owner","text":"","title":"7.2 Product Owner"},{"location":"agile/7-3-analyst/","text":"7.3 Business Analyst","title":"7.3 Business Analyst"},{"location":"agile/7-3-analyst/#73-business-analyst","text":"","title":"7.3 Business Analyst"},{"location":"agile/7-appendix-b/","text":"7 APPENDIX B - KEY PERSONNEL PLAYS 7.1 Agile Lead 7.2 Product Owner 7.3 Business Analyst","title":"7 appendix b"},{"location":"agile/7-appendix-b/#7-appendix-b-key-personnel-plays","text":"","title":"7 APPENDIX B - KEY PERSONNEL PLAYS"},{"location":"agile/7-appendix-b/#71-agile-lead","text":"","title":"7.1 Agile Lead"},{"location":"agile/7-appendix-b/#72-product-owner","text":"","title":"7.2 Product Owner"},{"location":"agile/7-appendix-b/#73-business-analyst","text":"","title":"7.3 Business Analyst"},{"location":"agile/8-1-general/","text":"8.1 Agile in General","title":"8.1 Agile in General"},{"location":"agile/8-1-general/#81-agile-in-general","text":"","title":"8.1 Agile in General"},{"location":"agile/8-2-tools/","text":"8.2 Agile Tools 8.2.1 VSTS / TFS Overview: https://msdn.microsoft.com/en-us/library/ms364062(v=vs.80).aspx Backlog Management: Work Mangement: Communication Management: Continuous Integration: 8.2.2 Atlassian (JIRA) Overview: https://confluence.atlassian.com/jirasoftwarecloud/jira-software-overview-779293724.html Backlog Management: Work Management: Communication Management: Continuous Integration:","title":"8.2 Agile Tools"},{"location":"agile/8-2-tools/#82-agile-tools","text":"","title":"8.2 Agile Tools"},{"location":"agile/8-2-tools/#821-vsts-tfs","text":"Overview: https://msdn.microsoft.com/en-us/library/ms364062(v=vs.80).aspx Backlog Management: Work Mangement: Communication Management: Continuous Integration:","title":"8.2.1 VSTS / TFS"},{"location":"agile/8-2-tools/#822-atlassian-jira","text":"Overview: https://confluence.atlassian.com/jirasoftwarecloud/jira-software-overview-779293724.html Backlog Management: Work Management: Communication Management: Continuous Integration:","title":"8.2.2 Atlassian (JIRA)"},{"location":"agile/9-1-appendix-d/","text":"9.1 AGILE TERMINOLOGY Source: https://www.agilealliance.org/agile101/agile-glossary/ A Acceptance Test Driven Development (ATDD) Acceptance Test Driven Development (ATDD) involves team members with different perspectives (customer, development, testing) collaborating to write acceptance tests in advance of implementing the corresponding functionality. Acceptance Testing An acceptance test is a formal description of the behavior of a software product, generally expressed as an example or a usage scenario. A number of different notations and approaches have been proposed for such examples or scenarios. In many cases the aim is that it should be possible to automate the execution of such tests by a software tool, either ad-hoc to the development team or off the shelf. Antipattern Antipatterns are common solutions to common problems where the solution is ineffective and may result in undesired consequences. Automated Build In the context of software development, build refers to the process that converts files and other assets under the developers' responsibility into a software product in its final or consumable form. The build is automated when these steps are repeatable, require no direct human intervention, and can be performed at any time with no information other than what is stored in the source code control repository. B Backlog A backlog is an ordered list of items representing everything that may be needed to deliver a specific outcome. There are different types of backlogs depending on the type of item they contain and the approach being used. Backlog Grooming Backlog grooming is when the PO and some, or all, of the rest of the team refine the backlog on a regular basis to ensure the backlog contains the appropriate items, that they are prioritized, and that the items at the top of the backlog are ready for delivery. Behavior Driven Development (BDD) BDD is a practice where members of the team discuss the expected behavior of a system in order to build a shared understanding of expected functionality. Burndown Chart Burndown charts and burnup charts track the amount of output (in terms of hours, story points, or backlog items) a team has completed across an iteration or a project. Business Agility Business agility is the ability of an organization to sense changes internally or externally and respond accordingly in order to deliver value to its customers. C Collective Ownership Collective code ownership is the explicit convention that every team member can make changes to any code file as necessary: either to complete a development task, to repair a defect, or to improve the code's overall structure. Continuous Deployment Continuous deployment aims to reduce the time elapsed between writing a line of code and making that code available to users in production. To achieve continuous deployment, the team relies on infrastructure that automates and instruments the various steps leading up to deployment, so that after each integration successfully meeting these release criteria, the live application is updated with new code. Continuous Integration Continuous Integration is the practice of merging code changes into a shared repository several times a day in order to release a product version at any moment. This requires an integration procedure which is reproducible and automated. CRC Cards Class Responsibility Collaborator (CRC) Cards are an object oriented design technique teams can use to discuss what a class should know and do and what other classes it interacts with. Customer Development Customer development is a four-step framework that provides a way to use a scientific approach to validate assumptions about your product and business. D Daily Meeting The daily meeting is one of the most commonly practiced Agile techniques and presents opportunity for a team to get together on a regular basis to coordinate their activities. Definition of Done The definition of done is an agreed upon list of the activities deemed necessary to get a product increment, usually represented by a user story, to a done state by the end of a sprint. Definition of Ready Definition of Ready involves creating clear criteria that a user story must meet before being accepted into an upcoming iteration. This is typically based on the INVEST matrix. E Epic An epic is a large user story. Estimation In software development, an \"estimate\" is the evaluation of the effort necessary to carry out a given development task; this is most often expressed in terms of duration. Exploratory Testing Exploratory testing is, more than strictly speaking a \"practice,\" a style or approach to testing software which is often contrasted to \"scripted testing.\" Extreme Programming Extreme Programming (XP) is an agile software development framework that aims to produce higher quality software, and higher quality of life for the development team. XP is the most specific of the agile frameworks regarding appropriate engineering practices for software development. F Facilitation A facilitator is a person who chooses or is given the explicit role of conducting a meeting. Frequent Releases An Agile team frequently releases its product into the hands of end users, listening to feedback, whether critical or appreciative. G Given When Then The Given-When-Then formula is a template intended to guide the writing of acceptance tests for a User Story: (Given) some context, (When) some action is carried out, (Then) a particular set of observable consequences should obtain. H Heartbeat Retrospective The team meets regularly to reflect on the most significant events that occurred since the previous such meeting, and identify opportunities for improvement. I Incremental Development In an Agile context, Incremental Development is when each successive version of a product is usable, and each builds upon the previous version by adding user-visible functionality. Information Radiators \"Information radiator\" is the term for any of a number of visual displays which a team places in a highly visible location, so that all team members can see the latest information at a glance. Integration \"Integration\" (or \"integrating\") refers to any efforts still required for a project team to deliver a product suitable for release as a functional whole. INVEST The acronym INVEST stands for a set of criteria used to assess the quality of a user story. If the story fails to meet one of these criteria, the team may want to reword it. Iteration An iteration is a timebox during which development takes place. The duration may vary from project to project and is usually fixed. Iterative Development Agile projects are iterative insofar as they intentionally allow for \"repeating\" software development activities, and for potentially \"revisiting\" the same work products (the phrase \"planned rework\" is sometimes used; refactoring is a good example). K Kanban The Kanban Method is a means to design, manage and improve flow for knowledge work and allows teams to start where they are to drive evolutionary change. Kanban Board A Kanban Board is a visual workflow tool consisting of multiple columns. Each column represents a different stage in the workflow process. L Lead Time Lead Time is the time between a customer order and delivery. In software development, it can also be the time between a requirement made and its fulfillment. M Milestone Retrospective A Milestone Retrospective is a team's detailed analysis of the project's significant events after a set period of time or at the project's end. Minimum Marketable Feature (MMF) A Minimum Marketable Feature is a small, self-contained feature that can be developed quickly and that delivers significant value to the user. Minimum Viable Product (MVP) A Minimum Viable Product is, as Eric Ries said, the \"version of a new product which allows a team to collect the maximum amount of validated learning about customers with the least effort.\" Mob Programming Mob Programming is a software development approach where the whole team works on the same thing, at the same time, in the same space, and at the same computer. Mock Objects Mock Objects (commonly used in the context of crafting automated unit tests) consist of instantiating a test-specific version of a software component. N Niko-niko Calendar A Niko-niko Calendar is updated daily with each team member's mood for that day. Over time the calendar reveals patterns of change in the moods of the team, or of individual members. P Pair Programming Pair programming consists of two programmers sharing a single workstation (one screen, keyboard and mouse among the pair). Personas Personas are synthetic biographies of fictitious users of the future product. Planning Poker An approach to estimation used by Agile teams. Each team member \"plays\" a card bearing a numerical value corresponding to a point estimation for a user story. Points (estimates in) Agile teams generally prefer to express estimates in units other than the time-honored \"man-hours.\" Possibly the most widespread unit is \"story points.\" Product Owner (PO) The PO is a role created by the Scrum Framework responsible for making sure the team delivers the desired outcome. Project Chartering A high-level summary of the project's key success factors displayed on one wall of the team room as a flipchart-sized sheet of paper. Q Quick Design Session When \"simple design\" choices have far-reaching consequences, two or more developers meet for a quick design session at a whiteboard. R Refactoring Refactoring consists of improving the internal structure of an existing program's source code, while preserving its external behavior. Relative Estimation Relative estimation consists of estimating tasks or user stories by comparison or by grouping of items of equivalent difficulty. Role-feature-reason The \"role-feature-reason\" template is one of the most commonly recommended aids to write user stories: As a ... I want ... So that ... Rule of Simplicity Rules of Simplicity is a set of criteria, in priority order, proposed by Kent Beck to judge whether some source code is \"simple enough.\" S Scrum Scrum is a process framework used to manage product development and other knowledge work. Scrum Master The scrum master is responsible for ensuring the team lives agile values and principles and follows the practices that the team agreed they would use. Scrum of Scrums A technique to scale Scrum up to large groups (over a dozen people), consisting of dividing the groups into Agile teams of 5-10. Sign Up for Tasks Members of an Agile development team normally choose which tasks to work on, rather than being assigned work by a manager. Simple Design A team adopting the \"simple design\" practice bases its software design strategy on a set of \"simple design\" principles. Sprint Planning Sprint planning is an event that occurs at the beginning of a sprint where the team determines the product backlog items they will work on during that sprint. Story Mapping Story mapping consists of ordering user stories along two independent dimensions. Story Splitting Splitting consists of breaking up one user story into smaller ones, while preserving the property that each user story separately has measurable business value. Sustainable Pace The team aims for a work pace that they would be able to sustain indefinitely. T Task Board The most basic form of a task board is divided into three columns labeled \"To Do,\" \"In Progress,\" and \"Done.\" Cards are placed in the columns to reflect the current status of that task. Test Driven Development (TDD) \"Test-driven development\" is a style of programming in which three activities are tightly interwoven: coding, testing (in the form of writing unit tests) and design (in the form of refactoring). Team A \"team\" in the Agile sense is a small group of people, assigned to the same project or effort, nearly all of them on a full-time basis. Team Room The team (ideally the whole team, including the PO or domain expert) has the use of a dedicated space for the duration of the project, set apart from other groups' activities. Three C's \"Card, Conversation, Confirmation\" is a formula that captures the components of a User Story. Three Amigos Three amigos refers to the primary perspectives to examine an increment of work before, during, and after development. Those perspectives are Business, Development, and Testing. Three Questions The daily meeting is structured around some variant of the following three questions: What have you completed? What will you do next? What is getting in your way? Timebox A timebox is a previously agreed period of time during which a person or a team works steadily towards completion of some goal. U Ubiquitous Language Striving to use the vocabulary of a given business domain, not only in discussions about the requirements for a software product, but in discussions of design as well and all the way into \"the product's source code itself.\" Unit Testing A unit test is a short program fragment written and maintained by the developers on the product team, which exercises some narrow part of the product's source code and checks the results. Usability Testing Usability testing is an empirical, exploratory technique to answer questions such as \"how would an end user respond to our software under realistic conditions?\" User Stories In consultation with the customer or PO, the team divides up the work to be done into functional increments called \"user stories.\" V Velocity At the end of each iteration, the team adds up effort estimates associated with user stories that were completed during that iteration. This total is called velocity. Version Control Version control is not strictly an Agile \"practice\" insofar as it is now widespread in the industry as a whole. But it is mentioned here for several reasons.","title":"9.1 Agile Terminology"},{"location":"agile/9-1-appendix-d/#91-agile-terminology","text":"Source: https://www.agilealliance.org/agile101/agile-glossary/ A Acceptance Test Driven Development (ATDD) Acceptance Test Driven Development (ATDD) involves team members with different perspectives (customer, development, testing) collaborating to write acceptance tests in advance of implementing the corresponding functionality. Acceptance Testing An acceptance test is a formal description of the behavior of a software product, generally expressed as an example or a usage scenario. A number of different notations and approaches have been proposed for such examples or scenarios. In many cases the aim is that it should be possible to automate the execution of such tests by a software tool, either ad-hoc to the development team or off the shelf. Antipattern Antipatterns are common solutions to common problems where the solution is ineffective and may result in undesired consequences. Automated Build In the context of software development, build refers to the process that converts files and other assets under the developers' responsibility into a software product in its final or consumable form. The build is automated when these steps are repeatable, require no direct human intervention, and can be performed at any time with no information other than what is stored in the source code control repository. B Backlog A backlog is an ordered list of items representing everything that may be needed to deliver a specific outcome. There are different types of backlogs depending on the type of item they contain and the approach being used. Backlog Grooming Backlog grooming is when the PO and some, or all, of the rest of the team refine the backlog on a regular basis to ensure the backlog contains the appropriate items, that they are prioritized, and that the items at the top of the backlog are ready for delivery. Behavior Driven Development (BDD) BDD is a practice where members of the team discuss the expected behavior of a system in order to build a shared understanding of expected functionality. Burndown Chart Burndown charts and burnup charts track the amount of output (in terms of hours, story points, or backlog items) a team has completed across an iteration or a project. Business Agility Business agility is the ability of an organization to sense changes internally or externally and respond accordingly in order to deliver value to its customers. C Collective Ownership Collective code ownership is the explicit convention that every team member can make changes to any code file as necessary: either to complete a development task, to repair a defect, or to improve the code's overall structure. Continuous Deployment Continuous deployment aims to reduce the time elapsed between writing a line of code and making that code available to users in production. To achieve continuous deployment, the team relies on infrastructure that automates and instruments the various steps leading up to deployment, so that after each integration successfully meeting these release criteria, the live application is updated with new code. Continuous Integration Continuous Integration is the practice of merging code changes into a shared repository several times a day in order to release a product version at any moment. This requires an integration procedure which is reproducible and automated. CRC Cards Class Responsibility Collaborator (CRC) Cards are an object oriented design technique teams can use to discuss what a class should know and do and what other classes it interacts with. Customer Development Customer development is a four-step framework that provides a way to use a scientific approach to validate assumptions about your product and business. D Daily Meeting The daily meeting is one of the most commonly practiced Agile techniques and presents opportunity for a team to get together on a regular basis to coordinate their activities. Definition of Done The definition of done is an agreed upon list of the activities deemed necessary to get a product increment, usually represented by a user story, to a done state by the end of a sprint. Definition of Ready Definition of Ready involves creating clear criteria that a user story must meet before being accepted into an upcoming iteration. This is typically based on the INVEST matrix. E Epic An epic is a large user story. Estimation In software development, an \"estimate\" is the evaluation of the effort necessary to carry out a given development task; this is most often expressed in terms of duration. Exploratory Testing Exploratory testing is, more than strictly speaking a \"practice,\" a style or approach to testing software which is often contrasted to \"scripted testing.\" Extreme Programming Extreme Programming (XP) is an agile software development framework that aims to produce higher quality software, and higher quality of life for the development team. XP is the most specific of the agile frameworks regarding appropriate engineering practices for software development. F Facilitation A facilitator is a person who chooses or is given the explicit role of conducting a meeting. Frequent Releases An Agile team frequently releases its product into the hands of end users, listening to feedback, whether critical or appreciative. G Given When Then The Given-When-Then formula is a template intended to guide the writing of acceptance tests for a User Story: (Given) some context, (When) some action is carried out, (Then) a particular set of observable consequences should obtain. H Heartbeat Retrospective The team meets regularly to reflect on the most significant events that occurred since the previous such meeting, and identify opportunities for improvement. I Incremental Development In an Agile context, Incremental Development is when each successive version of a product is usable, and each builds upon the previous version by adding user-visible functionality. Information Radiators \"Information radiator\" is the term for any of a number of visual displays which a team places in a highly visible location, so that all team members can see the latest information at a glance. Integration \"Integration\" (or \"integrating\") refers to any efforts still required for a project team to deliver a product suitable for release as a functional whole. INVEST The acronym INVEST stands for a set of criteria used to assess the quality of a user story. If the story fails to meet one of these criteria, the team may want to reword it. Iteration An iteration is a timebox during which development takes place. The duration may vary from project to project and is usually fixed. Iterative Development Agile projects are iterative insofar as they intentionally allow for \"repeating\" software development activities, and for potentially \"revisiting\" the same work products (the phrase \"planned rework\" is sometimes used; refactoring is a good example). K Kanban The Kanban Method is a means to design, manage and improve flow for knowledge work and allows teams to start where they are to drive evolutionary change. Kanban Board A Kanban Board is a visual workflow tool consisting of multiple columns. Each column represents a different stage in the workflow process. L Lead Time Lead Time is the time between a customer order and delivery. In software development, it can also be the time between a requirement made and its fulfillment. M Milestone Retrospective A Milestone Retrospective is a team's detailed analysis of the project's significant events after a set period of time or at the project's end. Minimum Marketable Feature (MMF) A Minimum Marketable Feature is a small, self-contained feature that can be developed quickly and that delivers significant value to the user. Minimum Viable Product (MVP) A Minimum Viable Product is, as Eric Ries said, the \"version of a new product which allows a team to collect the maximum amount of validated learning about customers with the least effort.\" Mob Programming Mob Programming is a software development approach where the whole team works on the same thing, at the same time, in the same space, and at the same computer. Mock Objects Mock Objects (commonly used in the context of crafting automated unit tests) consist of instantiating a test-specific version of a software component. N Niko-niko Calendar A Niko-niko Calendar is updated daily with each team member's mood for that day. Over time the calendar reveals patterns of change in the moods of the team, or of individual members. P Pair Programming Pair programming consists of two programmers sharing a single workstation (one screen, keyboard and mouse among the pair). Personas Personas are synthetic biographies of fictitious users of the future product. Planning Poker An approach to estimation used by Agile teams. Each team member \"plays\" a card bearing a numerical value corresponding to a point estimation for a user story. Points (estimates in) Agile teams generally prefer to express estimates in units other than the time-honored \"man-hours.\" Possibly the most widespread unit is \"story points.\" Product Owner (PO) The PO is a role created by the Scrum Framework responsible for making sure the team delivers the desired outcome. Project Chartering A high-level summary of the project's key success factors displayed on one wall of the team room as a flipchart-sized sheet of paper. Q Quick Design Session When \"simple design\" choices have far-reaching consequences, two or more developers meet for a quick design session at a whiteboard. R Refactoring Refactoring consists of improving the internal structure of an existing program's source code, while preserving its external behavior. Relative Estimation Relative estimation consists of estimating tasks or user stories by comparison or by grouping of items of equivalent difficulty. Role-feature-reason The \"role-feature-reason\" template is one of the most commonly recommended aids to write user stories: As a ... I want ... So that ... Rule of Simplicity Rules of Simplicity is a set of criteria, in priority order, proposed by Kent Beck to judge whether some source code is \"simple enough.\" S Scrum Scrum is a process framework used to manage product development and other knowledge work. Scrum Master The scrum master is responsible for ensuring the team lives agile values and principles and follows the practices that the team agreed they would use. Scrum of Scrums A technique to scale Scrum up to large groups (over a dozen people), consisting of dividing the groups into Agile teams of 5-10. Sign Up for Tasks Members of an Agile development team normally choose which tasks to work on, rather than being assigned work by a manager. Simple Design A team adopting the \"simple design\" practice bases its software design strategy on a set of \"simple design\" principles. Sprint Planning Sprint planning is an event that occurs at the beginning of a sprint where the team determines the product backlog items they will work on during that sprint. Story Mapping Story mapping consists of ordering user stories along two independent dimensions. Story Splitting Splitting consists of breaking up one user story into smaller ones, while preserving the property that each user story separately has measurable business value. Sustainable Pace The team aims for a work pace that they would be able to sustain indefinitely. T Task Board The most basic form of a task board is divided into three columns labeled \"To Do,\" \"In Progress,\" and \"Done.\" Cards are placed in the columns to reflect the current status of that task. Test Driven Development (TDD) \"Test-driven development\" is a style of programming in which three activities are tightly interwoven: coding, testing (in the form of writing unit tests) and design (in the form of refactoring). Team A \"team\" in the Agile sense is a small group of people, assigned to the same project or effort, nearly all of them on a full-time basis. Team Room The team (ideally the whole team, including the PO or domain expert) has the use of a dedicated space for the duration of the project, set apart from other groups' activities. Three C's \"Card, Conversation, Confirmation\" is a formula that captures the components of a User Story. Three Amigos Three amigos refers to the primary perspectives to examine an increment of work before, during, and after development. Those perspectives are Business, Development, and Testing. Three Questions The daily meeting is structured around some variant of the following three questions: What have you completed? What will you do next? What is getting in your way? Timebox A timebox is a previously agreed period of time during which a person or a team works steadily towards completion of some goal. U Ubiquitous Language Striving to use the vocabulary of a given business domain, not only in discussions about the requirements for a software product, but in discussions of design as well and all the way into \"the product's source code itself.\" Unit Testing A unit test is a short program fragment written and maintained by the developers on the product team, which exercises some narrow part of the product's source code and checks the results. Usability Testing Usability testing is an empirical, exploratory technique to answer questions such as \"how would an end user respond to our software under realistic conditions?\" User Stories In consultation with the customer or PO, the team divides up the work to be done into functional increments called \"user stories.\" V Velocity At the end of each iteration, the team adds up effort estimates associated with user stories that were completed during that iteration. This total is called velocity. Version Control Version control is not strictly an Agile \"practice\" insofar as it is now widespread in the industry as a whole. But it is mentioned here for several reasons.","title":"9.1 AGILE TERMINOLOGY"},{"location":"agile-model/1-1-plays/","text":"Agile Model-Driven Functionality Improvement","title":"1.1 Overview"},{"location":"agile-model/1-1-plays/#agile-model-driven-functionality-improvement","text":"","title":"Agile Model-Driven Functionality Improvement"},{"location":"agile-model/10-appendix-a/","text":"","title":"10.1 Appendix A: Sample Design 1"},{"location":"agile-model/10-appendix-b/","text":"","title":"10.2 Appendix B: Sample Design 2"},{"location":"agile-model/10-appendix-c/","text":"","title":"10.3 Appendix C: Checklists and Templates"},{"location":"agile-model/2-1-intro/","text":"2.1 Introduction The Agile Model-Driven Functionality Improvement Playbook provides the Air Force (AF) with methods for improving functionality within applications using business process modeling graphical tools that subject matter experts (SMEs) can understand and use. These models can be used to improve and extend application functionality using process modeling tools. This is accomplished by creating functional process models of the components from existing application code and by creating models for new application components. In some respects, this digital model of the application is a \u201cdigital clone\u201d that can be manipulated to improve the actual application.","title":"2.1 Introduction"},{"location":"agile-model/2-1-intro/#21-introduction","text":"The Agile Model-Driven Functionality Improvement Playbook provides the Air Force (AF) with methods for improving functionality within applications using business process modeling graphical tools that subject matter experts (SMEs) can understand and use. These models can be used to improve and extend application functionality using process modeling tools. This is accomplished by creating functional process models of the components from existing application code and by creating models for new application components. In some respects, this digital model of the application is a \u201cdigital clone\u201d that can be manipulated to improve the actual application.","title":"2.1 Introduction"},{"location":"agile-model/2-2-problem/","text":"2.2 Problem Statement In April 2016, AF A4 published the \u201cUS Air Force Enterprise Logistics Flight Plan v2.0\u201d (ELFP) and in June of 2016 a subordinate document called the \u201cEnterprise Logistics Technology Annex\u201d (ELTA). This plan and annex describe the \u201csynthesized logistics information\u201d desired future state of US Air Force (AF) Enterprise Logistics in 2035. The DoD and the AF understand that the use of agile methods hold great value. On Feb 2, 2017 DoDI 5000.75 \u201cBusiness Systems Requirements and Acquisition\u201d was published. This DoDI recognizes the need to rapidly deploy and improve business systems. The AF Logistics portfolio is comprised of many applications that are in a variety of lifecycle stages. These applications contain critical business rules and process flows used to support AF missions. They were generally developed using traditional means by different teams possessing varied skillsets. Applications were created using the functional and technical best practices and paradigms available at the time to support requirements but may now be outdated. This has resulted in a portfolio of applications where maintaining, improving and extending functionality is extremely difficult, costly, resource intensive and requires long schedules. Additionally, the AF has a shortage of true logistics SMEs capable of reinventing AF Logistics. Using traditional AF functional improvement methods, these valuable SMEs spend a lot of time on mundane activities that are not really aligned with truly transforming the business.","title":"2.2 Problem Statement"},{"location":"agile-model/2-2-problem/#22-problem-statement","text":"In April 2016, AF A4 published the \u201cUS Air Force Enterprise Logistics Flight Plan v2.0\u201d (ELFP) and in June of 2016 a subordinate document called the \u201cEnterprise Logistics Technology Annex\u201d (ELTA). This plan and annex describe the \u201csynthesized logistics information\u201d desired future state of US Air Force (AF) Enterprise Logistics in 2035. The DoD and the AF understand that the use of agile methods hold great value. On Feb 2, 2017 DoDI 5000.75 \u201cBusiness Systems Requirements and Acquisition\u201d was published. This DoDI recognizes the need to rapidly deploy and improve business systems. The AF Logistics portfolio is comprised of many applications that are in a variety of lifecycle stages. These applications contain critical business rules and process flows used to support AF missions. They were generally developed using traditional means by different teams possessing varied skillsets. Applications were created using the functional and technical best practices and paradigms available at the time to support requirements but may now be outdated. This has resulted in a portfolio of applications where maintaining, improving and extending functionality is extremely difficult, costly, resource intensive and requires long schedules. Additionally, the AF has a shortage of true logistics SMEs capable of reinventing AF Logistics. Using traditional AF functional improvement methods, these valuable SMEs spend a lot of time on mundane activities that are not really aligned with truly transforming the business.","title":"2.2 Problem Statement"},{"location":"agile-model/2-3-purpose/","text":"2.3 Purpose The intent of the Agile Model-Driven Functionality Improvement Playbook is to provide a set of instructions from \u201cstart to finish\u201d for BES AFLCMC/HIA programs identifying how and what proven agile processes and tools should be used to automate and standardize improvements in logistics systems. This Playbook will provide HIA with methods for improving functionality within applications using business process models management tools that subject matter experts (SMEs) can understand and utilize. The Agile Model-Driven Functionality Improvement Playbook Shall: Focus on how HIA should implement Agile Model-Driven Functionality Improvement to establish standard performance metrics for each level of the Agile Framework within AF Logistics. Include best practices encompassing the full Agile Modeling Software Development Life Cycle (SDLC) for Air Force applications in the Logistics Domain. Make use of application source code to create functional business rules modeling, remodeling, code generation and maintenance for the HIA programs. Standard use cases will be developed, described and exercised. Capability demonstrations shall be provided. Specify the Agile Model-Driven Functionality Improvement that includes: Scope, CONOPs; industry best practices; Conditions for Applications to use Model Driven Improvements; Business Value; business process models, limitations and various templates, processes and examples; recommendations for way forward.","title":"2.3 Purpose"},{"location":"agile-model/2-3-purpose/#23-purpose","text":"The intent of the Agile Model-Driven Functionality Improvement Playbook is to provide a set of instructions from \u201cstart to finish\u201d for BES AFLCMC/HIA programs identifying how and what proven agile processes and tools should be used to automate and standardize improvements in logistics systems. This Playbook will provide HIA with methods for improving functionality within applications using business process models management tools that subject matter experts (SMEs) can understand and utilize. The Agile Model-Driven Functionality Improvement Playbook Shall: Focus on how HIA should implement Agile Model-Driven Functionality Improvement to establish standard performance metrics for each level of the Agile Framework within AF Logistics. Include best practices encompassing the full Agile Modeling Software Development Life Cycle (SDLC) for Air Force applications in the Logistics Domain. Make use of application source code to create functional business rules modeling, remodeling, code generation and maintenance for the HIA programs. Standard use cases will be developed, described and exercised. Capability demonstrations shall be provided. Specify the Agile Model-Driven Functionality Improvement that includes: Scope, CONOPs; industry best practices; Conditions for Applications to use Model Driven Improvements; Business Value; business process models, limitations and various templates, processes and examples; recommendations for way forward.","title":"2.3 Purpose"},{"location":"agile-model/2-4-scope/","text":"2.4 Scope Concept of Operations (CONOPs) Survey of state of the art and practice within industry and DoD Conditions for Applications to use Model Driven Improvements Business Value and Return on Investment (ROI) Recommended Business Process Management (BPM) Architecture Model Driven Functionality Improvement Guidance Limitations of Model Driven Functionality Improvement Survey of Best Available Tools and Methods Tool Recommendations for AF Logistics Processes and Templates Recommended Team Organization by Project Phase Results and Examples of applying tools to AF pathfinder Java application Findings and Recommendations Recommended Way Forward","title":"2.4 Scope"},{"location":"agile-model/2-4-scope/#24-scope","text":"Concept of Operations (CONOPs) Survey of state of the art and practice within industry and DoD Conditions for Applications to use Model Driven Improvements Business Value and Return on Investment (ROI) Recommended Business Process Management (BPM) Architecture Model Driven Functionality Improvement Guidance Limitations of Model Driven Functionality Improvement Survey of Best Available Tools and Methods Tool Recommendations for AF Logistics Processes and Templates Recommended Team Organization by Project Phase Results and Examples of applying tools to AF pathfinder Java application Findings and Recommendations Recommended Way Forward","title":"2.4 Scope"},{"location":"agile-model/2-5-audience/","text":"2.5 Intended Audience The primary audience for this document is government project management teams and solutions architects determining how to enable an Agile Model-Driven Development approach utilizing \u201cLow Code\u201d platforms to modernize and integrate AF applications. Authors Notes: (Need to elaborate on) Program Managers Project Managers using Agile concepts to manage \u201cLow Code\u201d platform deliveries Solutions Architects \u2013 Design considerations for \u201cLow Code\u201d platforms Business Analysists (Business Process SME\u2019s) \u2013 \u201cFunctionals\u201d \u2013 Enabling them to use \u201cLow Code\u201d GUI based platform to rapidly implement new requirements to reduce development cycles. Developers \u2013 Their role in producing integration modules that can be easily implemented by Business Analysts","title":"2.5 Intended Audience"},{"location":"agile-model/2-5-audience/#25-intended-audience","text":"The primary audience for this document is government project management teams and solutions architects determining how to enable an Agile Model-Driven Development approach utilizing \u201cLow Code\u201d platforms to modernize and integrate AF applications.","title":"2.5 Intended Audience"},{"location":"agile-model/2-5-audience/#authors-notes-need-to-elaborate-on","text":"Program Managers Project Managers using Agile concepts to manage \u201cLow Code\u201d platform deliveries Solutions Architects \u2013 Design considerations for \u201cLow Code\u201d platforms Business Analysists (Business Process SME\u2019s) \u2013 \u201cFunctionals\u201d \u2013 Enabling them to use \u201cLow Code\u201d GUI based platform to rapidly implement new requirements to reduce development cycles. Developers \u2013 Their role in producing integration modules that can be easily implemented by Business Analysts","title":"Authors Notes: (Need to elaborate on)"},{"location":"agile-model/2-6-benefits/","text":"2.6 Benefits Create Playbook and methods once, used by everyone Consistent use of BEST practices and templates Individual AF Logistics applications can be functionally improved using model-driven business process graphical tools, Enables faster application functional upgrades These methods can be applied across the AF Logistics Enterprise and provide a way to manage the functional business rules across the portfolio allowing for portfolio-level remodeling and rationalization Business Process Modeling is designed to model, implement, maintain and improve work-flow style code, data structures and interfaces Once the groundwork has been laid and the models have been properly created, changes to the models can be made very quickly (compared to traditional methods) Gartner research states that 80% of organizations conducting business process management (BPM) projects will experience an internal rate of return better than 15%","title":"2.6 Benefits"},{"location":"agile-model/2-6-benefits/#26-benefits","text":"Create Playbook and methods once, used by everyone Consistent use of BEST practices and templates Individual AF Logistics applications can be functionally improved using model-driven business process graphical tools, Enables faster application functional upgrades These methods can be applied across the AF Logistics Enterprise and provide a way to manage the functional business rules across the portfolio allowing for portfolio-level remodeling and rationalization Business Process Modeling is designed to model, implement, maintain and improve work-flow style code, data structures and interfaces Once the groundwork has been laid and the models have been properly created, changes to the models can be made very quickly (compared to traditional methods) Gartner research states that 80% of organizations conducting business process management (BPM) projects will experience an internal rate of return better than 15%","title":"2.6 Benefits"},{"location":"agile-model/3-1-standards/","text":"","title":"3.1 Standards"},{"location":"agile-model/3-2-methodologies/","text":"","title":"3.2 Methodologies"},{"location":"agile-model/3-3-usecases/","text":"","title":"3.3 Common Use Case"},{"location":"agile-model/3-4-lessons/","text":"","title":"3.4 Lessons"},{"location":"agile-model/3-5-limitations/","text":"","title":"3.5 Limitations"},{"location":"agile-model/3-6-recommendations/","text":"","title":"3.6 Recommendations"},{"location":"agile-model/4-1-stakeholders/","text":"","title":"4.1 Key Stakeholders"},{"location":"agile-model/4-2-1-management/","text":"","title":"4.2.1 Project Management Team"},{"location":"agile-model/4-2-2-tech/","text":"","title":"4.2.2 Technical Team"},{"location":"agile-model/4-2-3-business/","text":"","title":"4.2.3 Business Analyst SME's"},{"location":"agile-model/4-2-team/","text":"","title":"4.2 Team Structure"},{"location":"agile-model/4-3-support/","text":"4.2 Team Structure 4.2.1 Project Management Team 4.2.2 Technical Team 4.2.3 Business Analyst SME's","title":"4.3 Support"},{"location":"agile-model/4-3-support/#42-team-structure","text":"4.2.1 Project Management Team 4.2.2 Technical Team 4.2.3 Business Analyst SME's","title":"4.2 Team Structure"},{"location":"agile-model/5-1-bestpractices/","text":"","title":"5.1 Best Practices"},{"location":"agile-model/6-1-phases/","text":"","title":"6.1 Phases"},{"location":"agile-model/6-2-risk/","text":"","title":"6.2 Risk Identification and Management"},{"location":"agile-model/7-1-definekpi/","text":"","title":"7.1 Define KPI's"},{"location":"agile-model/8-1-requirements/","text":"","title":"8.1 Requirements Analysis"},{"location":"agile-model/8-2-techdesign/","text":"","title":"8.2 Technical Design"},{"location":"agile-model/8-3-cybersecurity/","text":"","title":"8.3 Cybersecurity Considerations"},{"location":"agile-model/9-1-solutions/","text":"","title":"9.1 Identifying Viable Solutions"},{"location":"agile-model/9-2-1-besttools/","text":"","title":"9 2 1 besttools"},{"location":"agile-model/9-2-analysis/","text":"","title":"9.2 Conducting Analysis"},{"location":"agile-model/9-3-1-pega/","text":"","title":"9.3.1 Pega Platform"},{"location":"agile-model/9-3-2-appian/","text":"","title":"9.3.2 Appian Low-Code Platform"},{"location":"agile-model/9-3-3-mendix/","text":"","title":"9.3.3 Mendix Low-Code Platform"},{"location":"agile-model/9-3-4-microsoft/","text":"","title":"9.3.4 Microsoft PowerApps"},{"location":"agile-model/9-3-5-outsystems/","text":"","title":"9.3.5 OutSystems Low-Code Platform"},{"location":"agile-model/9-3-6-kony/","text":"","title":"9.3.6 Kony Quantum"},{"location":"agile-model/9-3-cots/","text":"9.3 COTS 9.3.1 Pega Platform 9.3.2 Appian Low-Code Platform 9.3.3 Mendix Low-Code Platform 9.3.4 Microsoft PowerApps 9.3.5 OutSystems Low-Code Platform 9.3.6 Kony Quantum","title":"9.3 Help COTS Too Is Available - Example List"},{"location":"agile-model/9-3-cots/#93-cots","text":"9.3.1 Pega Platform 9.3.2 Appian Low-Code Platform 9.3.3 Mendix Low-Code Platform 9.3.4 Microsoft PowerApps 9.3.5 OutSystems Low-Code Platform 9.3.6 Kony Quantum","title":"9.3 COTS"},{"location":"cloud/1-1-cloud/","text":"App Technology Refresh for Cloud","title":"1. Overview"},{"location":"cloud/1-1-cloud/#app-technology-refresh-for-cloud","text":"","title":"App Technology Refresh for Cloud"},{"location":"cyber/1-1-cyber/","text":"Cyber Security","title":"1. Overview"},{"location":"cyber/1-1-cyber/#cyber-security","text":"","title":"Cyber Security"},{"location":"devops/1-1-devops/","text":"","title":"1. Overview"},{"location":"modern/1-1-play-1/","text":"1.1 Play 1: Determine the application\u2019s future lifespan Before modernizing any legacy application, it is crucial to understand how long the application will remain in service and under what conditions, to ensure modifications produce a prudent mission value and return on investment (ROI) balance. It is essential for the application or product owner to provide requirements and constraints that direct the effort. Currently, DoD services are rationalizing their application portfolios for mission effectiveness and cost efficiency. This is leading to applications being subsumed by other applications, applications being retired, applications being refactored and deployed to cloud environments, and other portfolio consolidation approaches. Checklist Identify the application modernization project\u2019s champion, the application owner that believes in the value of the endeavor. Obtain and document the application\u2019s future lifespan from the application owner. Gather application modernization requirements and constraints from the application or product owner. If not documented, document these requirements in a formal manner, have it reviewed by the application or product owner, and place under configuration management. This application modernization requirements document should contain: application portfolio information, application rationalization information, application lifespan forecast including dates, requirements list, requirements analysis, other pertinent information that will help when determining the best modernization course of action (COA). Develop a (lean) communication plan to keep application owner, product owner, and stakeholders apprised of important project and related information. Key Questions How long will the modernized application be in service? Are there unique application mission or technical constraints that must be addressed? Will the application be subsumed by another application in the future? If so, when, and what is the technical plan for subsuming? What limitations does this place on the modernization solution?","title":"1.1 Applications Future Lifespan"},{"location":"modern/1-1-play-1/#11-play-1-determine-the-applications-future-lifespan","text":"Before modernizing any legacy application, it is crucial to understand how long the application will remain in service and under what conditions, to ensure modifications produce a prudent mission value and return on investment (ROI) balance. It is essential for the application or product owner to provide requirements and constraints that direct the effort. Currently, DoD services are rationalizing their application portfolios for mission effectiveness and cost efficiency. This is leading to applications being subsumed by other applications, applications being retired, applications being refactored and deployed to cloud environments, and other portfolio consolidation approaches.","title":"1.1  Play 1: Determine the application\u2019s future lifespan"},{"location":"modern/1-1-play-1/#checklist","text":"Identify the application modernization project\u2019s champion, the application owner that believes in the value of the endeavor. Obtain and document the application\u2019s future lifespan from the application owner. Gather application modernization requirements and constraints from the application or product owner. If not documented, document these requirements in a formal manner, have it reviewed by the application or product owner, and place under configuration management. This application modernization requirements document should contain: application portfolio information, application rationalization information, application lifespan forecast including dates, requirements list, requirements analysis, other pertinent information that will help when determining the best modernization course of action (COA). Develop a (lean) communication plan to keep application owner, product owner, and stakeholders apprised of important project and related information.","title":"Checklist"},{"location":"modern/1-1-play-1/#key-questions","text":"How long will the modernized application be in service? Are there unique application mission or technical constraints that must be addressed? Will the application be subsumed by another application in the future? If so, when, and what is the technical plan for subsuming? What limitations does this place on the modernization solution?","title":"Key Questions"},{"location":"modern/1-2-play-2/","text":"1.2 Play 2: Collect, analyze, and assess the existing (legacy) application\u2019s technical baseline Once Play 1 is complete, the existing (legacy) application technical baseline must be collected, reviewed, analyzed, and understood. Many applications will not be documented completely or accurately enough to allow for the necessary analysis and assessment required for developing a modernization solution and plan. In this case, technical baseline documentation gaps must be closed, or an automated discovery tool must be used to extract the technical baseline from the production application. The best approach is to collect the existing documentation and artifacts, as well as use an automated discovery tool, and then correlate the technical information. Checklist Gather technical baseline documentation and engineering artifacts (including models) from the application owner and team. Use an automated discovery tool to obtain the technical baseline from the actual production application. Create a new technical library of this information to be used during the modernization project. Review all information and develop updated architectural framework views of the application to ensure all stakeholders understand the application\u2019s current technical baseline. Run automated tools against the code baseline to understand the quality of the existing baseline. These tools also provide insight on the steps that may correct quality issues. More information on these tools is provided in the complete playbook. Key Questions Is the technical baseline complete? Are there important gaps? How can the gaps be closed? How can the right automated discovery tool be used to improve the quality of the current (legacy) technical baseline? How much technical debt exists with the current legacy application? How should this technical debt be accounted for?","title":"1.2 Technical Baseline"},{"location":"modern/1-2-play-2/#12-play-2-collect-analyze-and-assess-the-existing-legacy-applications-technical-baseline","text":"Once Play 1 is complete, the existing (legacy) application technical baseline must be collected, reviewed, analyzed, and understood. Many applications will not be documented completely or accurately enough to allow for the necessary analysis and assessment required for developing a modernization solution and plan. In this case, technical baseline documentation gaps must be closed, or an automated discovery tool must be used to extract the technical baseline from the production application. The best approach is to collect the existing documentation and artifacts, as well as use an automated discovery tool, and then correlate the technical information.","title":"1.2 Play 2: Collect, analyze, and assess the existing (legacy) application\u2019s technical baseline"},{"location":"modern/1-2-play-2/#checklist","text":"Gather technical baseline documentation and engineering artifacts (including models) from the application owner and team. Use an automated discovery tool to obtain the technical baseline from the actual production application. Create a new technical library of this information to be used during the modernization project. Review all information and develop updated architectural framework views of the application to ensure all stakeholders understand the application\u2019s current technical baseline. Run automated tools against the code baseline to understand the quality of the existing baseline. These tools also provide insight on the steps that may correct quality issues. More information on these tools is provided in the complete playbook.","title":"Checklist"},{"location":"modern/1-2-play-2/#key-questions","text":"Is the technical baseline complete? Are there important gaps? How can the gaps be closed? How can the right automated discovery tool be used to improve the quality of the current (legacy) technical baseline? How much technical debt exists with the current legacy application? How should this technical debt be accounted for?","title":"Key Questions"},{"location":"modern/1-3-play-3/","text":"1.3 Play 3: Determine viable application modernization solution options Building on the knowledge gained in the previous plays, this play begins the solution design process by considering the requirements and outcomes the application owner and the enterprise desire for this application\u2019s future state and identifying viable solution options (alternative solutions) that need to be analyzed, compared, evaluated, and communicated. Checklist Perform Solution Discovery and Design and identify viable solution options. Perform an analysis of alternatives (AoA) for viable solution options to determine the best option. Begin by creating a scoring matrix and evaluation method that measures the key solution parameters as objectively as possible. Ensure the matrix and method appropriately focuses on the value of applying automated tools over manual labor tasks. Analyze and score each of the viable solution options using the matrix and method. Report findings and recommendations to the appropriate stakeholders. Perform Proof of Concept (POC) projects to validate options and assumptions especially with respect to the use of automated software modernization and engineering tools for the solution options that scored the highest. Identify automated tools and methods to modernize the application. Work with vendors and see demonstrations of the most encouraging tools. Recommend the best solution option and gain concurrence from the application owner. This will likely be an iterative process. Key Questions What are the preferred enterprise IT technologies that should be considered for the modernized application? What IT licenses does the enterprise invest in? What will future licensing agreements be? Does the enterprise desire to move to open source products? Under what conditions? What environments must the modernized application operate in? Consider: development, test, pre-production, production and other environments.","title":"1.3 Modernization Options"},{"location":"modern/1-3-play-3/#13-play-3-determine-viable-application-modernization-solution-options","text":"Building on the knowledge gained in the previous plays, this play begins the solution design process by considering the requirements and outcomes the application owner and the enterprise desire for this application\u2019s future state and identifying viable solution options (alternative solutions) that need to be analyzed, compared, evaluated, and communicated.","title":"1.3 Play 3: Determine viable application modernization solution options"},{"location":"modern/1-3-play-3/#checklist","text":"Perform Solution Discovery and Design and identify viable solution options. Perform an analysis of alternatives (AoA) for viable solution options to determine the best option. Begin by creating a scoring matrix and evaluation method that measures the key solution parameters as objectively as possible. Ensure the matrix and method appropriately focuses on the value of applying automated tools over manual labor tasks. Analyze and score each of the viable solution options using the matrix and method. Report findings and recommendations to the appropriate stakeholders. Perform Proof of Concept (POC) projects to validate options and assumptions especially with respect to the use of automated software modernization and engineering tools for the solution options that scored the highest. Identify automated tools and methods to modernize the application. Work with vendors and see demonstrations of the most encouraging tools. Recommend the best solution option and gain concurrence from the application owner. This will likely be an iterative process.","title":"Checklist"},{"location":"modern/1-3-play-3/#key-questions","text":"What are the preferred enterprise IT technologies that should be considered for the modernized application? What IT licenses does the enterprise invest in? What will future licensing agreements be? Does the enterprise desire to move to open source products? Under what conditions? What environments must the modernized application operate in? Consider: development, test, pre-production, production and other environments.","title":"Key Questions"},{"location":"modern/1-4-play-4/","text":"1.4 Play 4: Determine the target (\u201cTo-Be\u201d) modernized application solution Building on Play 3, this play determines the complete target (or \u201cTo-Be\u201d) solution architecture for the modernized application for the approved option. This is like a deep preliminary design ending with a critical design. If the application modernization solution is complex, then break this play into sub parts to address the complexity. The complete target (or \u201cTo-Be\u201d) solution architecture includes a full stack solution consisting of: hosting (all needed environments), networks, security, accreditation, application, data and database, interface solution, processes, automated baseline promotion in a DevOps pipeline (if appropriate), production operations, help/service desk operations, and related solutions and activities. Significant design activities (such as those included in this section) are usually best completed using a formal agile methodology such as Scrum. Checklist Determine the application architecture including: overall architecture, operating system, coding paradigm, code language, frameworks, database type, etc. Determine the development methodologies to be used: agile, DevOps, hybrid, DAD, waterfall, etc. Determine DevOps automated baseline promotion pipeline solution (or use one provided by the enterprise). Integrate Cyber Security, accreditation, and compliance requirements as this target solution is defined. Determine the hosting solutions. Determine if the application will be hosted in the cloud (and under which model: government cloud, shared cloud, private cloud, on-premise hosting, hybrid)? Consider each environment the application needs: development, testing, pre-production, production, and others. Determine where each of these environments be hosted. Determine how the application baseline upgrades will flow between environments (ie: promoted from one environment to the next)? Determine the enterprise computing services (ie: platform as a service, PaaS) required to host the application. Determine solutions for identity and access management (IDAM), data interfacing such as enterprise service bus (ESB) solutions, web services, and microservices. Determine how this application will be operated in the new environment, including backups, monitoring, and disaster recovery. Create accreditation plan. Create the development environment. Create proper networking solution (consistent with production solution). Allow network access to the enterprise computing services (PaaS) needed (ie: IDAM, ESB, others). Install COTS products to be used. Map business logic components from the legacy to modernized application. Use automated software conversion tools to do the bulk of the work. Note: these tools are specialized, and the right tool(s) must be used for the task at hand. More about these tools are included in the complete playbook. These tools can convert the code (and thereby the business logic) into other code languages or dialects using a variety of approaches. Use Proof of Concepts and demonstrations to show automation results against the actual codebase. Choose a data store (database) that best meets the applications needs. Map data structures from the legacy to modernized application. Migrate the data from the legacy to modernized data store (database). Note: these tools are specialized, and the right tool(s) must be used for the task at hand. More about these tools are included in the complete playbook. Use Proof of Concepts and demonstrations to show automation results against the actual data schema and data set. Transform interfaces if necessary. Ensure interfaces can be tested from development and test environments. Test exhaustively using automated testing. Create plan to produce lean documentation using automated and digital means (to maximum extent possible) for: system design, application design, database design, interface design, test cases, operator\u2019s manual, help/service desk guides. Create software version list and forecast for the modernized application. Communicate to stakeholders. Determine the Help Desk or Service Desk model. Ensure the Help Desk or Service Desk is ready to provide service during and after production cutover. Coordinate cutover plan with stakeholders. Define cutover test. Achieve cyber security authorization to operate. Cutover and Go-Live. Leave legacy system in production, but not serving users, until the new modernized system is deemed ready for initial operating capability (IOC) and can reliably serve users. Perform as many iterative design reviews with stakeholders as needed throughout this play. Key Questions: - Are all lifecycle processes defined and documented? - What activities are not using automation? Why? - Does the target (or \u201cTo-Be\u201d) solution align or comply with enterprise requirements? - Are the products used aligned with the enterprise license agreements and forecast? - What are the key solution risks? How are they being mitigated? How are they reported to stakeholders?","title":"1.4 Target Application Solution"},{"location":"modern/1-4-play-4/#14-play-4-determine-the-target-to-be-modernized-application-solution","text":"Building on Play 3, this play determines the complete target (or \u201cTo-Be\u201d) solution architecture for the modernized application for the approved option. This is like a deep preliminary design ending with a critical design. If the application modernization solution is complex, then break this play into sub parts to address the complexity. The complete target (or \u201cTo-Be\u201d) solution architecture includes a full stack solution consisting of: hosting (all needed environments), networks, security, accreditation, application, data and database, interface solution, processes, automated baseline promotion in a DevOps pipeline (if appropriate), production operations, help/service desk operations, and related solutions and activities. Significant design activities (such as those included in this section) are usually best completed using a formal agile methodology such as Scrum.","title":"1.4 Play 4: Determine the target (\u201cTo-Be\u201d) modernized application solution"},{"location":"modern/1-4-play-4/#checklist","text":"Determine the application architecture including: overall architecture, operating system, coding paradigm, code language, frameworks, database type, etc. Determine the development methodologies to be used: agile, DevOps, hybrid, DAD, waterfall, etc. Determine DevOps automated baseline promotion pipeline solution (or use one provided by the enterprise). Integrate Cyber Security, accreditation, and compliance requirements as this target solution is defined. Determine the hosting solutions. Determine if the application will be hosted in the cloud (and under which model: government cloud, shared cloud, private cloud, on-premise hosting, hybrid)? Consider each environment the application needs: development, testing, pre-production, production, and others. Determine where each of these environments be hosted. Determine how the application baseline upgrades will flow between environments (ie: promoted from one environment to the next)? Determine the enterprise computing services (ie: platform as a service, PaaS) required to host the application. Determine solutions for identity and access management (IDAM), data interfacing such as enterprise service bus (ESB) solutions, web services, and microservices. Determine how this application will be operated in the new environment, including backups, monitoring, and disaster recovery. Create accreditation plan. Create the development environment. Create proper networking solution (consistent with production solution). Allow network access to the enterprise computing services (PaaS) needed (ie: IDAM, ESB, others). Install COTS products to be used. Map business logic components from the legacy to modernized application. Use automated software conversion tools to do the bulk of the work. Note: these tools are specialized, and the right tool(s) must be used for the task at hand. More about these tools are included in the complete playbook. These tools can convert the code (and thereby the business logic) into other code languages or dialects using a variety of approaches. Use Proof of Concepts and demonstrations to show automation results against the actual codebase. Choose a data store (database) that best meets the applications needs. Map data structures from the legacy to modernized application. Migrate the data from the legacy to modernized data store (database). Note: these tools are specialized, and the right tool(s) must be used for the task at hand. More about these tools are included in the complete playbook. Use Proof of Concepts and demonstrations to show automation results against the actual data schema and data set. Transform interfaces if necessary. Ensure interfaces can be tested from development and test environments. Test exhaustively using automated testing. Create plan to produce lean documentation using automated and digital means (to maximum extent possible) for: system design, application design, database design, interface design, test cases, operator\u2019s manual, help/service desk guides. Create software version list and forecast for the modernized application. Communicate to stakeholders. Determine the Help Desk or Service Desk model. Ensure the Help Desk or Service Desk is ready to provide service during and after production cutover. Coordinate cutover plan with stakeholders. Define cutover test. Achieve cyber security authorization to operate. Cutover and Go-Live. Leave legacy system in production, but not serving users, until the new modernized system is deemed ready for initial operating capability (IOC) and can reliably serve users. Perform as many iterative design reviews with stakeholders as needed throughout this play. Key Questions: - Are all lifecycle processes defined and documented? - What activities are not using automation? Why? - Does the target (or \u201cTo-Be\u201d) solution align or comply with enterprise requirements? - Are the products used aligned with the enterprise license agreements and forecast? - What are the key solution risks? How are they being mitigated? How are they reported to stakeholders?","title":"Checklist"},{"location":"modern/1-5-play-5/","text":"1.5 Play 5: Formulate the application modernization and migration plan (how to move from \u201cAs-Is\u201d to \u201cTo-Be\u201d state) This play develops the plan for migrating the application from the legacy (determined in Play 2) to modernized (determined in Play 3, 4) state using automated tools. Checklist Create a work plan and project schedule to perform the technical modernization tasks discussed in Play 4. Choose the best project methodology for the unique requirements of this project (ie: agile, DAD, waterfall, etc; note: hybrid models such as DAD have been used with success within DoD). Develop a risk management plan. Identify and manage risks. Key Questions Is this plan realistic? Have all tasks been scoped and bounded with success (done) criteria? What is the basis of estimate? What are the key execution risks? How are they being mitigated? How are they reported to stakeholders? How will status be collected and communicated to stakeholders? Has management reserve / margin been properly added to the plan?","title":"1.5 Modernization and Migration Plan"},{"location":"modern/1-5-play-5/#15-play-5-formulate-the-application-modernization-and-migration-plan-how-to-move-from-as-is-to-to-be-state","text":"This play develops the plan for migrating the application from the legacy (determined in Play 2) to modernized (determined in Play 3, 4) state using automated tools.","title":"1.5 Play 5: Formulate the application modernization and migration plan (how to move from \u201cAs-Is\u201d to \u201cTo-Be\u201d state)"},{"location":"modern/1-5-play-5/#checklist","text":"Create a work plan and project schedule to perform the technical modernization tasks discussed in Play 4. Choose the best project methodology for the unique requirements of this project (ie: agile, DAD, waterfall, etc; note: hybrid models such as DAD have been used with success within DoD). Develop a risk management plan. Identify and manage risks.","title":"Checklist"},{"location":"modern/1-5-play-5/#key-questions","text":"Is this plan realistic? Have all tasks been scoped and bounded with success (done) criteria? What is the basis of estimate? What are the key execution risks? How are they being mitigated? How are they reported to stakeholders? How will status be collected and communicated to stakeholders? Has management reserve / margin been properly added to the plan?","title":"Key Questions"},{"location":"modern/1-6-play-6/","text":"1.6 Play 6: Execute the application modernization plan This play executes the plan developed in Play 5. Checklist Manage project to plan and schedule, performance measures, and cost. Hold status meetings consistent with the product development methodology (ie: Agile Scrum). Provide stakeholders with executive-level status on a weekly basis. Key Questions What are the key service levels that must be met by the application? How are these measured and reported to key stakeholders? What controls will ensure the project stays on track or indicates it is off track? How is plan management reserve / margin been properly added to the plan managed? How will the application be sustained once modernized? How will the application evolve post modernization and migration?","title":"1.6 Executing the Plan"},{"location":"modern/1-6-play-6/#16-play-6-execute-the-application-modernization-plan","text":"This play executes the plan developed in Play 5.","title":"1.6 Play 6: Execute the application modernization plan"},{"location":"modern/1-6-play-6/#checklist","text":"Manage project to plan and schedule, performance measures, and cost. Hold status meetings consistent with the product development methodology (ie: Agile Scrum). Provide stakeholders with executive-level status on a weekly basis.","title":"Checklist"},{"location":"modern/1-6-play-6/#key-questions","text":"What are the key service levels that must be met by the application? How are these measured and reported to key stakeholders? What controls will ensure the project stays on track or indicates it is off track? How is plan management reserve / margin been properly added to the plan managed? How will the application be sustained once modernized? How will the application evolve post modernization and migration?","title":"Key Questions"},{"location":"modern/10-1-definekpi/","text":"","title":"10.1 Define KPIs"},{"location":"modern/11-appendix-a/","text":"","title":"11.1 Appendix A: Sample Design 1"},{"location":"modern/11-appendix-b/","text":"","title":"11.2 Appendix B: Sample Design 2"},{"location":"modern/11-appendix-c/","text":"","title":"11.3 Appendix C: Checklists and Templates"},{"location":"modern/2-1-intro/","text":"2.1 Introduction In April 2016, Air Force (AF) Logistics (A4) published the \u201cUS Air Force Enterprise Logistics Flight Plan v2.0\u201d (ELFP) and in June of 2016 a subordinate document called the \u201cEnterprise Logistics Technology Annex\u201d (ELTA). This plan and annex describe the \u201csynthesized logistics information\u201d future state of AF Enterprise Logistics in 2035. To maintain a path and schedule that will achieve those long-term goals, a series of enabling initiatives was defined to accelerate current progress to achieve the necessary near-term milestones. \u201cEnterprise Logistics IT (ELIT) Automated Application Modernization Playbook Chapter\u201d is one of these initiatives. Automated Application Modernization applies automated tools to transform proprietary, closed legacy applications and systems to modernized applications, aligned with current AF standards hosted on open platforms that are much easier to technically integrate and interoperate with other authorized applications. Further, the modernized application is more secure, easier to upgrade, componentize, functionally modernize, share information, use third party products (such as reporting tools), and less expensive to maintain and operate. The Automated Application Modernization Playbook Chapter will provide the Air Force (AF) with approaches and methodologies to convert, transform, and migrate legacy systems to modern, open, standards-based platforms using common languages such as Java and Microsoft .NET. For example, the modernization of the AF Logistics Standard Base Supply System (SBSS), that provided a major set of application functionality for Integrated Logistics System \u2013 Supply (ILS-S), was converted from a UNISYS 2200-based COBOL application to a well-designed RHEL-based Java application. This project provided many benefits including: cost reduction, technical improvements, improved user experience, and improved mission integration. This Playbook Chapter leverages this experience, and others, to create a set of plays that can be applied to modernize a wide variety of applications.","title":"2.1 Introduction"},{"location":"modern/2-1-intro/#21-introduction","text":"In April 2016, Air Force (AF) Logistics (A4) published the \u201cUS Air Force Enterprise Logistics Flight Plan v2.0\u201d (ELFP) and in June of 2016 a subordinate document called the \u201cEnterprise Logistics Technology Annex\u201d (ELTA). This plan and annex describe the \u201csynthesized logistics information\u201d future state of AF Enterprise Logistics in 2035. To maintain a path and schedule that will achieve those long-term goals, a series of enabling initiatives was defined to accelerate current progress to achieve the necessary near-term milestones. \u201cEnterprise Logistics IT (ELIT) Automated Application Modernization Playbook Chapter\u201d is one of these initiatives. Automated Application Modernization applies automated tools to transform proprietary, closed legacy applications and systems to modernized applications, aligned with current AF standards hosted on open platforms that are much easier to technically integrate and interoperate with other authorized applications. Further, the modernized application is more secure, easier to upgrade, componentize, functionally modernize, share information, use third party products (such as reporting tools), and less expensive to maintain and operate. The Automated Application Modernization Playbook Chapter will provide the Air Force (AF) with approaches and methodologies to convert, transform, and migrate legacy systems to modern, open, standards-based platforms using common languages such as Java and Microsoft .NET. For example, the modernization of the AF Logistics Standard Base Supply System (SBSS), that provided a major set of application functionality for Integrated Logistics System \u2013 Supply (ILS-S), was converted from a UNISYS 2200-based COBOL application to a well-designed RHEL-based Java application. This project provided many benefits including: cost reduction, technical improvements, improved user experience, and improved mission integration. This Playbook Chapter leverages this experience, and others, to create a set of plays that can be applied to modernize a wide variety of applications.","title":"2.1 Introduction"},{"location":"modern/2-2-problem/","text":"2.2 Problem Statement The AF has many legacy and outdated systems that collectively are comprised of tens of millions of source lines of code (SLOC). Many of these legacy and outdated systems are on \u201cclosed\u201d platforms that prevent the functionality and data contained within the system from being readily being accessed, integrated or interoperated with, extended or improved. These systems contain business-critical business rules and data that drive the AF and support numerous missions. This functionality and data need to be unlocked so it can be shared with the other AF applications to improve business and mission effectiveness. Other issues with the AF\u2019s legacy applications and systems include: poor cyber posture, current platform and language limitations, poor user experiences and user interfaces, poor performance, difficult to find labor skills, expensive platforms and hosting.","title":"2.2 Problem Statement"},{"location":"modern/2-2-problem/#22-problem-statement","text":"The AF has many legacy and outdated systems that collectively are comprised of tens of millions of source lines of code (SLOC). Many of these legacy and outdated systems are on \u201cclosed\u201d platforms that prevent the functionality and data contained within the system from being readily being accessed, integrated or interoperated with, extended or improved. These systems contain business-critical business rules and data that drive the AF and support numerous missions. This functionality and data need to be unlocked so it can be shared with the other AF applications to improve business and mission effectiveness. Other issues with the AF\u2019s legacy applications and systems include: poor cyber posture, current platform and language limitations, poor user experiences and user interfaces, poor performance, difficult to find labor skills, expensive platforms and hosting.","title":"2.2 Problem Statement"},{"location":"modern/2-3-purpose/","text":"2.3 Purpose This Automated Application Modernization Playbook Chapter is a continuation of effort by the BES PEO to establish modern practices and methods to accelerate software development and deployment of value for Logistics Information Systems and its end users. Preceded by playbooks for Agile and Automated Testing, and User Experience, this playbook aims to provide approaches, methods, and tools to apply automation to efficiently modernize legacy applications. Once legacy applications are modernized to the AF standard, the value of the business logic and data can be much more easily be shared with other applications increasing the completeness and accuracy of information and, ultimately, improving mission effectiveness. This Playbook Chapter serves as a \u201cliving document,\u201d delivering value in the present while providing a foundation for future updates and enhancements as the state of technology and the needs of stakeholders and users served by Logistics Information Systems continually evolve.","title":"2.3 Purpose"},{"location":"modern/2-3-purpose/#23-purpose","text":"This Automated Application Modernization Playbook Chapter is a continuation of effort by the BES PEO to establish modern practices and methods to accelerate software development and deployment of value for Logistics Information Systems and its end users. Preceded by playbooks for Agile and Automated Testing, and User Experience, this playbook aims to provide approaches, methods, and tools to apply automation to efficiently modernize legacy applications. Once legacy applications are modernized to the AF standard, the value of the business logic and data can be much more easily be shared with other applications increasing the completeness and accuracy of information and, ultimately, improving mission effectiveness. This Playbook Chapter serves as a \u201cliving document,\u201d delivering value in the present while providing a foundation for future updates and enhancements as the state of technology and the needs of stakeholders and users served by Logistics Information Systems continually evolve.","title":"2.3 Purpose"},{"location":"modern/2-4-scope/","text":"2.4 Scope The Automated Application Modernization Playbook Chapter describes approaches and methods to apply automation to modernize legacy applications and systems from their current platform and language to a sustainable platforms and language that can be more readily improved, integrated and interfaced with. This playbook chapter includes: Application Modernization Guidance Limitations of Automated Modernization Organizational Support Requirements and Lessons Architectural Standard for Application Modernization Modernization Tools Modernization Approaches and Methods Application Modernization Factory Concept of Operations (CONOPs) Conclusions and Recommendations","title":"2.4 Scope"},{"location":"modern/2-4-scope/#24-scope","text":"The Automated Application Modernization Playbook Chapter describes approaches and methods to apply automation to modernize legacy applications and systems from their current platform and language to a sustainable platforms and language that can be more readily improved, integrated and interfaced with. This playbook chapter includes: Application Modernization Guidance Limitations of Automated Modernization Organizational Support Requirements and Lessons Architectural Standard for Application Modernization Modernization Tools Modernization Approaches and Methods Application Modernization Factory Concept of Operations (CONOPs) Conclusions and Recommendations","title":"2.4 Scope"},{"location":"modern/2-5-audience/","text":"2.5 Audience While this playbook can provide value to all personnel involved in a software modernization project, the primary audience for this playbook are those individuals who are responsible for the planning, management and development of projects that employ or might benefit from automated application modernization approaches and methodologies. For program managers and those in similar roles, this playbook will provide a foundation of knowledge to enable more effective planning and support for programs where automated application modernization methodologies might be employed. For software engineers and those in similar roles, this playbook will provide a valuable resource for design patterns and guidelines from which legacy applications can be efficiently modernized, enabling faster design decisions and more rapid deployment while ensuring consistency and quality across all digital experiences delivered by Logistics Information Systems.","title":"2.5 Audience"},{"location":"modern/2-5-audience/#25-audience","text":"While this playbook can provide value to all personnel involved in a software modernization project, the primary audience for this playbook are those individuals who are responsible for the planning, management and development of projects that employ or might benefit from automated application modernization approaches and methodologies. For program managers and those in similar roles, this playbook will provide a foundation of knowledge to enable more effective planning and support for programs where automated application modernization methodologies might be employed. For software engineers and those in similar roles, this playbook will provide a valuable resource for design patterns and guidelines from which legacy applications can be efficiently modernized, enabling faster design decisions and more rapid deployment while ensuring consistency and quality across all digital experiences delivered by Logistics Information Systems.","title":"2.5 Audience"},{"location":"modern/2-6-benefits/","text":"2.6 Benefits The Automated Application Modernization Playbook Chapter provides the following benefits and provides a foundation of an enduring AF application modernization practice: Efficiently Modernizes AF Legacy applications to the current AF standard so they can interoperate with other modern AF applications Drives down sustainment costs by moving to more affordable software and hosting solutions Provides an Automated Application Modernization Approach and Methods to transform AF applications Identifies potential organizational, non-technical dependencies or barriers and how to mitigate them Provides examples of several successful modernization projects Provides a listing of proven automated modernization tools and how to use them. Could become basis for application modernization \u201cAssembly Line\u201d or Center of Excellence (CoE). For applications: lower sustainment and upgrade costs, easier to maintain and improve, easier to integrate with apps/data, easier to rationalize","title":"2.6 Benefits"},{"location":"modern/2-6-benefits/#26-benefits","text":"The Automated Application Modernization Playbook Chapter provides the following benefits and provides a foundation of an enduring AF application modernization practice: Efficiently Modernizes AF Legacy applications to the current AF standard so they can interoperate with other modern AF applications Drives down sustainment costs by moving to more affordable software and hosting solutions Provides an Automated Application Modernization Approach and Methods to transform AF applications Identifies potential organizational, non-technical dependencies or barriers and how to mitigate them Provides examples of several successful modernization projects Provides a listing of proven automated modernization tools and how to use them. Could become basis for application modernization \u201cAssembly Line\u201d or Center of Excellence (CoE). For applications: lower sustainment and upgrade costs, easier to maintain and improve, easier to integrate with apps/data, easier to rationalize","title":"2.6 Benefits"},{"location":"modern/3-1-guidance/","text":"","title":"3.1 Guidance"},{"location":"modern/3-10-limitations/","text":"","title":"3.10 Limitations"},{"location":"modern/3-2-standards/","text":"","title":"3.2 Standards"},{"location":"modern/3-3-common/","text":"","title":"3.3 Common Use Case"},{"location":"modern/3-4-approaches/","text":"","title":"3.4 Approaches"},{"location":"modern/3-5-methodologies/","text":"","title":"3.5 Methodologies"},{"location":"modern/3-6-patterns/","text":"","title":"3.6 Patterns"},{"location":"modern/3-7-barriers/","text":"","title":"3.7 Barriers and Challenges"},{"location":"modern/3-8-recommendations/","text":"","title":"3.8 Recommendations"},{"location":"modern/3-9-lessons/","text":"","title":"3.9 Lessons"},{"location":"modern/4-1-tooltypes/","text":"","title":"4.1 Tool Types"},{"location":"modern/4-2-toolmatrix/","text":"","title":"4.2 Tool Matrix"},{"location":"modern/5-1-best/","text":"","title":"5.1 Best Practices"},{"location":"modern/6-1-project/","text":"","title":"6.1 Project Team"},{"location":"modern/6-2-tech/","text":"","title":"6.2 Technical Team"},{"location":"modern/6-3-support/","text":"","title":"6.3 Support"},{"location":"modern/6-4-stakeholders/","text":"","title":"6.4 Stakeholders"},{"location":"modern/7-1-solutions/","text":"","title":"7.1 Identifying Viable Solutions"},{"location":"modern/7-2-alternatives/","text":"","title":"7.2 Analysis of Alternatives (AoA) Guidance"},{"location":"modern/8-1-requirements/","text":"","title":"8.1 Requirements"},{"location":"modern/8-2-tech/","text":"","title":"8.2 Technical Design"},{"location":"modern/8-3-cybersecurity/","text":"","title":"8.3 Cybersecurity"},{"location":"modern/9-1-phases/","text":"","title":"9.1 Phases"},{"location":"modern/9-2-risk/","text":"","title":"9.2 Risk Identification and Management"},{"location":"testing/1-1-purpose/","text":"1.1 Purpose AF/A4 published the \"US Air Force Enterprise Logistics Flight Plan v2.0\" (ELFP) in April 2016 and the subordinate document \"Enterprise Logistics Technology Annex\" (ELTA) in June of 2016. This plan and annex describe the desired \"synthesized logistics information\" future state of US Air Force (AF) Enterprise Logistics in 2035. BES believes that in order to maintain a path and schedule to achieve those long-term goals, a series of enabling initiatives are needed to accelerate current progress in order to achieve the necessary near-term milestones. It is the intent of the Business and Enterprise System (BES) to include the resulting Playbooks into the BES Process Directory (BPD) to ensure all members of Air Force Program Executive Office (AFPEO) BES, at all operating locations, have quick easy access to standard processes and templates for Defense Business System programs. DoD continues to recognize the need to apply automated software testing processes and procedures in a more consistent and repeatable manner. The Director of Operational Test & Evaluation (DOT&E) annual reports dating back to 2013 and earlier, show a concerted effort to improve the adoption rate of automation across the DoD. The National Defense Authorization Act (NDAA) for FY 2018 commissioned an Automated Testing Technologies study. There continues to be strong interest across the Services to investigate ways in which automation adoption and momentum can be increased. Defense Acquisition Policy, DoD Instruction 5000.02 and 5000.75 through AFMAN 63-144 contain language that encourages the use of automated testing. With the move towards more flexible and agile approaches to software development comes a greater urgency to implement test automation. This playbook addresses the desire to adopt test automation with practical, experienced-based methods and best practices. Automation Playbook Benefits Defines a common understanding of automation processes and terminology Establishes automation best practices to facilitate adoption by AF community Explains the various roles needed to start and maintain test automation Defines an overall architecture of automation applicable across projects and programs Helps programs understand how to migrate from manual to automated testing","title":"1.1 Purpose"},{"location":"testing/1-1-purpose/#11-purpose","text":"AF/A4 published the \"US Air Force Enterprise Logistics Flight Plan v2.0\" (ELFP) in April 2016 and the subordinate document \"Enterprise Logistics Technology Annex\" (ELTA) in June of 2016. This plan and annex describe the desired \"synthesized logistics information\" future state of US Air Force (AF) Enterprise Logistics in 2035. BES believes that in order to maintain a path and schedule to achieve those long-term goals, a series of enabling initiatives are needed to accelerate current progress in order to achieve the necessary near-term milestones. It is the intent of the Business and Enterprise System (BES) to include the resulting Playbooks into the BES Process Directory (BPD) to ensure all members of Air Force Program Executive Office (AFPEO) BES, at all operating locations, have quick easy access to standard processes and templates for Defense Business System programs. DoD continues to recognize the need to apply automated software testing processes and procedures in a more consistent and repeatable manner. The Director of Operational Test & Evaluation (DOT&E) annual reports dating back to 2013 and earlier, show a concerted effort to improve the adoption rate of automation across the DoD. The National Defense Authorization Act (NDAA) for FY 2018 commissioned an Automated Testing Technologies study. There continues to be strong interest across the Services to investigate ways in which automation adoption and momentum can be increased. Defense Acquisition Policy, DoD Instruction 5000.02 and 5000.75 through AFMAN 63-144 contain language that encourages the use of automated testing. With the move towards more flexible and agile approaches to software development comes a greater urgency to implement test automation. This playbook addresses the desire to adopt test automation with practical, experienced-based methods and best practices.","title":"1.1 Purpose"},{"location":"testing/1-1-purpose/#automation-playbook-benefits","text":"Defines a common understanding of automation processes and terminology Establishes automation best practices to facilitate adoption by AF community Explains the various roles needed to start and maintain test automation Defines an overall architecture of automation applicable across projects and programs Helps programs understand how to migrate from manual to automated testing","title":"Automation Playbook Benefits"},{"location":"testing/1-2-audience/","text":"1.2 Audience This playbook is intended for those individuals responsible for the management and engineering of test automation. It provides managers with the knowledge that will help them support programs looking to implement automation and it will provide engineers with the information they will need to successfully plan the implementation of a test automation solution. The approach is holistic in that it broadly defines many factors, not just technical ones, that are necessary to understand and apply when moving towards automation.","title":"1.2 Audience"},{"location":"testing/1-2-audience/#12-audience","text":"This playbook is intended for those individuals responsible for the management and engineering of test automation. It provides managers with the knowledge that will help them support programs looking to implement automation and it will provide engineers with the information they will need to successfully plan the implementation of a test automation solution. The approach is holistic in that it broadly defines many factors, not just technical ones, that are necessary to understand and apply when moving towards automation.","title":"1.2 Audience"},{"location":"testing/1-3-benefits/","text":"1.3 Benefits of Automated Testing An investment in automation can reap many rewards to the test team and overall project. There are primary and secondary benefits to using automated tools. Primary Benefits The primary benefits to using automation for testing can be summarized as follows: Faster test execution More reliable/repeatable test execution Increased quality from greater test coverage due to additional tests Facilitates testing of more complex scenarios Less error-prone than manual testing More consistent than manual testing Provides for unattended 24/7 test execution Ability to create additional test conditions from single script Reusability of tests within and across test events Ability to test more in the same or shorter time schedule Testing across a variety of software/hardware platforms Allows for the possibility of testing that which could not be tested manually Allows for increased frequency of testing Allows more effective use of testing resources (i.e. more test design, less manual execution) Secondary Benefits Secondary benefits for using automated test tools consist of support activities for testing, rather than the testing itself. These include: User account creation in advance of testing Database seeding with required test data Creation/management of test datasets Test environment configuration setup Pre-test initialization activities Post-test clean-up activities Automated data analysis of concluded test events Project and Program Benefits The use of automation brings benefits beyond testing to the project and program. These include: Improved software quality Earlier defect detection Fewer defects sent to next testing phase Greater efficiency in accomplishing testing Greater relevance of timely test results Reduced risk of deployment Improved test reporting Facilitated identification of defect root causes Reduced test execution cost Shortened test execution period Improved consistency of test executions Better adapted to iterative development where more frequent testing is required Improved feedback related to application quality Improved system reliability through repeatability and consistency of tests","title":"1.3 Benfits of Automated Testing"},{"location":"testing/1-3-benefits/#13-benefits-of-automated-testing","text":"An investment in automation can reap many rewards to the test team and overall project. There are primary and secondary benefits to using automated tools.","title":"1.3 Benefits of Automated Testing"},{"location":"testing/1-3-benefits/#primary-benefits","text":"The primary benefits to using automation for testing can be summarized as follows: Faster test execution More reliable/repeatable test execution Increased quality from greater test coverage due to additional tests Facilitates testing of more complex scenarios Less error-prone than manual testing More consistent than manual testing Provides for unattended 24/7 test execution Ability to create additional test conditions from single script Reusability of tests within and across test events Ability to test more in the same or shorter time schedule Testing across a variety of software/hardware platforms Allows for the possibility of testing that which could not be tested manually Allows for increased frequency of testing Allows more effective use of testing resources (i.e. more test design, less manual execution)","title":"Primary Benefits"},{"location":"testing/1-3-benefits/#secondary-benefits","text":"Secondary benefits for using automated test tools consist of support activities for testing, rather than the testing itself. These include: User account creation in advance of testing Database seeding with required test data Creation/management of test datasets Test environment configuration setup Pre-test initialization activities Post-test clean-up activities Automated data analysis of concluded test events","title":"Secondary Benefits"},{"location":"testing/1-3-benefits/#project-and-program-benefits","text":"The use of automation brings benefits beyond testing to the project and program. These include: Improved software quality Earlier defect detection Fewer defects sent to next testing phase Greater efficiency in accomplishing testing Greater relevance of timely test results Reduced risk of deployment Improved test reporting Facilitated identification of defect root causes Reduced test execution cost Shortened test execution period Improved consistency of test executions Better adapted to iterative development where more frequent testing is required Improved feedback related to application quality Improved system reliability through repeatability and consistency of tests","title":"Project and Program Benefits"},{"location":"testing/10-1-momentum/","text":"10.1 Building Momentum in Test Automation Test automation is not a static activity. Due to its nature, it is not as robust as a full-on software application solution. Automation may require review, evaluation, and tweaking from time to time in order that it continues to support testing activities. Therefore, there should always be someone assigned to monitor and maintain the test automation solution. Automation is additive. From the initial scripts that are automated, more scripts can be developed or reused. In this way, automation helps us build capability similar to stacking bricks to build a wall. Initial automation will be purpose-built, while later automation may be more about stringing prior automated scripts into larger and more complex business processes. Automation can also help with pre- and post-testing activities, as described in section 2.1. These activities often take time to carry out and the use of automation will help reduce the overall test execution activity time. Often other projects with similar technology will benefit from the automation developed. It is far easier to adapt an automation solution to another program than to build it again from scratch.","title":"10.1 Building Momentum in Test Automation"},{"location":"testing/10-1-momentum/#101-building-momentum-in-test-automation","text":"Test automation is not a static activity. Due to its nature, it is not as robust as a full-on software application solution. Automation may require review, evaluation, and tweaking from time to time in order that it continues to support testing activities. Therefore, there should always be someone assigned to monitor and maintain the test automation solution. Automation is additive. From the initial scripts that are automated, more scripts can be developed or reused. In this way, automation helps us build capability similar to stacking bricks to build a wall. Initial automation will be purpose-built, while later automation may be more about stringing prior automated scripts into larger and more complex business processes. Automation can also help with pre- and post-testing activities, as described in section 2.1. These activities often take time to carry out and the use of automation will help reduce the overall test execution activity time. Often other projects with similar technology will benefit from the automation developed. It is far easier to adapt an automation solution to another program than to build it again from scratch.","title":"10.1 Building Momentum in Test Automation"},{"location":"testing/10-2-landscape/","text":"10.2 Keeping Abreast of the Technology Landscape Software development continues to evolve and the SUT will, over time, incorporate these technology updates. The test automation solution must anticipate and adapt to these changes so that automation continues to work reliably and efficiently. It is recommended to perform a tool evaluation on a regular basis (every 2 years is recommended). Many tools, whether COTS, GOTS, or OSS regularly have incremental updates made to them. Before automatically updating the tool to the latest version, it is best to find out what improvements were made to the tool and if those improvements have any impact to the project using those tools. If it makes sense to make the update, this should first be tested in a separate environment to ensure that the update functions properly and does not inadvertently affect any other software or components previously installed. Generally, the latest release will have the latest security updates and features so updates are recommended. When an automated framework is built with modularity in mind, specific components of the framework can be swapped out or swapped in without having to re-write the entire framework. For example, a project that originally used browsers has now migrated to native mobile applications as well. The requirement now is to also support native mobile apps. This could potentially be accomplished in one of several ways: Identify if current tool in use also supports mobile apps Identify add-in component that supports mobile apps Identify new tool that supports mobile apps Each condition allows for a different approach to adding mobile app support to the framework.","title":"10.2 Keeping Abreast of the Technology Landscape"},{"location":"testing/10-2-landscape/#102-keeping-abreast-of-the-technology-landscape","text":"Software development continues to evolve and the SUT will, over time, incorporate these technology updates. The test automation solution must anticipate and adapt to these changes so that automation continues to work reliably and efficiently. It is recommended to perform a tool evaluation on a regular basis (every 2 years is recommended). Many tools, whether COTS, GOTS, or OSS regularly have incremental updates made to them. Before automatically updating the tool to the latest version, it is best to find out what improvements were made to the tool and if those improvements have any impact to the project using those tools. If it makes sense to make the update, this should first be tested in a separate environment to ensure that the update functions properly and does not inadvertently affect any other software or components previously installed. Generally, the latest release will have the latest security updates and features so updates are recommended. When an automated framework is built with modularity in mind, specific components of the framework can be swapped out or swapped in without having to re-write the entire framework. For example, a project that originally used browsers has now migrated to native mobile applications as well. The requirement now is to also support native mobile apps. This could potentially be accomplished in one of several ways: Identify if current tool in use also supports mobile apps Identify add-in component that supports mobile apps Identify new tool that supports mobile apps Each condition allows for a different approach to adding mobile app support to the framework.","title":"10.2 Keeping Abreast of the Technology Landscape"},{"location":"testing/10-3-improvements/","text":"10.3 Continuous Improvement Activities The areas that most benefit from ongoing inspection, review, and improvement will include: Functional script design Functional tests can be analyzed for their overall construction. This will include identifying areas of duplication with other scripts and extracting common functionality which can be better served by calling a shared component. Scripts that are excessively long will likely benefit from modularization. Error trapping Often, things go wrong during testing. This may include predicted and unpredicted errors. When errors occur they can cause havoc with automated tests. One approach to address this is through the use of error trapping. With error trapping we can direct abnormal application behavior to a known state. This is helpful when we have a large number of tests queued up and we do not want to hold up the next testing activity. Timing When executing tests we want the behavior to mimic real user interactions with the system. In order to do this we may need to slow down the rate at which automation interacts with the SUT. Adding a static number of seconds to pause the automation is not recommended as these static pauses can add up and eventually slow down test execution. A better approach is to use dynamic wait statements. These are often implemented by observing the attribute of a control. For example, if we want to make sure that the screen is ready to accept our order, we could dynamically interrogate the order field to make sure it is ready to accept input. This way, whether it takes .05 seconds or 3 seconds, the automation will wait the appropriate time before continuing with the order. Code Base Treat your automation code as any other software development project. This includes frequent reviews, coding guidelines, documentation, and the use of developers to help with some of the more challenging components. Performance After using automation for some time we may find that we are no longer able to execute all of our testing on an overnight run. This will require examining just what scripts are being executed, and evaluating if some of these no longer provide the value they once did and need to be re-written or merged into other more efficient scripts. Performance can also be measured at the component level, where much of the I/O activity occurs. You may want to evaluate if you we using resources effectively. For example, are you writing temporary results to a disk? If so, you may consider writing them to RAM which is much quicker. Script performance will also be affected with timing, as described above. Audit and reporting Understanding of what occurred during test execution can be enhance by audit logs and reporting. Audit logs can be developed that provide a level of granularity that helps us understand everything about our test execution. For example, we could develop a logging function that lets us know what windows were present at any time that the test was running. This make give us clues to windows that were not closed, messaging pop-ups that were not accounted for, or starting conditions that did not meet our expectations. Reporting requirements will come from stakeholders and once they start receiving information they will likely want additional information from the SUT and similar information but expressed differently. Satisfying the needs of stakeholders is an important part of keeping an automation solution viable.","title":"10.3 Continuous Improvement Activities"},{"location":"testing/10-3-improvements/#103-continuous-improvement-activities","text":"The areas that most benefit from ongoing inspection, review, and improvement will include:","title":"10.3 Continuous Improvement Activities"},{"location":"testing/10-3-improvements/#functional-script-design","text":"Functional tests can be analyzed for their overall construction. This will include identifying areas of duplication with other scripts and extracting common functionality which can be better served by calling a shared component. Scripts that are excessively long will likely benefit from modularization.","title":"Functional script design"},{"location":"testing/10-3-improvements/#error-trapping","text":"Often, things go wrong during testing. This may include predicted and unpredicted errors. When errors occur they can cause havoc with automated tests. One approach to address this is through the use of error trapping. With error trapping we can direct abnormal application behavior to a known state. This is helpful when we have a large number of tests queued up and we do not want to hold up the next testing activity.","title":"Error trapping"},{"location":"testing/10-3-improvements/#timing","text":"When executing tests we want the behavior to mimic real user interactions with the system. In order to do this we may need to slow down the rate at which automation interacts with the SUT. Adding a static number of seconds to pause the automation is not recommended as these static pauses can add up and eventually slow down test execution. A better approach is to use dynamic wait statements. These are often implemented by observing the attribute of a control. For example, if we want to make sure that the screen is ready to accept our order, we could dynamically interrogate the order field to make sure it is ready to accept input. This way, whether it takes .05 seconds or 3 seconds, the automation will wait the appropriate time before continuing with the order.","title":"Timing"},{"location":"testing/10-3-improvements/#code-base","text":"Treat your automation code as any other software development project. This includes frequent reviews, coding guidelines, documentation, and the use of developers to help with some of the more challenging components.","title":"Code Base"},{"location":"testing/10-3-improvements/#performance","text":"After using automation for some time we may find that we are no longer able to execute all of our testing on an overnight run. This will require examining just what scripts are being executed, and evaluating if some of these no longer provide the value they once did and need to be re-written or merged into other more efficient scripts. Performance can also be measured at the component level, where much of the I/O activity occurs. You may want to evaluate if you we using resources effectively. For example, are you writing temporary results to a disk? If so, you may consider writing them to RAM which is much quicker. Script performance will also be affected with timing, as described above.","title":"Performance"},{"location":"testing/10-3-improvements/#audit-and-reporting","text":"Understanding of what occurred during test execution can be enhance by audit logs and reporting. Audit logs can be developed that provide a level of granularity that helps us understand everything about our test execution. For example, we could develop a logging function that lets us know what windows were present at any time that the test was running. This make give us clues to windows that were not closed, messaging pop-ups that were not accounted for, or starting conditions that did not meet our expectations. Reporting requirements will come from stakeholders and once they start receiving information they will likely want additional information from the SUT and similar information but expressed differently. Satisfying the needs of stakeholders is an important part of keeping an automation solution viable.","title":"Audit and reporting"},{"location":"testing/10-4-process/","text":"10.4 Making the Process Repeatable Ultimately, we want to learn from our efforts and know that we can do this again for the next project. Repeatability of the process requires a sound overall design, clear implementation, and useful documentation. As the automation evolves, all aspects of its design need to be understood and documented so that as testers are rotated into and out of a program there is a permanence to the automation that was built.","title":"10.4 Making the Process Repeatable"},{"location":"testing/10-4-process/#104-making-the-process-repeatable","text":"Ultimately, we want to learn from our efforts and know that we can do this again for the next project. Repeatability of the process requires a sound overall design, clear implementation, and useful documentation. As the automation evolves, all aspects of its design need to be understood and documented so that as testers are rotated into and out of a program there is a permanence to the automation that was built.","title":"10.4 Making the Process Repeatable"},{"location":"testing/11-1-resources/","text":"11.1 Appendix A: Resources THE FOLLOWING RESOURCES PROVIDE CERTIFICATION AND ACCREDITED TRAINING FOR SOFTWARE TESTING AND TEST AUTOMATION TOPICS: International Software Testing Qualification Board https://www.istqb.org/ As of June 2018, ISTQB has administered over 830,000 exams and issued more than 605,000 certifications in over 120 countries world-wide. The scheme relies on a Body of Knowledge (Syllabi and Glossary) and exam rules that are applied consistently all over the world, with exams and supporting material being available in many languages. American Software Testing Qualification Board https://www.astqb.org/ The mission of ASTQB is to promote professionalism in Software Testing in the United States. We do this by providing and administering quality exams for the ISTQB, ASTQB and IQBBA certifications, by supporting and facilitating software training providers in delivering high quality courses, by actively engaging in the ISTQB working groups, and by supporting efforts to develop and encourage people who are already in or are entering the software testing profession. ASQ https://asq.org/cert/software-quality-engineer With individual and organizational members around the world, ASQ has the reputation and reach to bring together the diverse quality champions who are transforming the world's corporations, organizations and communities to meet tomorrow's critical challenges. The Certified Software Quality Engineer understands software quality development and implementation, software inspection, testing, verification and validation, and implements software development and maintenance processes and methods. QAI Global http://www.qaiusa.com/software-certifications/software-testing-certifications/ As the IT industry becomes more competitive, the ability for management to distinguish professional and skilled individuals in the field becomes mandatory. QAI Global Institute is the global program administrator for the International Software Certification Board (ISCB). Software Certifications has become recognized worldwide as the standard for information technology quality professionals - having certified over 50,000 professionals. ISCB test centers are located in 135 countries across 6 continents. Software certifications cover five major domains and provide eleven professional certifications. These internationally-recognized, examination-based and vendor-independent programs provide full career paths for professionals at all levels. THE FOLLOWING RESOURCES PROVIDE REPORTING ON AUTOMATED TEST TOOL TOPICS: Magic Quadrant for Software Testing Tools https://www.gartner.com/home The need to support faster time to market with higher quality is driving the demand for effective functional test automation tools. We evaluate vendors in this space to help application leaders who are modernizing software development select test automation tools that best match their needs. (note: may require subscription for access to reports) Carnegie Melon University Software Engineering Institute - The Importance of Automated Testing in Open Systems Architecture Initiatives https://insights.sei.cmu.edu/sei_blog/2014/03/the-importance-of-automated-testing-in-open-systems-architecture-initiatives.html The Better Buying Power 2.0 initiative is a concerted effort by the United States Department of Defense to achieve greater efficiencies in the development, sustainment, and re-competition of major defense acquisition programs through cost control, elimination of unproductive processes and bureaucracy, and promotion of open competition. Carnegie Melon University Software Engineering Institute - Five Keys to Effective Agile Test Automation for Government Programs https://resources.sei.cmu.edu/library/asset-view.cfm?assetid=503507 In this discussion-focused webinar, Bob Binder and SuZ Miller will discuss 5 key questions that government organizations contemplating embarking on adopting automated test techniques and tools in an Agile environment are likely to have. THE FOLLOWING RESOURCES PROVIDE FOR COLLABORATIVE DISCUSSIONS AROUND TEST TOOL TOPICS: SW Test Academy https://www.swtestacademy.com/ SW Test Academy (STA) is focused on mainly technical testing topics. In this site, you can find comprehensive descriptions and examples of test automation, performance testing, mobile testing, web service testing, API testing, DevOps, continuous integration, and similar topics. QA Testing Tools http://qatestingtools.com/ QA Testing Tools is an innovative platform and is the only website that gives you an Opportunity to read technical reviews on every software-testing tool, simultaneously giving you in-depth technical information, and comparison tables that direct you towards the most suitable group of tools to fulfill your requirements. Automate the Planet https://www.automatetheplanet.com/resources/ Learn how to write automated tests through working real-world examples. Stack Overflow https://stackoverflow.com/ Each month, over 50 million developers come to Stack Overflow to learn, share their knowledge, and build their careers. Software Testing and Quality Assurance Forums http://www.sqaforums.com/forums/ The online community for software testing and quality assurance professionals. Open Source Testing http://www.opensourcetesting.org/ The open source testing site aims to boost the profile of open source testing tools within the testing industry, principally by providing users with an easy to use gateway to information on the wide range of open source testing tools available. Test Automation Group on LinkedIn https://www.linkedin.com/groups/86204 LinkedIn is the world's largest professional network with more than 562 million users in more than 200 countries and territories worldwide. The Test Automation LinkedIn group is for people that are interested in QA test automation. The following issues can be found in the group discussions: Automation frameworks, Selenium, QTP, Web automation, Automation ROI, TestComplete, XUnit, JUnit, NUnit, JSystem, Automation strategics, Mobile testing (Android, iPhone, Blackberry), Load, agile, jobs and more! (Note: There are several additional groups in LinkedIn that cover test automation topics and specific tools)","title":"11.1 Appendix A: Resources"},{"location":"testing/11-1-resources/#111-appendix-a-resources","text":"","title":"11.1 Appendix A: Resources"},{"location":"testing/11-1-resources/#the-following-resources-provide-certification-and-accredited-training-for-software-testing-and-test-automation-topics","text":"","title":"THE FOLLOWING RESOURCES PROVIDE CERTIFICATION AND ACCREDITED TRAINING FOR SOFTWARE TESTING AND TEST AUTOMATION TOPICS:"},{"location":"testing/11-1-resources/#international-software-testing-qualification-board","text":"https://www.istqb.org/ As of June 2018, ISTQB has administered over 830,000 exams and issued more than 605,000 certifications in over 120 countries world-wide. The scheme relies on a Body of Knowledge (Syllabi and Glossary) and exam rules that are applied consistently all over the world, with exams and supporting material being available in many languages.","title":"International Software Testing Qualification Board"},{"location":"testing/11-1-resources/#american-software-testing-qualification-board","text":"https://www.astqb.org/ The mission of ASTQB is to promote professionalism in Software Testing in the United States. We do this by providing and administering quality exams for the ISTQB, ASTQB and IQBBA certifications, by supporting and facilitating software training providers in delivering high quality courses, by actively engaging in the ISTQB working groups, and by supporting efforts to develop and encourage people who are already in or are entering the software testing profession.","title":"American Software Testing Qualification Board"},{"location":"testing/11-1-resources/#asq","text":"https://asq.org/cert/software-quality-engineer With individual and organizational members around the world, ASQ has the reputation and reach to bring together the diverse quality champions who are transforming the world's corporations, organizations and communities to meet tomorrow's critical challenges. The Certified Software Quality Engineer understands software quality development and implementation, software inspection, testing, verification and validation, and implements software development and maintenance processes and methods.","title":"ASQ"},{"location":"testing/11-1-resources/#qai-global","text":"http://www.qaiusa.com/software-certifications/software-testing-certifications/ As the IT industry becomes more competitive, the ability for management to distinguish professional and skilled individuals in the field becomes mandatory. QAI Global Institute is the global program administrator for the International Software Certification Board (ISCB). Software Certifications has become recognized worldwide as the standard for information technology quality professionals - having certified over 50,000 professionals. ISCB test centers are located in 135 countries across 6 continents. Software certifications cover five major domains and provide eleven professional certifications. These internationally-recognized, examination-based and vendor-independent programs provide full career paths for professionals at all levels.","title":"QAI Global"},{"location":"testing/11-1-resources/#the-following-resources-provide-reporting-on-automated-test-tool-topics","text":"","title":"THE FOLLOWING RESOURCES PROVIDE REPORTING ON AUTOMATED TEST TOOL TOPICS:"},{"location":"testing/11-1-resources/#magic-quadrant-for-software-testing-tools","text":"https://www.gartner.com/home The need to support faster time to market with higher quality is driving the demand for effective functional test automation tools. We evaluate vendors in this space to help application leaders who are modernizing software development select test automation tools that best match their needs. (note: may require subscription for access to reports)","title":"Magic Quadrant for Software Testing Tools"},{"location":"testing/11-1-resources/#carnegie-melon-university-software-engineering-institute-the-importance-of-automated-testing-in-open-systems-architecture-initiatives","text":"https://insights.sei.cmu.edu/sei_blog/2014/03/the-importance-of-automated-testing-in-open-systems-architecture-initiatives.html The Better Buying Power 2.0 initiative is a concerted effort by the United States Department of Defense to achieve greater efficiencies in the development, sustainment, and re-competition of major defense acquisition programs through cost control, elimination of unproductive processes and bureaucracy, and promotion of open competition.","title":"Carnegie Melon University Software Engineering Institute - The Importance of Automated Testing in Open Systems Architecture Initiatives"},{"location":"testing/11-1-resources/#carnegie-melon-university-software-engineering-institute-five-keys-to-effective-agile-test-automation-for-government-programs","text":"https://resources.sei.cmu.edu/library/asset-view.cfm?assetid=503507 In this discussion-focused webinar, Bob Binder and SuZ Miller will discuss 5 key questions that government organizations contemplating embarking on adopting automated test techniques and tools in an Agile environment are likely to have.","title":"Carnegie Melon University Software Engineering Institute - Five Keys to Effective Agile Test Automation for Government Programs"},{"location":"testing/11-1-resources/#the-following-resources-provide-for-collaborative-discussions-around-test-tool-topics","text":"","title":"THE FOLLOWING RESOURCES PROVIDE FOR COLLABORATIVE DISCUSSIONS AROUND TEST TOOL TOPICS:"},{"location":"testing/11-1-resources/#sw-test-academy","text":"https://www.swtestacademy.com/ SW Test Academy (STA) is focused on mainly technical testing topics. In this site, you can find comprehensive descriptions and examples of test automation, performance testing, mobile testing, web service testing, API testing, DevOps, continuous integration, and similar topics.","title":"SW Test Academy"},{"location":"testing/11-1-resources/#qa-testing-tools","text":"http://qatestingtools.com/ QA Testing Tools is an innovative platform and is the only website that gives you an Opportunity to read technical reviews on every software-testing tool, simultaneously giving you in-depth technical information, and comparison tables that direct you towards the most suitable group of tools to fulfill your requirements.","title":"QA Testing Tools"},{"location":"testing/11-1-resources/#automate-the-planet","text":"https://www.automatetheplanet.com/resources/ Learn how to write automated tests through working real-world examples.","title":"Automate the Planet"},{"location":"testing/11-1-resources/#stack-overflow","text":"https://stackoverflow.com/ Each month, over 50 million developers come to Stack Overflow to learn, share their knowledge, and build their careers.","title":"Stack Overflow"},{"location":"testing/11-1-resources/#software-testing-and-quality-assurance-forums","text":"http://www.sqaforums.com/forums/ The online community for software testing and quality assurance professionals.","title":"Software Testing and Quality Assurance Forums"},{"location":"testing/11-1-resources/#open-source-testing","text":"http://www.opensourcetesting.org/ The open source testing site aims to boost the profile of open source testing tools within the testing industry, principally by providing users with an easy to use gateway to information on the wide range of open source testing tools available.","title":"Open Source Testing"},{"location":"testing/11-1-resources/#test-automation-group-on-linkedin","text":"https://www.linkedin.com/groups/86204 LinkedIn is the world's largest professional network with more than 562 million users in more than 200 countries and territories worldwide. The Test Automation LinkedIn group is for people that are interested in QA test automation. The following issues can be found in the group discussions: Automation frameworks, Selenium, QTP, Web automation, Automation ROI, TestComplete, XUnit, JUnit, NUnit, JSystem, Automation strategics, Mobile testing (Android, iPhone, Blackberry), Load, agile, jobs and more! (Note: There are several additional groups in LinkedIn that cover test automation topics and specific tools)","title":"Test Automation Group on LinkedIn"},{"location":"testing/11-2-tools/","text":"11.2 Appendix B: Test Tools KEY: M - Test Management/Reporting F - Functional/Regression Testing P - Performance Testing/Monitoring S - Security Testing","title":"11.2 Appendix B: Test Tools"},{"location":"testing/11-2-tools/#112-appendix-b-test-tools","text":"KEY: M - Test Management/Reporting F - Functional/Regression Testing P - Performance Testing/Monitoring S - Security Testing","title":"11.2 Appendix B: Test Tools"},{"location":"testing/2-1-what/","text":"2.1 What is Automation? Automation, in its simplest form, is the mechanization of a manual process that allows for that process to operate automatically. There are many applications of automation, and there are many ways in which we can test. Using automation allows us to mechanize an otherwise manual process for testing. There are many additional uses for automation, that are not specifically for testing, that can be performed with automated test tools. Examples of these may include pre-test activities such as creating user accounts and building data sets, which will ultimately be used in automated testing. Functional and regression test activities are those most frequently targeted for the use of automated testing. Additional uses for automation of tests include API testing, performance testing, security testing, and automation of test management activities.","title":"2.1 What is Automation?"},{"location":"testing/2-1-what/#21-what-is-automation","text":"Automation, in its simplest form, is the mechanization of a manual process that allows for that process to operate automatically. There are many applications of automation, and there are many ways in which we can test. Using automation allows us to mechanize an otherwise manual process for testing. There are many additional uses for automation, that are not specifically for testing, that can be performed with automated test tools. Examples of these may include pre-test activities such as creating user accounts and building data sets, which will ultimately be used in automated testing. Functional and regression test activities are those most frequently targeted for the use of automated testing. Additional uses for automation of tests include API testing, performance testing, security testing, and automation of test management activities.","title":"2.1 What is Automation?"},{"location":"testing/2-2-lifecycle/","text":"2.2 Automation in Software Lifecycle Methodologies Software development methodologies are evolving from traditional Waterfall to more recent Agile approaches. Testing is part of the overall software development process. When implementing testing automation, it must align with and conform to the project management methodology. In projects using Waterfall project management, cycles for development are long and the automation team can plan accordingly. This would include setting up many of the functions and components and building an automation framework. With Agile projects, there isn't as much time within each sprint to build out a complete automation framework so alternate solutions should be identified.","title":"2.2 Automation in Software Lifecycle Methodologies"},{"location":"testing/2-2-lifecycle/#22-automation-in-software-lifecycle-methodologies","text":"Software development methodologies are evolving from traditional Waterfall to more recent Agile approaches. Testing is part of the overall software development process. When implementing testing automation, it must align with and conform to the project management methodology. In projects using Waterfall project management, cycles for development are long and the automation team can plan accordingly. This would include setting up many of the functions and components and building an automation framework. With Agile projects, there isn't as much time within each sprint to build out a complete automation framework so alternate solutions should be identified.","title":"2.2 Automation in Software Lifecycle Methodologies"},{"location":"testing/2-3-benefits/","text":"2.3 Benefits to AF The AF is continually enhancing, upgrading, and/or replacing software systems and applications to meet mission and user needs. This presents an ideal opportunity to introduce test automation practices so that future iterations of the software development process will show greater: testing efficiency through faster execution testing effectiveness through additional functional coverage test repeatability through programmed execution improvement in timely reporting of system quality","title":"2.3 Benefits to AF"},{"location":"testing/2-3-benefits/#23-benefits-to-af","text":"The AF is continually enhancing, upgrading, and/or replacing software systems and applications to meet mission and user needs. This presents an ideal opportunity to introduce test automation practices so that future iterations of the software development process will show greater: testing efficiency through faster execution testing effectiveness through additional functional coverage test repeatability through programmed execution improvement in timely reporting of system quality","title":"2.3 Benefits to AF"},{"location":"testing/2-4-pitfalls/","text":"2.4 Avoiding Pitfalls Individuals involved in testing and test automation come from various walks of life. There is no Tester University or College of Automation. This creates a situation where there is a lack of consistency and knowledge that someone in this line of work should know in order to perform effectively. Over time there have been training and certification programs in DoD and industry that support the need for skills and knowledge of career testers and automators. However, success in automation is not solely a technical challenge. It is a multifaceted discipline requiring management, engineering, and other disciplines. This Test Automation Playbook will guide the AF on how to best approach automated testing.","title":"2.4 Avoiding Pitfalls"},{"location":"testing/2-4-pitfalls/#24-avoiding-pitfalls","text":"Individuals involved in testing and test automation come from various walks of life. There is no Tester University or College of Automation. This creates a situation where there is a lack of consistency and knowledge that someone in this line of work should know in order to perform effectively. Over time there have been training and certification programs in DoD and industry that support the need for skills and knowledge of career testers and automators. However, success in automation is not solely a technical challenge. It is a multifaceted discipline requiring management, engineering, and other disciplines. This Test Automation Playbook will guide the AF on how to best approach automated testing.","title":"2.4 Avoiding Pitfalls"},{"location":"testing/3-1-acquisitions/","text":"3.1 Acquisitions Automated testing is an integral part of modern software development. As such, requirements for automated testing should be identified in the requirement document (e.g., Statement of Objective or Statement of Work). It is essential to establish best practices as requirements at the onset of all new software acquisitions, not only to ensure they are delivered during execution, but to ensure quality vendors respond. The DoD 5000 already prescribes such requirements. Specifically, DoD Instruction (DoDI) 5000.02 defines, under DT&E Planning Considerations, the requirement to \"develop a software test automation strategy\" and under OT&E for software, for regression tests to be \"preferably automated.\" This policy, now superseded by policy 5000.75 for business systems and implemented with AFMAN 63-144, includes a directive to \"Employ effective use of integrated testing and automated software test tools.\" Acquisition guidelines should be stated at the objective level, however, they should allow for the ability for the contractor to recommend industry best testing tools that may be implemented with Government approval. The goal is to obtain the best solution for the program and the Government. Many contracting organizations with automation skills have honed the techniques necessary to deliver quality, reliable automated test solutions. If it is envisioned that government will take over the use and management of the automated testing suite, guidelines for training and transitioning of the solution should be required as part of the Statement of Objective or Statement of Work. If a different technology is envisioned for any continuation of test automation, then the test data used to drive the automation should be delivered in a standardized manner that allows for reusability and adaptability to another automated solution.","title":"3.1 Acquisitions"},{"location":"testing/3-1-acquisitions/#31-acquisitions","text":"Automated testing is an integral part of modern software development. As such, requirements for automated testing should be identified in the requirement document (e.g., Statement of Objective or Statement of Work). It is essential to establish best practices as requirements at the onset of all new software acquisitions, not only to ensure they are delivered during execution, but to ensure quality vendors respond. The DoD 5000 already prescribes such requirements. Specifically, DoD Instruction (DoDI) 5000.02 defines, under DT&E Planning Considerations, the requirement to \"develop a software test automation strategy\" and under OT&E for software, for regression tests to be \"preferably automated.\" This policy, now superseded by policy 5000.75 for business systems and implemented with AFMAN 63-144, includes a directive to \"Employ effective use of integrated testing and automated software test tools.\" Acquisition guidelines should be stated at the objective level, however, they should allow for the ability for the contractor to recommend industry best testing tools that may be implemented with Government approval. The goal is to obtain the best solution for the program and the Government. Many contracting organizations with automation skills have honed the techniques necessary to deliver quality, reliable automated test solutions. If it is envisioned that government will take over the use and management of the automated testing suite, guidelines for training and transitioning of the solution should be required as part of the Statement of Objective or Statement of Work. If a different technology is envisioned for any continuation of test automation, then the test data used to drive the automation should be delivered in a standardized manner that allows for reusability and adaptability to another automated solution.","title":"3.1 Acquisitions"},{"location":"testing/3-2-management/","text":"3.2 Management Support The role of management, at all levels, is key to the success of test automation in AF programs. Management can: Identify projects and programs where automation would likely provide benefits to the overall testing process Identify staff (government or contractors) who can be targeted to deliver automated solutions Identify relevant training and certification to ready staff for automation Anticipate funding requirements for test automation resources (people, tools, environments, process adjustment) Ensure that an adequate assessment of test tools takes place through market research and evaluation Provide equipment and environments in which to develop and execute automation Adopt cross-enterprise \"Best Practices\" for sharing of test automation methods and technology Define contract structures and CLINs that promote use of automation 3.2.1 Identifying and Funding Resources From a funding perspective, there are three areas a manager should consider when planning for test automation. People Who will be tasked to do the automation? And who will be implementing the testing tools framework? Government staff? Contractors? A combination? This needs to be decided on up front as it will affect the process by which these resources are identified and the timeframe under which they can be brought in to accomplish the work, including any training time required prior to project start. If the automation skillset is not easily found within government, a first step might be to contract the work out to an organization with expertise in this area. This will save time and money, and avoid missteps. Test Tools Software test tools have costs associated with licenses, maintenance, training, and support. Even tools that are open source software will require maintenance, training, and possibly support. However, the absence of an initial license fee may provide a significant cost savings (see section 6.3). This playbook will describe industry standard testing tools and the trade-offs between open source versus Commercial-Off-The-Shelf (COTS) software. Test Environments Test environments where automated tools reside can include: - An individual tester's workstation - A test lab with specific computers dedicated to automation - A server with virtual machine images - A cloud-based setup - A cloud-based service (SaaS)","title":"3.2 Management Support"},{"location":"testing/3-2-management/#32-management-support","text":"The role of management, at all levels, is key to the success of test automation in AF programs. Management can: Identify projects and programs where automation would likely provide benefits to the overall testing process Identify staff (government or contractors) who can be targeted to deliver automated solutions Identify relevant training and certification to ready staff for automation Anticipate funding requirements for test automation resources (people, tools, environments, process adjustment) Ensure that an adequate assessment of test tools takes place through market research and evaluation Provide equipment and environments in which to develop and execute automation Adopt cross-enterprise \"Best Practices\" for sharing of test automation methods and technology Define contract structures and CLINs that promote use of automation","title":"3.2 Management Support"},{"location":"testing/3-2-management/#321-identifying-and-funding-resources","text":"From a funding perspective, there are three areas a manager should consider when planning for test automation.","title":"3.2.1 Identifying and Funding Resources"},{"location":"testing/3-2-management/#people","text":"Who will be tasked to do the automation? And who will be implementing the testing tools framework? Government staff? Contractors? A combination? This needs to be decided on up front as it will affect the process by which these resources are identified and the timeframe under which they can be brought in to accomplish the work, including any training time required prior to project start. If the automation skillset is not easily found within government, a first step might be to contract the work out to an organization with expertise in this area. This will save time and money, and avoid missteps.","title":"People"},{"location":"testing/3-2-management/#test-tools","text":"Software test tools have costs associated with licenses, maintenance, training, and support. Even tools that are open source software will require maintenance, training, and possibly support. However, the absence of an initial license fee may provide a significant cost savings (see section 6.3). This playbook will describe industry standard testing tools and the trade-offs between open source versus Commercial-Off-The-Shelf (COTS) software.","title":"Test Tools"},{"location":"testing/3-2-management/#test-environments","text":"Test environments where automated tools reside can include: - An individual tester's workstation - A test lab with specific computers dedicated to automation - A server with virtual machine images - A cloud-based setup - A cloud-based service (SaaS)","title":"Test Environments"},{"location":"testing/3-3-technical/","text":"3.3 Technical Support Developer The software developer plays a key role in supporting the test automation team. The developer has first-hand knowledge of the tools and methods used to construct the software and system. This information will help guide the automation team in the evaluation and selection of tools that are compatible with the tools selected by the development team. Often developers can further aid the automation effort (and associated maintenance) by using uniquely identifiable names for objects/controls displayed by the software application. This is equally applicable to client-based or browser-based solutions. The key point here is that a little forethought by the developers can go a long way to facilitate the recognition of objects/controls by the automation team. For example, we avoid a common scenario where the properties of an application for a user \"name\" and user \"account\" show up as U25523 and A00056 within the automation tool rather than USER_NAME and USER_ACCOUNT. Database Administrator Data forms a large part of tests, and test automation amplifies this. The role of the Database Administrator is key in assisting the needs of the automation team. These may include: Assistance in configuring and selecting a database which emulates a production-like database The ability to restore or revert a database to an earlier condition for retesting Assistance in executing direct queries against the database in order to validate application behavior Systems Administrator The System Administrator (SA) ensures that the system, including software, network, and interfaces are available to the test automation team. Additionally, the SA controls the updates (patches, security releases, etc.) that are applied to the servers on which the test automation solution runs. This coordination is very important as any changes to the underlying system may have consequences to the reliability of the test automation solution, with the possibility that it ceases to function. The SA will also help the automation team with any updates to automation software that need to installed on the testing infrastructure and can assist the test automation team by providing an environment that emulates a production-like environment.","title":"3.3 Technical Support"},{"location":"testing/3-3-technical/#33-technical-support","text":"","title":"3.3 Technical Support"},{"location":"testing/3-3-technical/#developer","text":"The software developer plays a key role in supporting the test automation team. The developer has first-hand knowledge of the tools and methods used to construct the software and system. This information will help guide the automation team in the evaluation and selection of tools that are compatible with the tools selected by the development team. Often developers can further aid the automation effort (and associated maintenance) by using uniquely identifiable names for objects/controls displayed by the software application. This is equally applicable to client-based or browser-based solutions. The key point here is that a little forethought by the developers can go a long way to facilitate the recognition of objects/controls by the automation team. For example, we avoid a common scenario where the properties of an application for a user \"name\" and user \"account\" show up as U25523 and A00056 within the automation tool rather than USER_NAME and USER_ACCOUNT.","title":"Developer"},{"location":"testing/3-3-technical/#database-administrator","text":"Data forms a large part of tests, and test automation amplifies this. The role of the Database Administrator is key in assisting the needs of the automation team. These may include: Assistance in configuring and selecting a database which emulates a production-like database The ability to restore or revert a database to an earlier condition for retesting Assistance in executing direct queries against the database in order to validate application behavior","title":"Database Administrator"},{"location":"testing/3-3-technical/#systems-administrator","text":"The System Administrator (SA) ensures that the system, including software, network, and interfaces are available to the test automation team. Additionally, the SA controls the updates (patches, security releases, etc.) that are applied to the servers on which the test automation solution runs. This coordination is very important as any changes to the underlying system may have consequences to the reliability of the test automation solution, with the possibility that it ceases to function. The SA will also help the automation team with any updates to automation software that need to installed on the testing infrastructure and can assist the test automation team by providing an environment that emulates a production-like environment.","title":"Systems Administrator"},{"location":"testing/3-4-domain/","text":"3.4 Domain Support The Business User, Business Analyst, Product Owner, and other similar roles are subject matter experts (SMEs) when it comes to understanding how the software should work and what it needs to do in order to meet stated requirements, objectives and defined user stories. Domain knowledge is hard to come by and usually comes from individuals who have had or continue to have direct roles in using the business functionality that a system provides. These are the go-to people when a thorough understanding of use cases is required.","title":"3.4 Domain Support"},{"location":"testing/3-4-domain/#34-domain-support","text":"The Business User, Business Analyst, Product Owner, and other similar roles are subject matter experts (SMEs) when it comes to understanding how the software should work and what it needs to do in order to meet stated requirements, objectives and defined user stories. Domain knowledge is hard to come by and usually comes from individuals who have had or continue to have direct roles in using the business functionality that a system provides. These are the go-to people when a thorough understanding of use cases is required.","title":"3.4 Domain Support"},{"location":"testing/3-5-team/","text":"3.5 Automation Team Members The roles of the core automation team are important to define up front. The individuals filling those roles should have the necessary skills and experience to properly implement a maintainable, expandable automation solution. They should also be current on industry standards and have the ability to provide recommendations for changes based on the environment and user needs. The following roles can be assigned to individuals or could be performed by one or more individuals, depending on the complexity of the software project. Test Automation Architect The Automation Architect is the senior Subject Matter Expert (SME) in automation and is responsible for the overall design and implementation of a test automation solution. The automation architecture will be dependent on many factors, including: complexity of the system or software under test (SUT); number of interfaces to other systems or subsystems; richness of the Integrated Development Environment (IDE) controls; and technical level of automation team. The Architect needs to have a broad vision of what current and forthcoming requirements for automation may be based on for overall system architectures. Selecting appropriate tools to meet a diverse set of needs and understanding how multiple tools may need to be integrated for complex testing and reporting requirements will need to be considered as part of the planning process. Test Automation Engineer The Test Automation Engineer is an intermediate-level technical individual who is responsible for developing and maintaining automation components and subsystems. This may include development of purpose-built functions, creation of function libraries, and documentation of the test automation components. As new requirements for automation are defined (e.g. a new \"calendar\" control being added) the automation engineer makes the appropriate updates/additions to the test automation solution, including documentation, to incorporate the new functionality. There may be multiple roles for an automation engineer which may include: Development of input/output (I/O) functions Development of test interfaces to external systems Development of user interface (UI) component test functions Development of navigational paradigms across the application Development of timing and synchronization requirements Development of logging and reporting functions Test Automation User The Test Automation User is the individual or individuals who are the \"customer\" of the test automation solution. They need not be concerned with the technical implementation of a test automation solution, but rather with the ability to use automation to execute tests and report findings. A properly designed test automation solution will allow a user to select a test, the corresponding data set, and run the test. At the conclusion of the test execution, the user should be presented with a report indicating the pass/fail status of the test and any information gathered as a result of a passed/failed test. Often the user of test automation will be a manual tester, a business analyst, or another individual with a strong understanding of the software or system and the requirements that it needs to meet. These individuals may not have programming backgrounds and would not be productive or motivated to take on the role of an automation engineer. By identifying and assigning roles based on skills we are able to keep each individual fully productive and motivated and allow automation to be used by the team, not just by select technical individuals.","title":"3.5 Automation Team Members"},{"location":"testing/3-5-team/#35-automation-team-members","text":"The roles of the core automation team are important to define up front. The individuals filling those roles should have the necessary skills and experience to properly implement a maintainable, expandable automation solution. They should also be current on industry standards and have the ability to provide recommendations for changes based on the environment and user needs. The following roles can be assigned to individuals or could be performed by one or more individuals, depending on the complexity of the software project.","title":"3.5 Automation Team Members"},{"location":"testing/3-5-team/#test-automation-architect","text":"The Automation Architect is the senior Subject Matter Expert (SME) in automation and is responsible for the overall design and implementation of a test automation solution. The automation architecture will be dependent on many factors, including: complexity of the system or software under test (SUT); number of interfaces to other systems or subsystems; richness of the Integrated Development Environment (IDE) controls; and technical level of automation team. The Architect needs to have a broad vision of what current and forthcoming requirements for automation may be based on for overall system architectures. Selecting appropriate tools to meet a diverse set of needs and understanding how multiple tools may need to be integrated for complex testing and reporting requirements will need to be considered as part of the planning process.","title":"Test Automation Architect"},{"location":"testing/3-5-team/#test-automation-engineer","text":"The Test Automation Engineer is an intermediate-level technical individual who is responsible for developing and maintaining automation components and subsystems. This may include development of purpose-built functions, creation of function libraries, and documentation of the test automation components. As new requirements for automation are defined (e.g. a new \"calendar\" control being added) the automation engineer makes the appropriate updates/additions to the test automation solution, including documentation, to incorporate the new functionality. There may be multiple roles for an automation engineer which may include: Development of input/output (I/O) functions Development of test interfaces to external systems Development of user interface (UI) component test functions Development of navigational paradigms across the application Development of timing and synchronization requirements Development of logging and reporting functions","title":"Test Automation Engineer"},{"location":"testing/3-5-team/#test-automation-user","text":"The Test Automation User is the individual or individuals who are the \"customer\" of the test automation solution. They need not be concerned with the technical implementation of a test automation solution, but rather with the ability to use automation to execute tests and report findings. A properly designed test automation solution will allow a user to select a test, the corresponding data set, and run the test. At the conclusion of the test execution, the user should be presented with a report indicating the pass/fail status of the test and any information gathered as a result of a passed/failed test. Often the user of test automation will be a manual tester, a business analyst, or another individual with a strong understanding of the software or system and the requirements that it needs to meet. These individuals may not have programming backgrounds and would not be productive or motivated to take on the role of an automation engineer. By identifying and assigning roles based on skills we are able to keep each individual fully productive and motivated and allow automation to be used by the team, not just by select technical individuals.","title":"Test Automation User"},{"location":"testing/4-1-tools/","text":"4.1 Tools Target Areas Test automation tools support the full software development lifecycle. There is likely a tool for every aspect of software that is developed. Tools are categorized by types of testing they can perform. Test types can span multiple test levels (e.g. component, integration, system, etc.). The following represents broad categories of functionality that test tools are capable of. 4.1.1 Functional & Regression Testing By far the most common use of automation is to test application functionality. This can include new functionality added in a release or retesting of existing functionality. In both cases the goal is to use automation to test the functionality of an application or system. Applications use a variety of user interfaces. Most common today is the graphical user interface (GUI) that make up a large proportion of existing interfaces. All modern computers, tablets, and smartphones use a GUI (e.g. Windows, MacOS, Android, iOS, etc.). Historically, interfaces to systems were very crude and text based (e.g. Mainframe, Terminal, etc.). Some continue to be that way as the text based interface has very low overhead required by some applications or underlying hardware. Interface (UI or GUI) testing test tools must be able to recognize the interface graphical or otherwise, and the fields or objects on that interface. Where there is a standard implementation of the interface the test tool can easily recognize and interact with the fields or objects. For fully graphical interfaces there are a number of significant elements which can be tested or verified for each object or control. For example, a simple text field where a user enters a name can have many attributes: value (what data is in the field); focus (is this the field where the cursor is at); enabled (can we interact with this field); etc. Challenges: Not all interfaces are implemented consistently and some objects within an application may not be recognized by automation Use of third-party controls embedded into an interface can cause challenges in automation Some automation may need to be custom-coded. This requires programming skill and knowledge that should be performed by individuals with this level of experience. Migrating from one tool to another can pose challenges Adapting to changing technology (browser, OS, etc.) requires tools that are compatible Best Practices: Understand the full suite of technology that is being implemented for the SUT. Don't take a vendor or a reviewer as the last word on compatibility without the using the tool in its intended environment to ensure it is compatible. Look at tools that are compatible with the level of skill in the organization 4.1.2 API Testing Application Programming Interface (API) testing is testing that does not involve a user interface (e.g., text or graphical interface). This is often referred to as client-less testing or non-GUI testing. It simply means that there is no direct user interface by which the tester interacts. However, data is still passed to the application, the application responds, and these interactions can be measured, verified, and reported on with automated test tools. Most applications that have a UI or GUI also are communicating at the API level. With API testing, rather than interacting with an object or control, we interact with a function or service. In order to do so we need to understand the structure of data that the function or service is expected to respond with. If we send the wrong data structure will it identify it as an error or do something unexpectedly? If we send data and receive no return data, what does that tell us? There needs to be a full understanding of what the function or service is supposed to do so that verification can be effective. This includes an understanding of knowing what the function or service is not supposed to do as well. Challenges: Working with functions and services can be a little like working in the dark because you don't have the familiar context of UI controls Tools that are compatible with the implemented services Tools that can test APIs across devices and operating systems APIs may degrade in performance with multiple simultaneous calls to them Best Practices: Understand what you are testing and get the full specification before you start Make sure that you change only one variable at a time to understand the effect of changes Have some known baseline test interfaces to start with in order to realize the expected behavior Work closely with the API developer or vendor who implemented the function or service to fully understand the expected behavior and to report any unusual or missing return data values. 4.1.3 Performance Testing Performance testing is not concerned directly with testing functionality, but rather testing that functionality under load. Hence it is categorized as non-functional testing. Performance testing is difficult to impossible to do without test tools. In years past, rooms full of people and computers attempted to do performance testing but the cost was high, the consistency was low, and the breadth of testing left much to be desired. Modern performance testing tools simulate those same people on those same computers. Now, however, all the people and all the computers can be contained within one powerful computer. For extremely large simulations with heavy loads, performance testing can be distributed across multiple computers. Performance testing tools have 3 areas of functionality: Main controller The controller function starts tests, ramps them up or down, and stops tests. The controller function schedules the testing event and runs the operational profiles (often referred to as a scenarios) to simulate the conditions of a system in use. User Script The user script is the recorded or programmed sequence of events, or operational profile, that mimic the way in which a user interacts with a system. Running the same user script concurrently is how the test tool simulates multiple users performing the same function. User scripts often contain timing information (or wait statements as they are often called) that help regulate the speed of execution to something representative of a human user. Otherwise, scripts could run at speeds far faster than how real users interact with a system, which would not accurately represent the performance of the system. Reporting and Analysis Performance test tools need to have robust reporting and analysis capabilities in order to accurately convey what has transpired. Reporting will include transactional timing information (e.g., how long it took for a search to return) and aggregate data (e.g., all searches on average took no more than 3 seconds to complete). Reporting can also include system resource utilization (e.g., caching, CPU, queues, etc.) and analysis can help correlate how system resources are impacted by heavy transactional use. Cross analysis reporting aides in comparing subsequent test runs in order to evaluate effects on changes. Challenges: Finding an environment that mimics production is difficult and expensive Test tools can be very expensive Performance testing can affect other users sharing that environment Improperly created setup tests may yield no errors when actual errors exist Erroneous results can result from lack of proper correlation or using the wrong database Best Practices: Work with stakeholders to accurately define the various operational profiles Do not make multiple changes between tests as it makes it difficult to know each impact Test each operational profile individually and under load first before adding other profiles Keep scripts and data updated to reflect current changes in SUT Look at the test logs for errors that may not be present in the reports 4.1.4 Security Testing Security testing tools help analyze and test applications for possible security vulnerabilities. There are 3 areas of interest for security testing: static, dynamic, and interactive [ref: Gartner Magic Quadrant for Application Security Testing]. Static analysis refers to looking at the application code from the inside, not when executed. Dynamic analysis is performing testing against a running application. There is overlap with the security qualities identified in static and dynamic security testing and interactive tests span both. Therefore it is advisable to begin security testing early in the software development lifecycle, as code is developed so that it can be assessed. Static Application Security Testing Static application security testing (SAST) of an application refers to analyzing the underlying application code for vulnerabilities. This may include source code, byte code, or binaries. Due to the visibility of the code these tests are often referred to as \"white box\" tests. Scanning of code can be accomplished before any code is compiled. Examining code can assist in uncovering vulnerabilities including SQL Injection, Buffer Overflows, Cross-Site Scripting, and Cross-Site Request Forgery. Dynamic Application Security Testing Dynamic Application Security Testing (DAST) provides applications that are running to be analyzed for possible security flaws. These may include traditional client server applications of web-based applications. DAST has no visibility to the underlying application and therefore is referred to as \"black box\" testing. Interactive Application Security Testing Interactive Application Security Testing (IAST) allows visibility of possible vulnerabilities simultaneously within the application, often through the use of an agent within the runtime environment, and externally via application interfaces. This can include penetration testing which includes the scanning of ports for vulnerabilities. A set of complementary and overlapping vulnerabilities can be identified with SAST, DAST, and IAST tools at different phase of the software development lifecycle. However, automated tools cannot find all vulnerabilities so be careful to not get a false sense of security. A comprehensive guide for cyber security testing can be found at [ref: DoD Cybersecurity Test and Evaluation Guidebook, version 2.0] Blue team - penetration test. Red team - hacker to break into application. Red team in a box. Additional references for web application security available at: www.owasp.org. Challenges: Getting developers to use SAST tools early on Tools may have difficulty parsing through code Tools will not find all vulnerabilities Understand the tool capabilities and how to properly set up for testing Best Practices: Use tools regularly during development before code base becomes to large Security folks being proactive and helping program early Run tools on test and production environments Use a combination of best-in-class tools Identify and perform tests manually which tools may miss Decide what tests need to be run through automation. Automate when necessary as setup may take a lot of effort 4.1.5 Test Management Test Management tools provide for collaboration and reporting of test events. These tools can be a one-stop repository of manual and automated tests, documentation, data files, and more. Test Management tools should provide a level of configuration management for the many test artifacts in use. Additionally, these tools should allow for the classification of a test event by version so that all components used for a test can be traced back to each individual component version and state. This is extremely important as it's not uncommon to have to revert to an earlier release of testing should the current release find problems. Test Management tools often include a defect management module that allows manual and automated tests to create entries for defects. The defects can be assigned to owners and alerts can be configured to inform owners of the stages the defect is going through (e.g. identification, remediation, re-test, etc.) Challenges: Identifying a solution that meets all your needs (e.g. requirements traceability, test case storage, defect cataloging, version control, etc.) Getting buy-in from team to keep data current Best Practices: Plan out what features, functions, and customization will be necessary for your organization Make fields of important information required entry or selectable to aid in queries searching and reporting Produce targeted reporting and dashboards to meet stakeholder needs Identify areas of commonality to produce consolidated reporting (RTM). Combining so everybody sees the same source of truth. Allow ability to run canned reports as needed.","title":"4.1 Tools Target Areas"},{"location":"testing/4-1-tools/#41-tools-target-areas","text":"Test automation tools support the full software development lifecycle. There is likely a tool for every aspect of software that is developed. Tools are categorized by types of testing they can perform. Test types can span multiple test levels (e.g. component, integration, system, etc.). The following represents broad categories of functionality that test tools are capable of.","title":"4.1 Tools Target Areas"},{"location":"testing/4-1-tools/#411-functional-regression-testing","text":"By far the most common use of automation is to test application functionality. This can include new functionality added in a release or retesting of existing functionality. In both cases the goal is to use automation to test the functionality of an application or system. Applications use a variety of user interfaces. Most common today is the graphical user interface (GUI) that make up a large proportion of existing interfaces. All modern computers, tablets, and smartphones use a GUI (e.g. Windows, MacOS, Android, iOS, etc.). Historically, interfaces to systems were very crude and text based (e.g. Mainframe, Terminal, etc.). Some continue to be that way as the text based interface has very low overhead required by some applications or underlying hardware. Interface (UI or GUI) testing test tools must be able to recognize the interface graphical or otherwise, and the fields or objects on that interface. Where there is a standard implementation of the interface the test tool can easily recognize and interact with the fields or objects. For fully graphical interfaces there are a number of significant elements which can be tested or verified for each object or control. For example, a simple text field where a user enters a name can have many attributes: value (what data is in the field); focus (is this the field where the cursor is at); enabled (can we interact with this field); etc.","title":"4.1.1 Functional &amp; Regression Testing"},{"location":"testing/4-1-tools/#challenges","text":"Not all interfaces are implemented consistently and some objects within an application may not be recognized by automation Use of third-party controls embedded into an interface can cause challenges in automation Some automation may need to be custom-coded. This requires programming skill and knowledge that should be performed by individuals with this level of experience. Migrating from one tool to another can pose challenges Adapting to changing technology (browser, OS, etc.) requires tools that are compatible","title":"Challenges:"},{"location":"testing/4-1-tools/#best-practices","text":"Understand the full suite of technology that is being implemented for the SUT. Don't take a vendor or a reviewer as the last word on compatibility without the using the tool in its intended environment to ensure it is compatible. Look at tools that are compatible with the level of skill in the organization","title":"Best Practices:"},{"location":"testing/4-1-tools/#412-api-testing","text":"Application Programming Interface (API) testing is testing that does not involve a user interface (e.g., text or graphical interface). This is often referred to as client-less testing or non-GUI testing. It simply means that there is no direct user interface by which the tester interacts. However, data is still passed to the application, the application responds, and these interactions can be measured, verified, and reported on with automated test tools. Most applications that have a UI or GUI also are communicating at the API level. With API testing, rather than interacting with an object or control, we interact with a function or service. In order to do so we need to understand the structure of data that the function or service is expected to respond with. If we send the wrong data structure will it identify it as an error or do something unexpectedly? If we send data and receive no return data, what does that tell us? There needs to be a full understanding of what the function or service is supposed to do so that verification can be effective. This includes an understanding of knowing what the function or service is not supposed to do as well.","title":"4.1.2 API Testing"},{"location":"testing/4-1-tools/#challenges_1","text":"Working with functions and services can be a little like working in the dark because you don't have the familiar context of UI controls Tools that are compatible with the implemented services Tools that can test APIs across devices and operating systems APIs may degrade in performance with multiple simultaneous calls to them","title":"Challenges:"},{"location":"testing/4-1-tools/#best-practices_1","text":"Understand what you are testing and get the full specification before you start Make sure that you change only one variable at a time to understand the effect of changes Have some known baseline test interfaces to start with in order to realize the expected behavior Work closely with the API developer or vendor who implemented the function or service to fully understand the expected behavior and to report any unusual or missing return data values.","title":"Best Practices:"},{"location":"testing/4-1-tools/#413-performance-testing","text":"Performance testing is not concerned directly with testing functionality, but rather testing that functionality under load. Hence it is categorized as non-functional testing. Performance testing is difficult to impossible to do without test tools. In years past, rooms full of people and computers attempted to do performance testing but the cost was high, the consistency was low, and the breadth of testing left much to be desired. Modern performance testing tools simulate those same people on those same computers. Now, however, all the people and all the computers can be contained within one powerful computer. For extremely large simulations with heavy loads, performance testing can be distributed across multiple computers. Performance testing tools have 3 areas of functionality:","title":"4.1.3 Performance Testing"},{"location":"testing/4-1-tools/#main-controller","text":"The controller function starts tests, ramps them up or down, and stops tests. The controller function schedules the testing event and runs the operational profiles (often referred to as a scenarios) to simulate the conditions of a system in use.","title":"Main controller"},{"location":"testing/4-1-tools/#user-script","text":"The user script is the recorded or programmed sequence of events, or operational profile, that mimic the way in which a user interacts with a system. Running the same user script concurrently is how the test tool simulates multiple users performing the same function. User scripts often contain timing information (or wait statements as they are often called) that help regulate the speed of execution to something representative of a human user. Otherwise, scripts could run at speeds far faster than how real users interact with a system, which would not accurately represent the performance of the system.","title":"User Script"},{"location":"testing/4-1-tools/#reporting-and-analysis","text":"Performance test tools need to have robust reporting and analysis capabilities in order to accurately convey what has transpired. Reporting will include transactional timing information (e.g., how long it took for a search to return) and aggregate data (e.g., all searches on average took no more than 3 seconds to complete). Reporting can also include system resource utilization (e.g., caching, CPU, queues, etc.) and analysis can help correlate how system resources are impacted by heavy transactional use. Cross analysis reporting aides in comparing subsequent test runs in order to evaluate effects on changes.","title":"Reporting and Analysis"},{"location":"testing/4-1-tools/#challenges_2","text":"Finding an environment that mimics production is difficult and expensive Test tools can be very expensive Performance testing can affect other users sharing that environment Improperly created setup tests may yield no errors when actual errors exist Erroneous results can result from lack of proper correlation or using the wrong database","title":"Challenges:"},{"location":"testing/4-1-tools/#best-practices_2","text":"Work with stakeholders to accurately define the various operational profiles Do not make multiple changes between tests as it makes it difficult to know each impact Test each operational profile individually and under load first before adding other profiles Keep scripts and data updated to reflect current changes in SUT Look at the test logs for errors that may not be present in the reports","title":"Best Practices:"},{"location":"testing/4-1-tools/#414-security-testing","text":"Security testing tools help analyze and test applications for possible security vulnerabilities. There are 3 areas of interest for security testing: static, dynamic, and interactive [ref: Gartner Magic Quadrant for Application Security Testing]. Static analysis refers to looking at the application code from the inside, not when executed. Dynamic analysis is performing testing against a running application. There is overlap with the security qualities identified in static and dynamic security testing and interactive tests span both. Therefore it is advisable to begin security testing early in the software development lifecycle, as code is developed so that it can be assessed.","title":"4.1.4 Security Testing"},{"location":"testing/4-1-tools/#static-application-security-testing","text":"Static application security testing (SAST) of an application refers to analyzing the underlying application code for vulnerabilities. This may include source code, byte code, or binaries. Due to the visibility of the code these tests are often referred to as \"white box\" tests. Scanning of code can be accomplished before any code is compiled. Examining code can assist in uncovering vulnerabilities including SQL Injection, Buffer Overflows, Cross-Site Scripting, and Cross-Site Request Forgery.","title":"Static Application Security Testing"},{"location":"testing/4-1-tools/#dynamic-application-security-testing","text":"Dynamic Application Security Testing (DAST) provides applications that are running to be analyzed for possible security flaws. These may include traditional client server applications of web-based applications. DAST has no visibility to the underlying application and therefore is referred to as \"black box\" testing.","title":"Dynamic Application Security Testing"},{"location":"testing/4-1-tools/#interactive-application-security-testing","text":"Interactive Application Security Testing (IAST) allows visibility of possible vulnerabilities simultaneously within the application, often through the use of an agent within the runtime environment, and externally via application interfaces. This can include penetration testing which includes the scanning of ports for vulnerabilities. A set of complementary and overlapping vulnerabilities can be identified with SAST, DAST, and IAST tools at different phase of the software development lifecycle. However, automated tools cannot find all vulnerabilities so be careful to not get a false sense of security. A comprehensive guide for cyber security testing can be found at [ref: DoD Cybersecurity Test and Evaluation Guidebook, version 2.0] Blue team - penetration test. Red team - hacker to break into application. Red team in a box. Additional references for web application security available at: www.owasp.org.","title":"Interactive Application Security Testing"},{"location":"testing/4-1-tools/#challenges_3","text":"Getting developers to use SAST tools early on Tools may have difficulty parsing through code Tools will not find all vulnerabilities Understand the tool capabilities and how to properly set up for testing","title":"Challenges:"},{"location":"testing/4-1-tools/#best-practices_3","text":"Use tools regularly during development before code base becomes to large Security folks being proactive and helping program early Run tools on test and production environments Use a combination of best-in-class tools Identify and perform tests manually which tools may miss Decide what tests need to be run through automation. Automate when necessary as setup may take a lot of effort","title":"Best Practices:"},{"location":"testing/4-1-tools/#415-test-management","text":"Test Management tools provide for collaboration and reporting of test events. These tools can be a one-stop repository of manual and automated tests, documentation, data files, and more. Test Management tools should provide a level of configuration management for the many test artifacts in use. Additionally, these tools should allow for the classification of a test event by version so that all components used for a test can be traced back to each individual component version and state. This is extremely important as it's not uncommon to have to revert to an earlier release of testing should the current release find problems. Test Management tools often include a defect management module that allows manual and automated tests to create entries for defects. The defects can be assigned to owners and alerts can be configured to inform owners of the stages the defect is going through (e.g. identification, remediation, re-test, etc.)","title":"4.1.5 Test Management"},{"location":"testing/4-1-tools/#challenges_4","text":"Identifying a solution that meets all your needs (e.g. requirements traceability, test case storage, defect cataloging, version control, etc.) Getting buy-in from team to keep data current","title":"Challenges:"},{"location":"testing/4-1-tools/#best-practices_4","text":"Plan out what features, functions, and customization will be necessary for your organization Make fields of important information required entry or selectable to aid in queries searching and reporting Produce targeted reporting and dashboards to meet stakeholder needs Identify areas of commonality to produce consolidated reporting (RTM). Combining so everybody sees the same source of truth. Allow ability to run canned reports as needed.","title":"Best Practices:"},{"location":"testing/4-2-levels/","text":"4.2 Test Levels Government systems cover a broad range of testing levels. Some of these are contractor-focused test deliverables. Others are joint contractor-government, and yet others are government-only tests. Automation can be used at many test levels but provides greater value at lower levels. The following table summarizes the test levels, methods, role, and feasibility of automation: White Box White Box texting defines a testing approach that provides visibility to the program code as part of the test event. This allows the tester to understand the underlying structure of the software system or component under test. Testing at this level exposes design and implementation methods (and potential flaws) used to construct the software. The goal to understand and validate the individual lines of code, to the extent possible. Software construction with excessive levels of branching or nesting often cannot be easily tested. Black Box Black Box testing defines an approach to testing that focuses on the functionality of the software under test. The tester exercises the software much as a user would interact with the software and thus attempts to validate if the expected features and functionality have been properly implemented, based on a specification or user story. This technique complements the White Box approach as the system is tested from two different perspectives. Gray Box Gray Box testing combines elements of White and Black Box testing in order to simultaneously identify any defects in application usage that are due to poorly implemented coding constructs. Feasibility of Automation Software Tests Automation is generally easier to implement earlier in smaller, contained, software components. Developers can make good use of automation to test incremental code updates, functions, and components. There are many tools for developers that not only automate test execution but that also provide assessments of the code quality and complexity. These should be run first while modules are still manageable rather than when software has been integrated and combined with other systems, where analysis becomes much more difficult on a significantly larger code base. System Tests Automated system tests verify and validate system behavior. Although there may be some reuse from automation at the earlier software test level, system tests by and large are Black Box tests where the activity is focused on functional requirements that the software must meet. Often these will focus more on testing the system through interfaces (graphical user interface, API, etc.). Mission Tests As the system grows and becomes integrated with other systems, automation can still play a part. For example in a System-of-Systems test automation can be used to drive the individual subsystems comprising the System-of-Systems solution. This automation solution will help coordinate or synchronize the execution of various systems under test. However, this is a broad vs deep application of test automation, as it pertains to functionality. Finally, as systems are migrated to a production environment we lose the ability to test and automate.","title":"4.2 Test Levels"},{"location":"testing/4-2-levels/#42-test-levels","text":"Government systems cover a broad range of testing levels. Some of these are contractor-focused test deliverables. Others are joint contractor-government, and yet others are government-only tests. Automation can be used at many test levels but provides greater value at lower levels. The following table summarizes the test levels, methods, role, and feasibility of automation:","title":"4.2 Test Levels"},{"location":"testing/4-2-levels/#white-box","text":"White Box texting defines a testing approach that provides visibility to the program code as part of the test event. This allows the tester to understand the underlying structure of the software system or component under test. Testing at this level exposes design and implementation methods (and potential flaws) used to construct the software. The goal to understand and validate the individual lines of code, to the extent possible. Software construction with excessive levels of branching or nesting often cannot be easily tested.","title":"White Box"},{"location":"testing/4-2-levels/#black-box","text":"Black Box testing defines an approach to testing that focuses on the functionality of the software under test. The tester exercises the software much as a user would interact with the software and thus attempts to validate if the expected features and functionality have been properly implemented, based on a specification or user story. This technique complements the White Box approach as the system is tested from two different perspectives.","title":"Black Box"},{"location":"testing/4-2-levels/#gray-box","text":"Gray Box testing combines elements of White and Black Box testing in order to simultaneously identify any defects in application usage that are due to poorly implemented coding constructs.","title":"Gray Box"},{"location":"testing/4-2-levels/#feasibility-of-automation","text":"","title":"Feasibility of Automation"},{"location":"testing/4-2-levels/#software-tests","text":"Automation is generally easier to implement earlier in smaller, contained, software components. Developers can make good use of automation to test incremental code updates, functions, and components. There are many tools for developers that not only automate test execution but that also provide assessments of the code quality and complexity. These should be run first while modules are still manageable rather than when software has been integrated and combined with other systems, where analysis becomes much more difficult on a significantly larger code base.","title":"Software Tests"},{"location":"testing/4-2-levels/#system-tests","text":"Automated system tests verify and validate system behavior. Although there may be some reuse from automation at the earlier software test level, system tests by and large are Black Box tests where the activity is focused on functional requirements that the software must meet. Often these will focus more on testing the system through interfaces (graphical user interface, API, etc.).","title":"System Tests"},{"location":"testing/4-2-levels/#mission-tests","text":"As the system grows and becomes integrated with other systems, automation can still play a part. For example in a System-of-Systems test automation can be used to drive the individual subsystems comprising the System-of-Systems solution. This automation solution will help coordinate or synchronize the execution of various systems under test. However, this is a broad vs deep application of test automation, as it pertains to functionality. Finally, as systems are migrated to a production environment we lose the ability to test and automate.","title":"Mission Tests"},{"location":"testing/4-3-types/","text":"4.3 Test Types Functional Tests Functional tests are those tests that evaluate if a system is performing according to a specification or requirement, often detailed in a use case or user story. Functional tests look at the system behavior as a guide to what the system should be do. Non-Functional Tests Non-Functional Tests evaluate the characteristics of a system and include performance testing, security testing, and other testing that does not directly test system functionality. However, as in the case of performance testing, we do exercise a functional test under load to examine its characteristics for the purposes of acquiring timing/performance information. Structural/architectural Structural/architectural testing is a white box testing activity which is concerned with coverage of code within modules and functions. The structure is tested to meet all possible conditions that the code provides for. Any code not exercised from the tests would require specific test conditions to be created, thus improving the overall coverage of code within the module or function. Retesting/regression When changes are made to a system or component, testing should be done in order to ensure the system or component continues to operate as expected. This can include testing done after a defect has been fixed to ensure that the fix was applied correctly, or testing done as a result new features added to the system and where existing features need to continue to operate as expected. Each these test types can be executed across test levels, where applicable. Also, there may be opportunities to reuse tests if designed as part of the automation strategy. For example, tests developed at the system test level may be re-used at the mission test level.","title":"4.3 Test Types"},{"location":"testing/4-3-types/#43-test-types","text":"","title":"4.3 Test Types"},{"location":"testing/4-3-types/#functional-tests","text":"Functional tests are those tests that evaluate if a system is performing according to a specification or requirement, often detailed in a use case or user story. Functional tests look at the system behavior as a guide to what the system should be do.","title":"Functional Tests"},{"location":"testing/4-3-types/#non-functional-tests","text":"Non-Functional Tests evaluate the characteristics of a system and include performance testing, security testing, and other testing that does not directly test system functionality. However, as in the case of performance testing, we do exercise a functional test under load to examine its characteristics for the purposes of acquiring timing/performance information.","title":"Non-Functional Tests"},{"location":"testing/4-3-types/#structuralarchitectural","text":"Structural/architectural testing is a white box testing activity which is concerned with coverage of code within modules and functions. The structure is tested to meet all possible conditions that the code provides for. Any code not exercised from the tests would require specific test conditions to be created, thus improving the overall coverage of code within the module or function.","title":"Structural/architectural"},{"location":"testing/4-3-types/#retestingregression","text":"When changes are made to a system or component, testing should be done in order to ensure the system or component continues to operate as expected. This can include testing done after a defect has been fixed to ensure that the fix was applied correctly, or testing done as a result new features added to the system and where existing features need to continue to operate as expected. Each these test types can be executed across test levels, where applicable. Also, there may be opportunities to reuse tests if designed as part of the automation strategy. For example, tests developed at the system test level may be re-used at the mission test level.","title":"Retesting/regression"},{"location":"testing/5-1-approach/","text":"5.1 Approaches to Test Automation Many approaches are available for how automated tools are used. Some involve using the tool functionality as-is with little to no modification or customization. This is what is usually done by test groups with less technically skilled testers. Where more technically skilled testers exist, more customizable solutions are created. While these ultimately offer greater flexibility, they also require greater up-front investment and continued skilled resources to maintain and expand the automated solution. The following represent common approaches to automation. Capture/replay Capture/replay or record/replay is a technique which directly uses the test automation tool's built-in feature to allow a user session to be \"captured\" as the tester interacts with the SUT. Often this is what a vendor might demonstrate as capability with a controlled sample application. After the user keystrokes are captured, they are faithfully replayed by the test tool. While this makes for a very effective demonstration of ultimate simplicity, the application of capture/replay in real-world complex applications often doesn't fair as well. In most cases, using capture/replay in moderate to complex system will likely not yield a successful playback of the initially recorded script. This is due to the fact that most tools are not sophisticated enough to understand all the nuances and the multiple parameters and values that are changing dynamically as a user navigates across an application. For tests that are recorded, the maintenance becomes quite difficult to achieve once the number of tests is in the double digits or higher. Each captured script is unique to the others and needs to be individually maintained. This becomes very time consuming and inefficient. Data driven As the name implies, the data driven test technique aims to use data, external to the tool, to drive the test execution. The remaining techniques below are all variations or adaptations of driving tests with data. In its simplest form, data driven tests replace the specific (static) values that were used for one iteration of a test with a set of data that can be used for many iterations of the same test. With the proper application of specification-based test techniques test datasets can be constructed that minimize the number of tests require for the maximum coverage. Keyword driven The keyword-driven approach at its core also uses data to drive tests but the keyword descriptor is a meaningful action for the system to perform. For example, to test a system with parts that need to be placed in inventory, taken out of inventory, or repaired, a keyword driven script may use the keywords \"check in\", \"check out\", and \"repair\" for any given part number. The way a keyword driven script works is that test cases are developed using keywords and the corresponding values for each keyword. Then, the test automation solution parses those keywords and values and lets pre-defined scripts navigate in a pre-determined manner, through the application in order to execute the desired functionality. Process driven The process driven approach also uses data to drive tests, but the data it defines is more detailed than that of the keyword approach. This approach requires the definition of the following in order to create a test script: control, instruction, data. The \"control\" identifies the application window/object which needs to be interrogated. The \"instruction\" may include: input, verification, navigation, timing, etc. The \"data\" would be the relevant data for the given instruction in the context of the window/object. The process driven approach therefore is the most abstracted model as its very design is built around externalizing all data. The benefit this approach brings is highly reusable function libraries. Model driven A model driven approach for testing is derived from a model based design methodology. With modeling, the desired behavior of a SUT can be represented. From this behavior design test cases can be derived and then specific data can be created to make these tests executable. Model based techniques can facilitate creating tests which can then be used under automated control. The following table compares popular approaches to automation, where LOW/MED/HIGH indicates the degree in achieving that characteristic:","title":"5.1 Approaches to Test Automation"},{"location":"testing/5-1-approach/#51-approaches-to-test-automation","text":"Many approaches are available for how automated tools are used. Some involve using the tool functionality as-is with little to no modification or customization. This is what is usually done by test groups with less technically skilled testers. Where more technically skilled testers exist, more customizable solutions are created. While these ultimately offer greater flexibility, they also require greater up-front investment and continued skilled resources to maintain and expand the automated solution. The following represent common approaches to automation.","title":"5.1 Approaches to Test Automation"},{"location":"testing/5-1-approach/#capturereplay","text":"Capture/replay or record/replay is a technique which directly uses the test automation tool's built-in feature to allow a user session to be \"captured\" as the tester interacts with the SUT. Often this is what a vendor might demonstrate as capability with a controlled sample application. After the user keystrokes are captured, they are faithfully replayed by the test tool. While this makes for a very effective demonstration of ultimate simplicity, the application of capture/replay in real-world complex applications often doesn't fair as well. In most cases, using capture/replay in moderate to complex system will likely not yield a successful playback of the initially recorded script. This is due to the fact that most tools are not sophisticated enough to understand all the nuances and the multiple parameters and values that are changing dynamically as a user navigates across an application. For tests that are recorded, the maintenance becomes quite difficult to achieve once the number of tests is in the double digits or higher. Each captured script is unique to the others and needs to be individually maintained. This becomes very time consuming and inefficient.","title":"Capture/replay"},{"location":"testing/5-1-approach/#data-driven","text":"As the name implies, the data driven test technique aims to use data, external to the tool, to drive the test execution. The remaining techniques below are all variations or adaptations of driving tests with data. In its simplest form, data driven tests replace the specific (static) values that were used for one iteration of a test with a set of data that can be used for many iterations of the same test. With the proper application of specification-based test techniques test datasets can be constructed that minimize the number of tests require for the maximum coverage.","title":"Data driven"},{"location":"testing/5-1-approach/#keyword-driven","text":"The keyword-driven approach at its core also uses data to drive tests but the keyword descriptor is a meaningful action for the system to perform. For example, to test a system with parts that need to be placed in inventory, taken out of inventory, or repaired, a keyword driven script may use the keywords \"check in\", \"check out\", and \"repair\" for any given part number. The way a keyword driven script works is that test cases are developed using keywords and the corresponding values for each keyword. Then, the test automation solution parses those keywords and values and lets pre-defined scripts navigate in a pre-determined manner, through the application in order to execute the desired functionality.","title":"Keyword driven"},{"location":"testing/5-1-approach/#process-driven","text":"The process driven approach also uses data to drive tests, but the data it defines is more detailed than that of the keyword approach. This approach requires the definition of the following in order to create a test script: control, instruction, data. The \"control\" identifies the application window/object which needs to be interrogated. The \"instruction\" may include: input, verification, navigation, timing, etc. The \"data\" would be the relevant data for the given instruction in the context of the window/object. The process driven approach therefore is the most abstracted model as its very design is built around externalizing all data. The benefit this approach brings is highly reusable function libraries.","title":"Process driven"},{"location":"testing/5-1-approach/#model-driven","text":"A model driven approach for testing is derived from a model based design methodology. With modeling, the desired behavior of a SUT can be represented. From this behavior design test cases can be derived and then specific data can be created to make these tests executable. Model based techniques can facilitate creating tests which can then be used under automated control. The following table compares popular approaches to automation, where LOW/MED/HIGH indicates the degree in achieving that characteristic:","title":"Model driven"},{"location":"testing/5-2-architecture/","text":"5.2 Overall Architectural Considerations The International Software Testing Qualifications Board (ISTQB) has published a body of knowledge (BoK) and glossary that comprise the syllabus for the Advanced Level Test Automation Engineer certification. This BoK contains a general test automation architecture (GTAA) from which a purpose-built test automation architecture (TAA) can be derived. The Generic Test Automation Architecture GTAA (Source: ISTQB Advanced Level Test Automation Engineer Syllabus) The defined layers of the GTAA include: test generation, test definition, test execution, and test adaptation. This generic architecture can be thought of as a catalog of functionality from which we select those capabilities required for a given project or program. Test Generation Layer Test cases, whether for manual or automated testing must be developed. The development process might include a test case generator for systems that are fully integrated with requirements and design. Some advanced development environments will included system modeling from which high-level test cases can be generated. Use of advanced model based testing techniques and tools can provide an efficient method of preparing an initial set of test cases for automation. Test Definition Layer The Test Definition Layer provides support to help define high- and low-level test cases and test procedures. Data needed for test procedures would be generated at this layer. This can include the data needed for keywords used access libraries when a keyword-driven approach is used. Test libraries can include purpose built test sequences and reusable components for our automation solution. Test Execution Layer The execution layer represents that portion of our automated solution that \"runs\" the tests. We often think of the various test tools available as test execution tools. At their core is the capability to execute a test. This applies equally to COTS, GOTS, and OSS test tool solutions, as discussed in Section 6.3.1. Within the execution layer we have two additional components: test reporting and test logging. The test reporting function provides us with information regarding the SUT so that we can ascertain if the SUT if performing as our tests expect it to. This will alert us to changes in the SUT, although it will not necessarily identify the root cause, which may be SUT, data, or environment related. The test logs will help us identify where some of the errors may be originating from, when providing information at a very granular level. The logs can also provide us with information regarding the operation of our Test Automation Framework components and libraries. Errors in any of these should be written out to the log files so that corrective action can be taken. Test Adaptation Layer Test tool solutions must be compatible to work with the intended environment. This can include a graphical user interface (e.g., browser, mobile app, desktop app, etc.) or a messaging interface. Most systems that we are testing will often contain several interfaces which need to be exercised for thoroughness in testing. By analyzing the SUT architecture we gain an understanding of what interfaces exist, and what are requirements are for testing them. Test tools often provide support for multiple interfaces. However, additional interface testing requirements can be met through other test tools or by developing such capability. Test Automation & Framework The 4 layers discussed above all encompass a test automation solution. This ranges from the test script definitions through the automated execution via a given interface. We can think of the Test Automation Framework as those purpose built testware components that allow us to define data inputs, execute tests, and produce reports. The idea behind a framework is that it should be built with reusable components and not contain embedded application data. This will allow for greatest reuse, and ease of maintenance. Configuration Management There are many artifacts that comprise our test automation solution. In order to keep them manageable, we need to understand what version of what component we are using. This applies equally to test data, test automation functions and libraries, test reporting, and test environments (which may include OS patches, security updates, and updated DLLs). By having this understanding and control we are able to revert to a known working environment quickly. Test Management Test management is there to support the development and maintenance of the test automation solution. This includes providing the proper funding, staffing, tools, and environments in order to be successful. Project Management Test automation is a project, often requiring significant development tasks. As such it needs to be planned, and executed with tasks, milestones, people, and resources. Different development lifecycle methodologies will affect the approach and timing of developing an automated solution, as discussed in Chapter 7. 5.2.1 Understanding the system architecture Enterprise systems are built with a multitude of servers and other specialized components with which they interface. A first step in understanding what needs to be tested and what can potentially be automated is to understand the overall system architecture, or enterprise system architecture (ESA). Most systems receive an input (e.g., a user request for data) and route that request through a multitude of servers, that may include application servers, web servers, database servers, and report servers. A good test strategy is to isolate the various requests as they go from one server to another and identify a way to test each individual component in addition to creating an end-to-end test. Automated tools exist that support multiple protocols and that can act as stubs to simulate behavior between systems or components. 5.2.2 Defining interface components Software and systems communicate with one another often via a protocol across an interface. Understanding what interfaces the SUT communicates with will help determine the testing requirement for that interface. Once the interface has been identified, the communications protocol needs to be established in order to simulate data over that interface. Common interfaces and protocols include: Graphical User Interface Text User Interface Applications Programming Interface (API) Services (including web services) Database (ODBC, etc.) A test automation engineer will need to gather the specification for the interface in order that testing can simulate, or emulate the given interface. 5.2.3 Creating a purpose-built architecture The GTAA, as described above, allows for flexibility in the creation of a purpose built Test Automation Architecture (TAA). By understanding the various components that make up our SUT, we can devise a TAA that fits our needs. For example, if we use model based testing, we may already have a solution for the test generation layer in creating our high-level test cases. Otherwise, we need to define the manual process by which these are created. Once we start building our test cases and corresponding data we need to define a repository where these will be stored. This repository should be under configuration control in order to support the versioning of data by release. For test execution, we may already have a robust test tool with pre-made logging and reporting functions. Otherwise, we'll need to develop these to cover those specific needs. A broad overview of interfaces, as described in section 6.2.2 will guide us as to what tools we'll need to support interface testing.","title":"5.2 Overall Architectural Considerations"},{"location":"testing/5-2-architecture/#52-overall-architectural-considerations","text":"The International Software Testing Qualifications Board (ISTQB) has published a body of knowledge (BoK) and glossary that comprise the syllabus for the Advanced Level Test Automation Engineer certification. This BoK contains a general test automation architecture (GTAA) from which a purpose-built test automation architecture (TAA) can be derived.","title":"5.2 Overall Architectural Considerations"},{"location":"testing/5-2-architecture/#the-generic-test-automation-architecture-gtaa","text":"(Source: ISTQB Advanced Level Test Automation Engineer Syllabus) The defined layers of the GTAA include: test generation, test definition, test execution, and test adaptation. This generic architecture can be thought of as a catalog of functionality from which we select those capabilities required for a given project or program.","title":"The Generic Test Automation Architecture GTAA"},{"location":"testing/5-2-architecture/#test-generation-layer","text":"Test cases, whether for manual or automated testing must be developed. The development process might include a test case generator for systems that are fully integrated with requirements and design. Some advanced development environments will included system modeling from which high-level test cases can be generated. Use of advanced model based testing techniques and tools can provide an efficient method of preparing an initial set of test cases for automation.","title":"Test Generation Layer"},{"location":"testing/5-2-architecture/#test-definition-layer","text":"The Test Definition Layer provides support to help define high- and low-level test cases and test procedures. Data needed for test procedures would be generated at this layer. This can include the data needed for keywords used access libraries when a keyword-driven approach is used. Test libraries can include purpose built test sequences and reusable components for our automation solution.","title":"Test Definition Layer"},{"location":"testing/5-2-architecture/#test-execution-layer","text":"The execution layer represents that portion of our automated solution that \"runs\" the tests. We often think of the various test tools available as test execution tools. At their core is the capability to execute a test. This applies equally to COTS, GOTS, and OSS test tool solutions, as discussed in Section 6.3.1. Within the execution layer we have two additional components: test reporting and test logging. The test reporting function provides us with information regarding the SUT so that we can ascertain if the SUT if performing as our tests expect it to. This will alert us to changes in the SUT, although it will not necessarily identify the root cause, which may be SUT, data, or environment related. The test logs will help us identify where some of the errors may be originating from, when providing information at a very granular level. The logs can also provide us with information regarding the operation of our Test Automation Framework components and libraries. Errors in any of these should be written out to the log files so that corrective action can be taken.","title":"Test Execution Layer"},{"location":"testing/5-2-architecture/#test-adaptation-layer","text":"Test tool solutions must be compatible to work with the intended environment. This can include a graphical user interface (e.g., browser, mobile app, desktop app, etc.) or a messaging interface. Most systems that we are testing will often contain several interfaces which need to be exercised for thoroughness in testing. By analyzing the SUT architecture we gain an understanding of what interfaces exist, and what are requirements are for testing them. Test tools often provide support for multiple interfaces. However, additional interface testing requirements can be met through other test tools or by developing such capability.","title":"Test Adaptation Layer"},{"location":"testing/5-2-architecture/#test-automation-framework","text":"The 4 layers discussed above all encompass a test automation solution. This ranges from the test script definitions through the automated execution via a given interface. We can think of the Test Automation Framework as those purpose built testware components that allow us to define data inputs, execute tests, and produce reports. The idea behind a framework is that it should be built with reusable components and not contain embedded application data. This will allow for greatest reuse, and ease of maintenance.","title":"Test Automation &amp; Framework"},{"location":"testing/5-2-architecture/#configuration-management","text":"There are many artifacts that comprise our test automation solution. In order to keep them manageable, we need to understand what version of what component we are using. This applies equally to test data, test automation functions and libraries, test reporting, and test environments (which may include OS patches, security updates, and updated DLLs). By having this understanding and control we are able to revert to a known working environment quickly.","title":"Configuration Management"},{"location":"testing/5-2-architecture/#test-management","text":"Test management is there to support the development and maintenance of the test automation solution. This includes providing the proper funding, staffing, tools, and environments in order to be successful.","title":"Test Management"},{"location":"testing/5-2-architecture/#project-management","text":"Test automation is a project, often requiring significant development tasks. As such it needs to be planned, and executed with tasks, milestones, people, and resources. Different development lifecycle methodologies will affect the approach and timing of developing an automated solution, as discussed in Chapter 7.","title":"Project Management"},{"location":"testing/5-2-architecture/#521-understanding-the-system-architecture","text":"Enterprise systems are built with a multitude of servers and other specialized components with which they interface. A first step in understanding what needs to be tested and what can potentially be automated is to understand the overall system architecture, or enterprise system architecture (ESA). Most systems receive an input (e.g., a user request for data) and route that request through a multitude of servers, that may include application servers, web servers, database servers, and report servers. A good test strategy is to isolate the various requests as they go from one server to another and identify a way to test each individual component in addition to creating an end-to-end test. Automated tools exist that support multiple protocols and that can act as stubs to simulate behavior between systems or components.","title":"5.2.1 Understanding the system architecture"},{"location":"testing/5-2-architecture/#522-defining-interface-components","text":"Software and systems communicate with one another often via a protocol across an interface. Understanding what interfaces the SUT communicates with will help determine the testing requirement for that interface. Once the interface has been identified, the communications protocol needs to be established in order to simulate data over that interface. Common interfaces and protocols include: Graphical User Interface Text User Interface Applications Programming Interface (API) Services (including web services) Database (ODBC, etc.) A test automation engineer will need to gather the specification for the interface in order that testing can simulate, or emulate the given interface.","title":"5.2.2 Defining interface components"},{"location":"testing/5-2-architecture/#523-creating-a-purpose-built-architecture","text":"The GTAA, as described above, allows for flexibility in the creation of a purpose built Test Automation Architecture (TAA). By understanding the various components that make up our SUT, we can devise a TAA that fits our needs. For example, if we use model based testing, we may already have a solution for the test generation layer in creating our high-level test cases. Otherwise, we need to define the manual process by which these are created. Once we start building our test cases and corresponding data we need to define a repository where these will be stored. This repository should be under configuration control in order to support the versioning of data by release. For test execution, we may already have a robust test tool with pre-made logging and reporting functions. Otherwise, we'll need to develop these to cover those specific needs. A broad overview of interfaces, as described in section 6.2.2 will guide us as to what tools we'll need to support interface testing.","title":"5.2.3 Creating a purpose-built architecture"},{"location":"testing/5-3-evaluation/","text":"5.3 Evaluation and Selection of Test Tools 5.3.1 Tool choices Testing tools are available from a number of sources. These include: commercial vendors, open source, and custom built. Each source brings with it possible benefits and limitations. For example, a commercial test tool may be rich in features and offer good support, but require a significant initial and ongoing financial investment for license, maintenance, and support. An open source solution may provide the functionality needed at a given moment, but may lack the ability to expand to meet future needs. Custom solutions, which are either contracted out or developed by the government, will be purpose-built and target key areas needing automation. However, the burden of maintaining, correcting, and expanding such a solution will fall to the government to direct either through internal resources or through additional contractor funding. Appendix B: Test Tools provides a listing of tools frequently found to perform well. Each project will have its own needs and will need to evaluate the fit to a given requirement. Open Source Software (OSS) The OSS category of software products has many options available for the various functional and non-functional requirements for test automation (as described in Chapter 4 Scope of test automation). OSS test tools are updated by a community of contributors, and keeping abreast of the changes ensures that the tool is functioning optimally. Many OSS test tools are have been developed to support a specific environment. For example, there are tools to support browser based testing and tools for mobile application testing. A few tools may provide support for multiple environments, but that is more commonly seen in commercial test tools. (see Defense Acquisition Policy, DoD Instruction 5000.75) OSS test tools have expanded rapidly over the years and can meet many enterprise testing needs. The U.S. Government supports the use of OSS solutions to meet information technology needs, as addressed in the Federal Source Code Policy (https://sourcecode.cio.gov/). Commercial off the Shelf (COTS) Commercial software testing tools developed and supported by vendors have been around for many years. These tools tend to be fully featured and often support multiple environments (e.g., browsers, terminals, API, mobile, etc.). Ideally, there would be one tool that offers the tester capabilities to test all software. In reality, even those tools that cover a wide range of environments may support some better than others. So when evaluating a tool across multiple environments, a separate evaluation will need to be made to see how well each environment is supported. By using a paid license and maintenance model, COTS tools provide for research and development to fund a steady stream of improvements, feature additions, and defect patching. A well-funded organization has the resources to do this and keep the tools current. COTS vendors often have multiple test tools in their catalog to support various testing activity (e.g. functional testing, load testing, test management). These tools are often integrated with one another, such as keeping all test artifacts under configuration version control through a test management module. Government of-the-shelf (GOTS) The GOTS solution is one that has been engineered to meet a specific need. Most often this level of effort is only undertaken when an exhaustive evaluation of existing COTS and OSS solutions shows no matches. The GOTS solution Government directed and can be developed internally with technical staff or via subcontract to an external entity. These solutions vary in scope and features and it is often difficult to know what may be available as no centralized catalog exists that lists them. See Appendix B: Test Tools for commonly used test tools by government and industry. 5.3.2 Test tool evaluation In order to properly evaluate test automation tools we need to make sure we identify the correct quality criteria that will help identify the proper fit for the project or program. Environment Each environment presents unique opportunities and challenges for the implementation of test automation tools. Environments may include multiple platforms to support workstations, laptops, and mobile tables and phones. Each of these platforms often will run a different operating system and the software implementation, whether native or through a browser (which are also native to the device), will dictate the functionality to that platform. When evaluating test automation tools, we need to be aware of our complete needs for the environment, including all platforms and devices that ultimately might be required. Be aware that tools perform differently across devices so a prioritization of which device testing should be automated first will help ensure that those most critical platforms can be implemented successfully. Test Types Types and test levels are discussed in sections 4.2 and 4.3 and need to be considering when evaluating test automation tools. This includes defining the types of tests that are candidates for automation across test levels. Tools are purpose built, and each combination of test type/test level may require re-evaluation of a given test tool selected for a prior test type/test level pairing. Technology Test tools exist to support a variety of older and current technologies (e.g. terminals, text UIs, graphical UIs, browsers, etc.) Some tools support many technologies within one product suite. These are most often COTS tools (see section 6.3.1) where a vendor's tools evolve technology to meet continuing technology needs. However, a bundled solution suite may not necessarily contain \"best-in-class\" for each technology that is supported. An a-la-carte approach to find a specific tool for each technology stack that needs testing may ultimately provide a more satisfactory solution, even where integration amongst disparate tools may be required. Language Most tools provide customization through programmability. In this way they are similar to Interactive Development Environments (IDEs) used by developers to design and build applications. With automation we are building an application too, one that when run automates the execution of testing. Common languages used by test tools include Java, Python, C#, C++, VBScript, etc., with some tools supporting multiple languages. Languages offer a range of features and flexibility and a range of effort to learn them. Identifying tools with the right balance of features and flexibility as they pertain to the knowledge and skills in the organization are key to using a specified tool effectively. Reporting When we look at the reporting features of tools we want evaluate both test reporting and logging functionality. The logging function which is often undervalued or not evaluated and is important in determining if errors occurred specific to the automation of tests. The logging function is an audit trail that can be examined to know exactly what occurred doing execution, irrespective of any application criteria being examined. Often the logging function can be customizable to provide specific information that may be required for analysis. For reporting, features should include meaningful displays of data and analysis from which decisions can be made. These may require additional test runs in order to show varying parameters or cumulative data. Most tools have a basic level of on-demand reporting but the better tools will allow for the customization of reporting. Dashboards also provide reporting, most often in a graphical or table based approach where application health is readily displayed. Where other reporting systems exist, exporting data from the tool may be necessary in order to integrate the results with project-level metrics required by project management for planning and reporting purposes. Ease of Use Test tools should be intuitive and easy to understand and use. They should be well documented so that common answers to questions and techniques can be quickly identified. For example, one may need to know what function is used to verify the content on a calendar widget or how to export a test report in csv format. Although some test tools may claim that there is no need to use scripting for test development, most medium to complex test requirements will likely require a level of scripting that makes the test perform to meet standards and requirements. Therefore, \"ease of use\" is a relative term. For those automators with strong programming skills it will be easy to develop scripts with a test tool. However, for the traditional manual tester, the requirement to write program code may be difficult and counterproductive to their domain-specific skills. Sourcing As defined in section 6.3.1 tools are available form a variety of sources. These can include COTS, GOTS, and OSS. Each category may have very good solutions. The solution needs to include the sourcing of the tool, the training required for the tool, the ongoing maintenance requirements to the tool, and the support available for the tool (in the form of technical support, product updates, etc.) Support Support for test tools can be available from many sources. Vendors often have dedicated support sites that allow logging of issues. These are normally restricted to customers paying for support, either directly or indirectly via maintenance contracts. Additionally forums exist on the internet that focus specifically on particular testing tools. These forums may not be a reliable place for answers as contributions are made by volunteers who may or may not have the knowledge to answer correctly. Investment Test tools from commercial entities have a variety of licensing fees structures. These can include individual, per seat, and floating, among others. Understanding the intended usage by your team will help define which license structure that is best for you. Additionally, commercial entities have support and maintenance fees. Support allows for technical questions or issues to be submitted and responded to, while maintenance usually provides for minor and major updates to the testing tool. Working with the most recent version of a tool will often help resolve technical issues (e.g., compatibility, defects, etc.). Training should also be included as a cost, both for vendor and open source solutions. EVALUATION TABLES Evaluation of the above criteria can be reduced to a table indicating the relative importance for each category, with a total weight at 100%. As tools are evaluated, completing the table helps to compare one tool against another tool objectively. (sample values for illustration) Categories should be rated for how closely they meet the intended requirements. A score allows the category to show a strength or weakness in meeting the category requirement. Use of a scoring system ranging from 0 to 5 where each value indicates alignment to the requirement score. Finally, no tool evaluation is complete without trying the tool out in the intended environment where there is a need to automate. This will ensure that the tool really does work as intended. 5.3.3 Proof-of-concept A proof of concept (PoC) is the first step in understanding if a given test tool meets the anticipated needs of the project. With a PoC we want to ensure that the technology that we've provisionally selected actually works in the intended environment. This is key as any prior demonstration of the tool, however capable, needs to now show the same capability in the intended environment. 5.3.4 Test tool prototypes A prototype takes the PoC further along by automating a narrowly defined set of functionality from a system where automation is to take hold. The prototype helps the project team understand the complexities of implementation in a timeframe that will not drain excessive resources or funding. The prototype shows the way ahead and serves to realign expectations of how long it actually takes to get test built for automation. The lessons learned from the prototype will bring greater efficiency to the next phase of automation. 5.3.5 Tool training and support Success in automation requires an overall understanding of testing, technology, and the features and functions of specific testing tools in meeting the stated requirements. Appendix B: Test Tools provides a listing of tools that have been shown to be successful across projects and programs. Appendix A: Resources provides additional resources that cover education and certification around testing best practices and test tools. Combining education and certification with hands-on test tool knowledge empowers the test team to develop purpose-built solutions that align with project and program needs.","title":"5.3 Evaluation and Selection of Test Tools"},{"location":"testing/5-3-evaluation/#53-evaluation-and-selection-of-test-tools","text":"","title":"5.3 Evaluation and Selection of Test Tools"},{"location":"testing/5-3-evaluation/#531-tool-choices","text":"Testing tools are available from a number of sources. These include: commercial vendors, open source, and custom built. Each source brings with it possible benefits and limitations. For example, a commercial test tool may be rich in features and offer good support, but require a significant initial and ongoing financial investment for license, maintenance, and support. An open source solution may provide the functionality needed at a given moment, but may lack the ability to expand to meet future needs. Custom solutions, which are either contracted out or developed by the government, will be purpose-built and target key areas needing automation. However, the burden of maintaining, correcting, and expanding such a solution will fall to the government to direct either through internal resources or through additional contractor funding. Appendix B: Test Tools provides a listing of tools frequently found to perform well. Each project will have its own needs and will need to evaluate the fit to a given requirement.","title":"5.3.1 Tool choices"},{"location":"testing/5-3-evaluation/#open-source-software-oss","text":"The OSS category of software products has many options available for the various functional and non-functional requirements for test automation (as described in Chapter 4 Scope of test automation). OSS test tools are updated by a community of contributors, and keeping abreast of the changes ensures that the tool is functioning optimally. Many OSS test tools are have been developed to support a specific environment. For example, there are tools to support browser based testing and tools for mobile application testing. A few tools may provide support for multiple environments, but that is more commonly seen in commercial test tools. (see Defense Acquisition Policy, DoD Instruction 5000.75) OSS test tools have expanded rapidly over the years and can meet many enterprise testing needs. The U.S. Government supports the use of OSS solutions to meet information technology needs, as addressed in the Federal Source Code Policy (https://sourcecode.cio.gov/).","title":"Open Source Software (OSS)"},{"location":"testing/5-3-evaluation/#commercial-off-the-shelf-cots","text":"Commercial software testing tools developed and supported by vendors have been around for many years. These tools tend to be fully featured and often support multiple environments (e.g., browsers, terminals, API, mobile, etc.). Ideally, there would be one tool that offers the tester capabilities to test all software. In reality, even those tools that cover a wide range of environments may support some better than others. So when evaluating a tool across multiple environments, a separate evaluation will need to be made to see how well each environment is supported. By using a paid license and maintenance model, COTS tools provide for research and development to fund a steady stream of improvements, feature additions, and defect patching. A well-funded organization has the resources to do this and keep the tools current. COTS vendors often have multiple test tools in their catalog to support various testing activity (e.g. functional testing, load testing, test management). These tools are often integrated with one another, such as keeping all test artifacts under configuration version control through a test management module.","title":"Commercial off the Shelf (COTS)"},{"location":"testing/5-3-evaluation/#government-of-the-shelf-gots","text":"The GOTS solution is one that has been engineered to meet a specific need. Most often this level of effort is only undertaken when an exhaustive evaluation of existing COTS and OSS solutions shows no matches. The GOTS solution Government directed and can be developed internally with technical staff or via subcontract to an external entity. These solutions vary in scope and features and it is often difficult to know what may be available as no centralized catalog exists that lists them. See Appendix B: Test Tools for commonly used test tools by government and industry.","title":"Government of-the-shelf (GOTS)"},{"location":"testing/5-3-evaluation/#532-test-tool-evaluation","text":"In order to properly evaluate test automation tools we need to make sure we identify the correct quality criteria that will help identify the proper fit for the project or program.","title":"5.3.2 Test tool evaluation"},{"location":"testing/5-3-evaluation/#environment","text":"Each environment presents unique opportunities and challenges for the implementation of test automation tools. Environments may include multiple platforms to support workstations, laptops, and mobile tables and phones. Each of these platforms often will run a different operating system and the software implementation, whether native or through a browser (which are also native to the device), will dictate the functionality to that platform. When evaluating test automation tools, we need to be aware of our complete needs for the environment, including all platforms and devices that ultimately might be required. Be aware that tools perform differently across devices so a prioritization of which device testing should be automated first will help ensure that those most critical platforms can be implemented successfully.","title":"Environment"},{"location":"testing/5-3-evaluation/#test-types","text":"Types and test levels are discussed in sections 4.2 and 4.3 and need to be considering when evaluating test automation tools. This includes defining the types of tests that are candidates for automation across test levels. Tools are purpose built, and each combination of test type/test level may require re-evaluation of a given test tool selected for a prior test type/test level pairing.","title":"Test Types"},{"location":"testing/5-3-evaluation/#technology","text":"Test tools exist to support a variety of older and current technologies (e.g. terminals, text UIs, graphical UIs, browsers, etc.) Some tools support many technologies within one product suite. These are most often COTS tools (see section 6.3.1) where a vendor's tools evolve technology to meet continuing technology needs. However, a bundled solution suite may not necessarily contain \"best-in-class\" for each technology that is supported. An a-la-carte approach to find a specific tool for each technology stack that needs testing may ultimately provide a more satisfactory solution, even where integration amongst disparate tools may be required.","title":"Technology"},{"location":"testing/5-3-evaluation/#language","text":"Most tools provide customization through programmability. In this way they are similar to Interactive Development Environments (IDEs) used by developers to design and build applications. With automation we are building an application too, one that when run automates the execution of testing. Common languages used by test tools include Java, Python, C#, C++, VBScript, etc., with some tools supporting multiple languages. Languages offer a range of features and flexibility and a range of effort to learn them. Identifying tools with the right balance of features and flexibility as they pertain to the knowledge and skills in the organization are key to using a specified tool effectively.","title":"Language"},{"location":"testing/5-3-evaluation/#reporting","text":"When we look at the reporting features of tools we want evaluate both test reporting and logging functionality. The logging function which is often undervalued or not evaluated and is important in determining if errors occurred specific to the automation of tests. The logging function is an audit trail that can be examined to know exactly what occurred doing execution, irrespective of any application criteria being examined. Often the logging function can be customizable to provide specific information that may be required for analysis. For reporting, features should include meaningful displays of data and analysis from which decisions can be made. These may require additional test runs in order to show varying parameters or cumulative data. Most tools have a basic level of on-demand reporting but the better tools will allow for the customization of reporting. Dashboards also provide reporting, most often in a graphical or table based approach where application health is readily displayed. Where other reporting systems exist, exporting data from the tool may be necessary in order to integrate the results with project-level metrics required by project management for planning and reporting purposes.","title":"Reporting"},{"location":"testing/5-3-evaluation/#ease-of-use","text":"Test tools should be intuitive and easy to understand and use. They should be well documented so that common answers to questions and techniques can be quickly identified. For example, one may need to know what function is used to verify the content on a calendar widget or how to export a test report in csv format. Although some test tools may claim that there is no need to use scripting for test development, most medium to complex test requirements will likely require a level of scripting that makes the test perform to meet standards and requirements. Therefore, \"ease of use\" is a relative term. For those automators with strong programming skills it will be easy to develop scripts with a test tool. However, for the traditional manual tester, the requirement to write program code may be difficult and counterproductive to their domain-specific skills.","title":"Ease of Use"},{"location":"testing/5-3-evaluation/#sourcing","text":"As defined in section 6.3.1 tools are available form a variety of sources. These can include COTS, GOTS, and OSS. Each category may have very good solutions. The solution needs to include the sourcing of the tool, the training required for the tool, the ongoing maintenance requirements to the tool, and the support available for the tool (in the form of technical support, product updates, etc.)","title":"Sourcing"},{"location":"testing/5-3-evaluation/#support","text":"Support for test tools can be available from many sources. Vendors often have dedicated support sites that allow logging of issues. These are normally restricted to customers paying for support, either directly or indirectly via maintenance contracts. Additionally forums exist on the internet that focus specifically on particular testing tools. These forums may not be a reliable place for answers as contributions are made by volunteers who may or may not have the knowledge to answer correctly.","title":"Support"},{"location":"testing/5-3-evaluation/#investment","text":"Test tools from commercial entities have a variety of licensing fees structures. These can include individual, per seat, and floating, among others. Understanding the intended usage by your team will help define which license structure that is best for you. Additionally, commercial entities have support and maintenance fees. Support allows for technical questions or issues to be submitted and responded to, while maintenance usually provides for minor and major updates to the testing tool. Working with the most recent version of a tool will often help resolve technical issues (e.g., compatibility, defects, etc.). Training should also be included as a cost, both for vendor and open source solutions.","title":"Investment"},{"location":"testing/5-3-evaluation/#evaluation-tables","text":"Evaluation of the above criteria can be reduced to a table indicating the relative importance for each category, with a total weight at 100%. As tools are evaluated, completing the table helps to compare one tool against another tool objectively. (sample values for illustration) Categories should be rated for how closely they meet the intended requirements. A score allows the category to show a strength or weakness in meeting the category requirement. Use of a scoring system ranging from 0 to 5 where each value indicates alignment to the requirement score. Finally, no tool evaluation is complete without trying the tool out in the intended environment where there is a need to automate. This will ensure that the tool really does work as intended.","title":"EVALUATION TABLES"},{"location":"testing/5-3-evaluation/#533-proof-of-concept","text":"A proof of concept (PoC) is the first step in understanding if a given test tool meets the anticipated needs of the project. With a PoC we want to ensure that the technology that we've provisionally selected actually works in the intended environment. This is key as any prior demonstration of the tool, however capable, needs to now show the same capability in the intended environment.","title":"5.3.3 Proof-of-concept"},{"location":"testing/5-3-evaluation/#534-test-tool-prototypes","text":"A prototype takes the PoC further along by automating a narrowly defined set of functionality from a system where automation is to take hold. The prototype helps the project team understand the complexities of implementation in a timeframe that will not drain excessive resources or funding. The prototype shows the way ahead and serves to realign expectations of how long it actually takes to get test built for automation. The lessons learned from the prototype will bring greater efficiency to the next phase of automation.","title":"5.3.4 Test tool prototypes"},{"location":"testing/5-3-evaluation/#535-tool-training-and-support","text":"Success in automation requires an overall understanding of testing, technology, and the features and functions of specific testing tools in meeting the stated requirements. Appendix B: Test Tools provides a listing of tools that have been shown to be successful across projects and programs. Appendix A: Resources provides additional resources that cover education and certification around testing best practices and test tools. Combining education and certification with hands-on test tool knowledge empowers the test team to develop purpose-built solutions that align with project and program needs.","title":"5.3.5 Tool training and support"},{"location":"testing/6-1-what/","text":"6.1 What Makes Sense to Automate The process of test case selection and evaluation Complex tests Tests that are complex create the possibility of introducing errors into their execution. Testers, careful as they are, are susceptible to making execution errors while they are testing. Complexity is a quality best delegated to computers. However, the complexity of the test need to be designed and tested before or during the codification to automation. Complex tests may require testing results from calculations and those calculation results are best defined outside of the automated test. Otherwise, the automated test adds additional risk of coding and logic error if it not only has to validate results but also needs to calculate them as well. Long tests Often, business scenarios can take 100 - 300 steps in order to be fully exercised. This creates very long tests which are very time-consuming to execute. The longer the manual execution time, the more time there is to introduce an operator error. Additionally, manually executing a long test introduces risk as it is difficult to document the intervening steps making it difficult to state unequivocally if the test was 100% successful. Automation allows us to codify any number of steps necessary to execute a test, all with the possibility of providing an audit trail of that activity. Repeatable tests Tests that are run on a regular basis in order to validate application functionality are excellent candidates for automation as reuse provides high return on investment. Often, projects will use a risk-based approach to testing that balances the risks with the high-value system functions. In this way the most critical functionality is tested first, while less critical functionality is tested time permitting. Often the first tests that will be automated are the smoke tests, or high-level tests which are run for every release, major and minor, and whose function is to establish the most basic operational requirement. This often will include the ability to access a system with valid credentials, navigate across major application functions, and verify that there is database connectivity. Dependency tests Although we develop tests as stand-alone entities, often tests and test data are dependent on one another. For example, when entering an airplane part into an order, we would expect to find that same airplane part in a shipping manifest for that newly created order. If we have one test that creates the order, we need to somehow pass the new order number to the next test so that it can verify the shipping manifest. When testing this manually, we look at the system generated order number, write it down, and then use that order number on the following test. This can get cumbersome and inefficient. With automation, we merely provide an instruction to capture and store the value that we can access at a later time. Scenario based tests Business scenarios often involve multiple dependencies, as described above. However, for the scenario we have a larger scale process that we need to confirm is working. Automation provides an opportunity to link the various functions that are tested across a business scenario. Automation can help synchronize the starting of tests, passing of data from one test to another, aggregating test results, and monitoring systems while the automated test is running. For example, when testing month-end or quarter-end close activities there are a number of processes that need to be done, in a specific order, and dependent upon each other, in order that the final month-end reporting is correct and accurate.","title":"6.1 What Makes Sense to Automate"},{"location":"testing/6-1-what/#61-what-makes-sense-to-automate","text":"The process of test case selection and evaluation","title":"6.1 What Makes Sense to Automate"},{"location":"testing/6-1-what/#complex-tests","text":"Tests that are complex create the possibility of introducing errors into their execution. Testers, careful as they are, are susceptible to making execution errors while they are testing. Complexity is a quality best delegated to computers. However, the complexity of the test need to be designed and tested before or during the codification to automation. Complex tests may require testing results from calculations and those calculation results are best defined outside of the automated test. Otherwise, the automated test adds additional risk of coding and logic error if it not only has to validate results but also needs to calculate them as well.","title":"Complex tests"},{"location":"testing/6-1-what/#long-tests","text":"Often, business scenarios can take 100 - 300 steps in order to be fully exercised. This creates very long tests which are very time-consuming to execute. The longer the manual execution time, the more time there is to introduce an operator error. Additionally, manually executing a long test introduces risk as it is difficult to document the intervening steps making it difficult to state unequivocally if the test was 100% successful. Automation allows us to codify any number of steps necessary to execute a test, all with the possibility of providing an audit trail of that activity.","title":"Long tests"},{"location":"testing/6-1-what/#repeatable-tests","text":"Tests that are run on a regular basis in order to validate application functionality are excellent candidates for automation as reuse provides high return on investment. Often, projects will use a risk-based approach to testing that balances the risks with the high-value system functions. In this way the most critical functionality is tested first, while less critical functionality is tested time permitting. Often the first tests that will be automated are the smoke tests, or high-level tests which are run for every release, major and minor, and whose function is to establish the most basic operational requirement. This often will include the ability to access a system with valid credentials, navigate across major application functions, and verify that there is database connectivity.","title":"Repeatable tests"},{"location":"testing/6-1-what/#dependency-tests","text":"Although we develop tests as stand-alone entities, often tests and test data are dependent on one another. For example, when entering an airplane part into an order, we would expect to find that same airplane part in a shipping manifest for that newly created order. If we have one test that creates the order, we need to somehow pass the new order number to the next test so that it can verify the shipping manifest. When testing this manually, we look at the system generated order number, write it down, and then use that order number on the following test. This can get cumbersome and inefficient. With automation, we merely provide an instruction to capture and store the value that we can access at a later time.","title":"Dependency tests"},{"location":"testing/6-1-what/#scenario-based-tests","text":"Business scenarios often involve multiple dependencies, as described above. However, for the scenario we have a larger scale process that we need to confirm is working. Automation provides an opportunity to link the various functions that are tested across a business scenario. Automation can help synchronize the starting of tests, passing of data from one test to another, aggregating test results, and monitoring systems while the automated test is running. For example, when testing month-end or quarter-end close activities there are a number of processes that need to be done, in a specific order, and dependent upon each other, in order that the final month-end reporting is correct and accurate.","title":"Scenario based tests"},{"location":"testing/6-2-when/","text":"6.2 When Should Automation Occur Ideally, automation of tests should occur once we have a strong understanding of what needs to be tested, when have defined test cases, and when we've developed test data for those test cases. When we've achieved the above, we've likely already tested once or more times manually in order to ensure that our test definitions are correct. This initial manual test activity can save much time for the test automation engineer as that individual will now have valid test conditions and data to work with in developing automated tests. For traditional development methodologies, this would occur after the initial software release and automation would be most helpful in building out a regression test bed. For Agile projects, the automation will likely be scheduled as a 1- or 2-sprint offset as individual sprints themselves do not often allow adequate time to build out the test automation architecture and framework. Automation in Agile becomes a work-in-progress where each successive sprint sees additional functionality and maturation added to the test automation solution.","title":"6.2 When Should Automation Occur"},{"location":"testing/6-2-when/#62-when-should-automation-occur","text":"Ideally, automation of tests should occur once we have a strong understanding of what needs to be tested, when have defined test cases, and when we've developed test data for those test cases. When we've achieved the above, we've likely already tested once or more times manually in order to ensure that our test definitions are correct. This initial manual test activity can save much time for the test automation engineer as that individual will now have valid test conditions and data to work with in developing automated tests. For traditional development methodologies, this would occur after the initial software release and automation would be most helpful in building out a regression test bed. For Agile projects, the automation will likely be scheduled as a 1- or 2-sprint offset as individual sprints themselves do not often allow adequate time to build out the test automation architecture and framework. Automation in Agile becomes a work-in-progress where each successive sprint sees additional functionality and maturation added to the test automation solution.","title":"6.2 When Should Automation Occur"},{"location":"testing/6-3-newtests/","text":"6.3 Criteria for Creating New Automated Tests There may be an opportunity to immediately automate tests, rather than prepare for automation with manual tests. In order for this to happen, the test automation framework has to be sufficiently developed to allow its use for new tests. The timing of this is dependent on the complexity of the application and components that need to be tested. For example, if an application has 200 different controls spread across 5 functional areas, the framework must be capable of identifying and interacting with each of those controls, so that no test is off limits for development. If the 200 controls can be mapped out against the 5 functional areas, then as long as the functional areas under test have been verified and are compatible automation can proceed. Often automation teams do not do their due diligence to find all controls and ensure that the test automation solution indeed is compatible with every one of them. It only takes 1 error in trying to interact with a control for testing to fail, thus undermining the automated approach.","title":"6.3 Criteria for Creating New Automated Tests"},{"location":"testing/6-3-newtests/#63-criteria-for-creating-new-automated-tests","text":"There may be an opportunity to immediately automate tests, rather than prepare for automation with manual tests. In order for this to happen, the test automation framework has to be sufficiently developed to allow its use for new tests. The timing of this is dependent on the complexity of the application and components that need to be tested. For example, if an application has 200 different controls spread across 5 functional areas, the framework must be capable of identifying and interacting with each of those controls, so that no test is off limits for development. If the 200 controls can be mapped out against the 5 functional areas, then as long as the functional areas under test have been verified and are compatible automation can proceed. Often automation teams do not do their due diligence to find all controls and ensure that the test automation solution indeed is compatible with every one of them. It only takes 1 error in trying to interact with a control for testing to fail, thus undermining the automated approach.","title":"6.3 Criteria for Creating New Automated Tests"},{"location":"testing/6-4-manualtests/","text":"6.4 Criteria for Converting Manual Tests to Automation Most projects have documented test cases and scripts defining the test steps. When evaluating the migration of tests to automation, the following should be considered: Is the test necessary? Often, tests that were developed a long time ago no longer provide the value they once did as applications evolve and other newer tests may already cover parts of the functionality from older tests. When examining existing tests, evaluate if any or all of a test's functions may already be present in other tests. Is the test complete? Does the test exercise the complete business process so that it provides relevant, meaningful results? If not, the business analyst, manual tester, and test automation engineer will likely need to work together to enhance the capability of the automated test. Is the test effective? Effectiveness for any test can be measured by coverage to the requirement and coverage to the code. Specification based testing techniques help us determine how to construct data sets that will provide us with the least amount of test cases that can generate the highest level of coverage, thus mitigating deployment risk. Code coverage tools can be run by developers in parallel to test execution to verify the code coverage. Automation provides us the vehicle by which additional quality test cases can now be executed which previously were not possible, due to time and resource constraints, under a manual scenario. Is the test efficient? Test efficiency can take on several aspects, from speed of execution to overall construction. Automation by itself will always execute a test faster than can be done manually. However, not all manual tests have been designed in a way that is efficient for automation to process them. For example, for many applications, testing follows a prescribed path through an application. The path of navigation is what allows us to test getting from, say, the login screen to the reports screen. Some applications can have complex or lengthy paths before even getting to test for data. With automation we have the opportunity to create navigational tests that can be combined with functional tests in a manner that allows for reuse. Regarding test construction, the approach used to develop a manual test will often differ from the approach to develop an automated test. This is due to the different ways in which each test is executed. Testers think and act linearly, while computers can be made to jump from one area to another. Decomposition of manual tests and reconstruction is a worthwhile effort in order to maximize the quality of automated tests produced. Depending on how manual tests have been created, they may be: Broken apart and reconstituted across various automated tests Combined into one larger automated test Converted one-for-one as an automated test","title":"6.4 Criteria for Converting Manual Tests to Automation"},{"location":"testing/6-4-manualtests/#64-criteria-for-converting-manual-tests-to-automation","text":"Most projects have documented test cases and scripts defining the test steps. When evaluating the migration of tests to automation, the following should be considered:","title":"6.4 Criteria for Converting Manual Tests to Automation"},{"location":"testing/6-4-manualtests/#is-the-test-necessary","text":"Often, tests that were developed a long time ago no longer provide the value they once did as applications evolve and other newer tests may already cover parts of the functionality from older tests. When examining existing tests, evaluate if any or all of a test's functions may already be present in other tests.","title":"Is the test necessary?"},{"location":"testing/6-4-manualtests/#is-the-test-complete","text":"Does the test exercise the complete business process so that it provides relevant, meaningful results? If not, the business analyst, manual tester, and test automation engineer will likely need to work together to enhance the capability of the automated test.","title":"Is the test complete?"},{"location":"testing/6-4-manualtests/#is-the-test-effective","text":"Effectiveness for any test can be measured by coverage to the requirement and coverage to the code. Specification based testing techniques help us determine how to construct data sets that will provide us with the least amount of test cases that can generate the highest level of coverage, thus mitigating deployment risk. Code coverage tools can be run by developers in parallel to test execution to verify the code coverage. Automation provides us the vehicle by which additional quality test cases can now be executed which previously were not possible, due to time and resource constraints, under a manual scenario.","title":"Is the test effective?"},{"location":"testing/6-4-manualtests/#is-the-test-efficient","text":"Test efficiency can take on several aspects, from speed of execution to overall construction. Automation by itself will always execute a test faster than can be done manually. However, not all manual tests have been designed in a way that is efficient for automation to process them. For example, for many applications, testing follows a prescribed path through an application. The path of navigation is what allows us to test getting from, say, the login screen to the reports screen. Some applications can have complex or lengthy paths before even getting to test for data. With automation we have the opportunity to create navigational tests that can be combined with functional tests in a manner that allows for reuse. Regarding test construction, the approach used to develop a manual test will often differ from the approach to develop an automated test. This is due to the different ways in which each test is executed. Testers think and act linearly, while computers can be made to jump from one area to another. Decomposition of manual tests and reconstruction is a worthwhile effort in order to maximize the quality of automated tests produced. Depending on how manual tests have been created, they may be: Broken apart and reconstituted across various automated tests Combined into one larger automated test Converted one-for-one as an automated test","title":"Is the test efficient?"},{"location":"testing/6-5-transition/","text":"6.5 Transitioning Staff to Automation Since the advent of testing tools there has been talk that automation kills testing jobs. For large programs heavily dependent on manual testers we would expect to see a reduction of an inefficient process. However, automation mostly helps testers increase their focus on developing quality tests rather than on time consuming \"key pounding\" test execution activity. It is very important to make sure staff understand that they are valued for their knowledge of systems and their testing acumen. It is also important to acknowledge that in order for automation to be successful, there are different roles, and not all roles require programing skills. Section 3.5 describes the various roles necessary for the tasks required to make automation successful. Outsourced solutions Test automation solutions can be contracted out to firms knowledgeable in creating and maintaining test automation frameworks. In this scenario, a firm would be contracted to assist in automating all or part of a system. Once the capability is developed the government would request documentation, training, and transitioning as part of the final deliverable. Government developed solutions Government developed automation would require the selection of staff with the appropriate skills and training so that the process of building out automation does not become a learn-as-you-go exercise, which ultimately will not show a high return on investment. Appendix A covers many resources that can provide the necessary knowledge for team members. Acquiring automation assets In addition to the wide range of test automation tools described in Section 5.3.1 and listed in Appendix B there exist pre-made frameworks that can be used with existing tools. These frameworks can offer a shortcut to a build-from-scratch strategy, and often one can continue to build additional functionality to the base framework libraries. Regardless of the solution used in migrating to automation, a tool-agnostic approach that abstracts test data from test tools provides the greatest flexibility and overall longevity.","title":"6.5 Transitioning Staff to Automation"},{"location":"testing/6-5-transition/#65-transitioning-staff-to-automation","text":"Since the advent of testing tools there has been talk that automation kills testing jobs. For large programs heavily dependent on manual testers we would expect to see a reduction of an inefficient process. However, automation mostly helps testers increase their focus on developing quality tests rather than on time consuming \"key pounding\" test execution activity. It is very important to make sure staff understand that they are valued for their knowledge of systems and their testing acumen. It is also important to acknowledge that in order for automation to be successful, there are different roles, and not all roles require programing skills. Section 3.5 describes the various roles necessary for the tasks required to make automation successful.","title":"6.5 Transitioning Staff to Automation"},{"location":"testing/6-5-transition/#outsourced-solutions","text":"Test automation solutions can be contracted out to firms knowledgeable in creating and maintaining test automation frameworks. In this scenario, a firm would be contracted to assist in automating all or part of a system. Once the capability is developed the government would request documentation, training, and transitioning as part of the final deliverable.","title":"Outsourced solutions"},{"location":"testing/6-5-transition/#government-developed-solutions","text":"Government developed automation would require the selection of staff with the appropriate skills and training so that the process of building out automation does not become a learn-as-you-go exercise, which ultimately will not show a high return on investment. Appendix A covers many resources that can provide the necessary knowledge for team members.","title":"Government developed solutions"},{"location":"testing/6-5-transition/#acquiring-automation-assets","text":"In addition to the wide range of test automation tools described in Section 5.3.1 and listed in Appendix B there exist pre-made frameworks that can be used with existing tools. These frameworks can offer a shortcut to a build-from-scratch strategy, and often one can continue to build additional functionality to the base framework libraries. Regardless of the solution used in migrating to automation, a tool-agnostic approach that abstracts test data from test tools provides the greatest flexibility and overall longevity.","title":"Acquiring automation assets"},{"location":"testing/7-1-phases/","text":"7.1 Phases in Automation Development The lifecycle phases for development of a test automation solution are not unlike software development for software applications. Although there are differences the process mostly follows the same steps, which include: Requirements Phase The requirements phase allows us to capture features and functionality necessary for our automation solution. Requirements for automation often fall within the following areas: Input/output (I/O) requirements External interface requirements User interface (UI) control requirements Navigation requirements Timing and synchronization requirements Development of logging and reporting functions Requirements for utilities Requirements for automation are all about enabling interaction with the SUT and providing clear reporting of those interactions. Design Phase The design phase for automation includes the evaluation of tools that as described in section 5.3, Evaluation and selection of test tools. Here, we differ from traditional software development in that we are finding test tools that offer compatibility and features needed to interact with our SUT. This is the primary consideration of our design as failing to ensure compatibility with the SUT prevents us from effectively automating. Technical Design Phase The technical design for automation is described in section 5.2, Overall architectural considerations. Here we define an approach that covers the various interfaces and reporting requirements of our SUT. We map out our framework components in this phase so that we know what development activity will be required. Development Phase The development phase in automation will include the possibility of programming functions and developing libraries that address our stated requirements. Additionally it may include some pre-built modules that provide out-of-the-box functionality from our selected test tool or it may include some third-party modules that integrate with the test tool add functionality. Test Phase Each feature and function that has been engineered into our automated solution must be tested in order that we can be certain it will reliably and accurately help us in testing the SUT. A test baseline must be used in order to validate functionality as testing the automation against the SUT may be unpredictable. Implementation Phase Once we have a high degree of confidence that our automation solution is working properly, we can start using it to test against the SUT. While we expect that automated testing will help us uncover defects, we always first need to verify that the defect is not attributable to the automation components. With automation, the features and functions we build are there to support our need for testing against software application programs.","title":"7.1 Phases in Automation Development"},{"location":"testing/7-1-phases/#71-phases-in-automation-development","text":"The lifecycle phases for development of a test automation solution are not unlike software development for software applications. Although there are differences the process mostly follows the same steps, which include:","title":"7.1 Phases in Automation Development"},{"location":"testing/7-1-phases/#requirements-phase","text":"The requirements phase allows us to capture features and functionality necessary for our automation solution. Requirements for automation often fall within the following areas: Input/output (I/O) requirements External interface requirements User interface (UI) control requirements Navigation requirements Timing and synchronization requirements Development of logging and reporting functions Requirements for utilities Requirements for automation are all about enabling interaction with the SUT and providing clear reporting of those interactions.","title":"Requirements Phase"},{"location":"testing/7-1-phases/#design-phase","text":"The design phase for automation includes the evaluation of tools that as described in section 5.3, Evaluation and selection of test tools. Here, we differ from traditional software development in that we are finding test tools that offer compatibility and features needed to interact with our SUT. This is the primary consideration of our design as failing to ensure compatibility with the SUT prevents us from effectively automating.","title":"Design Phase"},{"location":"testing/7-1-phases/#technical-design-phase","text":"The technical design for automation is described in section 5.2, Overall architectural considerations. Here we define an approach that covers the various interfaces and reporting requirements of our SUT. We map out our framework components in this phase so that we know what development activity will be required.","title":"Technical Design Phase"},{"location":"testing/7-1-phases/#development-phase","text":"The development phase in automation will include the possibility of programming functions and developing libraries that address our stated requirements. Additionally it may include some pre-built modules that provide out-of-the-box functionality from our selected test tool or it may include some third-party modules that integrate with the test tool add functionality.","title":"Development Phase"},{"location":"testing/7-1-phases/#test-phase","text":"Each feature and function that has been engineered into our automated solution must be tested in order that we can be certain it will reliably and accurately help us in testing the SUT. A test baseline must be used in order to validate functionality as testing the automation against the SUT may be unpredictable.","title":"Test Phase"},{"location":"testing/7-1-phases/#implementation-phase","text":"Once we have a high degree of confidence that our automation solution is working properly, we can start using it to test against the SUT. While we expect that automated testing will help us uncover defects, we always first need to verify that the defect is not attributable to the automation components. With automation, the features and functions we build are there to support our need for testing against software application programs.","title":"Implementation Phase"},{"location":"testing/7-2-appdev/","text":"7.2 Similarities to Application Development Development of a test automation solution should include best practices. These include, but are not limited to: Reviews Reviews are an important aspect of making sure that what is being developed stays true to what is expected. Implied in the review process is that someone other than the individual producing the work has an opportunity to evaluate and provide feedback. Reviews are often done incrementally thus allowing for changes or course-correction to take place if warranted. Reviews can start at a very high level, progress to a very detailed level, and also include quality characteristics. Reviews can cover: Architecture Code Peer Usability Documentation Documentation helps us understand the automated solution at various levels. From how it works, and what features it contains, to how to make changes or upgrades. Documentation helps make sure that the automated solution is used properly. In summary we can use documentation for: In-line code descriptions Function descriptions Library cataloging User instructions Maintenance instructions Standard variable naming The way in which variables are named in a software often helps us understand the context and use of that variable. Using capitalization and lower case letters in a mixed case format along with compound word choices can facilitate the automation engineer's understanding of what variables are used for. This also provides discipline for new variables to use the same naming standards and conventions. Abstraction Abstraction indicates a separation of all but the most relevant data in order to reduce complexity, duplication, and increase efficiency of the automation code base. With automation, an example of abstraction may revolve around the way we choose to interact with a given UI control. With abstraction, we would create a single function that is used to interact with that control regardless of where it appears within the SUT. The function itself will have the details pertaining to what methods and data the control needs. Cyclomatic complexity The cyclomatic complexity metric is a quantitative measure of code complexity. It allows us to measure the number of linearly independent paths through code. A high cyclomatic complexity value indicates that many (if not too many) paths exist which in turn indicates a high number of test cases necessary to fully validate code. Although most test automation code is not subject to cyclomatic complexity analysis, in the way that a software program might, it is good practice to keep the number of independent paths (often referred to as conditions) low so that we can ensure proper testing can be done while minimizing potential errors due to code complexity.","title":"7.2 Similarities to Application Development"},{"location":"testing/7-2-appdev/#72-similarities-to-application-development","text":"Development of a test automation solution should include best practices. These include, but are not limited to:","title":"7.2 Similarities to Application Development"},{"location":"testing/7-2-appdev/#reviews","text":"Reviews are an important aspect of making sure that what is being developed stays true to what is expected. Implied in the review process is that someone other than the individual producing the work has an opportunity to evaluate and provide feedback. Reviews are often done incrementally thus allowing for changes or course-correction to take place if warranted. Reviews can start at a very high level, progress to a very detailed level, and also include quality characteristics. Reviews can cover: Architecture Code Peer Usability","title":"Reviews"},{"location":"testing/7-2-appdev/#documentation","text":"Documentation helps us understand the automated solution at various levels. From how it works, and what features it contains, to how to make changes or upgrades. Documentation helps make sure that the automated solution is used properly. In summary we can use documentation for: In-line code descriptions Function descriptions Library cataloging User instructions Maintenance instructions","title":"Documentation"},{"location":"testing/7-2-appdev/#standard-variable-naming","text":"The way in which variables are named in a software often helps us understand the context and use of that variable. Using capitalization and lower case letters in a mixed case format along with compound word choices can facilitate the automation engineer's understanding of what variables are used for. This also provides discipline for new variables to use the same naming standards and conventions.","title":"Standard variable naming"},{"location":"testing/7-2-appdev/#abstraction","text":"Abstraction indicates a separation of all but the most relevant data in order to reduce complexity, duplication, and increase efficiency of the automation code base. With automation, an example of abstraction may revolve around the way we choose to interact with a given UI control. With abstraction, we would create a single function that is used to interact with that control regardless of where it appears within the SUT. The function itself will have the details pertaining to what methods and data the control needs.","title":"Abstraction"},{"location":"testing/7-2-appdev/#cyclomatic-complexity","text":"The cyclomatic complexity metric is a quantitative measure of code complexity. It allows us to measure the number of linearly independent paths through code. A high cyclomatic complexity value indicates that many (if not too many) paths exist which in turn indicates a high number of test cases necessary to fully validate code. Although most test automation code is not subject to cyclomatic complexity analysis, in the way that a software program might, it is good practice to keep the number of independent paths (often referred to as conditions) low so that we can ensure proper testing can be done while minimizing potential errors due to code complexity.","title":"Cyclomatic complexity"},{"location":"testing/7-3-manualtests/","text":"7.3 Similarities to Manual Testing Once we have our test automation solution up and running and we have data to drive some testing we can confirm that our automated solution properly tests the SUT and faithfully reproduces the testing that was otherwise done manually. We still need to analyze what is to be tested. From this we need to define the test types and develop test data. This is all still very similar to the work we do in manual testing. However, the definition of data for automated testing is more structured as we're now reliant on the test automation solution and specifically the testing framework to work with our defined data and provide us with results. As with manual testing we will have results to analyze, but with automation we would expect a good portion of this analysis, at least as it pertains to verifications and comparisons, to occur automatically via automation. From here we need to ensure the confidence in our acceptance in that automation has met or exceeded our previous manual testing efforts. This is an important milestone and one which can help bring the team together in supporting their contributions in the development of test automation.","title":"7.3 Similarities to Manual Testing"},{"location":"testing/7-3-manualtests/#73-similarities-to-manual-testing","text":"Once we have our test automation solution up and running and we have data to drive some testing we can confirm that our automated solution properly tests the SUT and faithfully reproduces the testing that was otherwise done manually. We still need to analyze what is to be tested. From this we need to define the test types and develop test data. This is all still very similar to the work we do in manual testing. However, the definition of data for automated testing is more structured as we're now reliant on the test automation solution and specifically the testing framework to work with our defined data and provide us with results. As with manual testing we will have results to analyze, but with automation we would expect a good portion of this analysis, at least as it pertains to verifications and comparisons, to occur automatically via automation. From here we need to ensure the confidence in our acceptance in that automation has met or exceeded our previous manual testing efforts. This is an important milestone and one which can help bring the team together in supporting their contributions in the development of test automation.","title":"7.3 Similarities to Manual Testing"},{"location":"testing/8-1-measurements/","text":"8.1 Effectiveness Measurements Automation impacts can be measured beyond speed of test execution. The following areas provide metrics on use and effectiveness of automation. Schedule Test Execution time improvement Test Setup time improvement Time to determine failures/defects Time to analyze data Effectiveness Failures found Number of test required/number of system errors Defects found/number of test procedures executed Test procedures executed without defects/ total test procedures Coverage Test Coverage - test procedures/test requirements Automation test coverage - automated test cases/total test cases New test capabilities Reusability improvement within project/program Cost Man-hour reduction Total project or program savings Additional training requirement Additional resources Additional Maintainability Each project or program should select those metrics that provide the most relevant feedback to assess quality and effectiveness of automation. Although test execution times are dramatically shortened through the use of automation, test preparation steps and maintenance for automation need to be measured as part of the overall effort and cost, as compared to manual testing.","title":"8.1 Effectiveness Measurements"},{"location":"testing/8-1-measurements/#81-effectiveness-measurements","text":"Automation impacts can be measured beyond speed of test execution. The following areas provide metrics on use and effectiveness of automation.","title":"8.1 Effectiveness Measurements"},{"location":"testing/8-1-measurements/#schedule","text":"Test Execution time improvement Test Setup time improvement Time to determine failures/defects Time to analyze data","title":"Schedule"},{"location":"testing/8-1-measurements/#effectiveness","text":"Failures found Number of test required/number of system errors Defects found/number of test procedures executed Test procedures executed without defects/ total test procedures","title":"Effectiveness"},{"location":"testing/8-1-measurements/#coverage","text":"Test Coverage - test procedures/test requirements Automation test coverage - automated test cases/total test cases New test capabilities Reusability improvement within project/program","title":"Coverage"},{"location":"testing/8-1-measurements/#cost","text":"Man-hour reduction Total project or program savings Additional training requirement Additional resources Additional Maintainability Each project or program should select those metrics that provide the most relevant feedback to assess quality and effectiveness of automation. Although test execution times are dramatically shortened through the use of automation, test preparation steps and maintenance for automation need to be measured as part of the overall effort and cost, as compared to manual testing.","title":"Cost"},{"location":"testing/9-1-reporting/","text":"9.1 Reporting Test reporting is an important part of the process of automated testing. There are a number of different ways by which we can produce reports from automation. Test execution tools Test execution tools represent the most critical component in reporting. It is at the execution of tests that data can be captured which is then used for reporting. Test execution tools will often have one reporting mechanism that shows that occurred during the test execution, including any verification steps and any failure points. The reporting provided by these tools is very focused to the sequence of events that occurred and often do not capture larger trends. Execution tools often have the ability to customize the reporting, often by inserting messages and custom output text to complement default reporting. Additionally, execution tools may have separate logs that show if any errors occurred as functions were called. Custom logs can be created in order to meet specific output requirements, like when data needs to be imported into other tools in a standardized format. Test management tools Test management tools often help to store and aggregate data and can report on an individual or group of tests. This is helpful in order to see trends in application quality. Management tools often have several mechanisms for reporting including standard table reports, summary table reports, and graphical data representations. These can include charts, graphs, and other visual elements that facilitate analyzing high level data without having to look at the more granular data. Test reporting tools Reporting tools exist that are not specific to testing but that can take a variety of input files (e.g. xls, cvs, etc.) and aggregate results for meaningful display. There is overlap with the management tools on some reports. However, reporting tools can also include project management tools that help with scheduling activities and task interdependencies. As tools have a diverse set of reporting functionality, it may take more than one tool to get all reporting requirements in place. Therefore, understanding each tool's import/export capabilities and requirements will ensure that data can freely flow from one reporting tool to another.","title":"9.1 Reporting"},{"location":"testing/9-1-reporting/#91-reporting","text":"Test reporting is an important part of the process of automated testing. There are a number of different ways by which we can produce reports from automation.","title":"9.1 Reporting"},{"location":"testing/9-1-reporting/#test-execution-tools","text":"Test execution tools represent the most critical component in reporting. It is at the execution of tests that data can be captured which is then used for reporting. Test execution tools will often have one reporting mechanism that shows that occurred during the test execution, including any verification steps and any failure points. The reporting provided by these tools is very focused to the sequence of events that occurred and often do not capture larger trends. Execution tools often have the ability to customize the reporting, often by inserting messages and custom output text to complement default reporting. Additionally, execution tools may have separate logs that show if any errors occurred as functions were called. Custom logs can be created in order to meet specific output requirements, like when data needs to be imported into other tools in a standardized format.","title":"Test execution tools"},{"location":"testing/9-1-reporting/#test-management-tools","text":"Test management tools often help to store and aggregate data and can report on an individual or group of tests. This is helpful in order to see trends in application quality. Management tools often have several mechanisms for reporting including standard table reports, summary table reports, and graphical data representations. These can include charts, graphs, and other visual elements that facilitate analyzing high level data without having to look at the more granular data.","title":"Test management tools"},{"location":"testing/9-1-reporting/#test-reporting-tools","text":"Reporting tools exist that are not specific to testing but that can take a variety of input files (e.g. xls, cvs, etc.) and aggregate results for meaningful display. There is overlap with the management tools on some reports. However, reporting tools can also include project management tools that help with scheduling activities and task interdependencies. As tools have a diverse set of reporting functionality, it may take more than one tool to get all reporting requirements in place. Therefore, understanding each tool's import/export capabilities and requirements will ensure that data can freely flow from one reporting tool to another.","title":"Test reporting tools"},{"location":"ux/1-1-purpose/","text":"1.1 Purpose This User Experience Playbook is a continuation of effort by the BES PEO to establish modern practices and methods to accelerate software development and deployment of value for Logistics Information Systems and its end users. Preceded by playbooks for Agile and Automated Testing, the User Experience playbook aims to define a framework for how to plan and structure projects to include User Experience (UX) in both Agile and Waterfall environments, describing each stage of the process including key activities and deliverables as well as their benefits. Furthermore, this playbook aims to establish core User Experience design principles unique to the mission of Logistics Information Systems, as well as Web and Component Design Standards to create a more unified and consistent digital presence while enabling more rapid planning and deployment of future digital initiatives. These Web and Component Design Standards will include guidelines for typography, color and other style parameters in addition to reusable design patterns for common components such as navigation, buttons, alerts and form controls. The User Experience Playbook will serve as a \u201cliving document,\u201d delivering value in the present while providing a foundation for future updates and enhancements as the state of technology and the needs of stakeholders and users served by Logistics Information Systems continually evolve. For those seeking further resources and materials to help you advance along the path the understanding User Experience practices and methods, please see the Resources page within the Appendix.","title":"1.1 Purpose"},{"location":"ux/1-1-purpose/#11-purpose","text":"This User Experience Playbook is a continuation of effort by the BES PEO to establish modern practices and methods to accelerate software development and deployment of value for Logistics Information Systems and its end users. Preceded by playbooks for Agile and Automated Testing, the User Experience playbook aims to define a framework for how to plan and structure projects to include User Experience (UX) in both Agile and Waterfall environments, describing each stage of the process including key activities and deliverables as well as their benefits. Furthermore, this playbook aims to establish core User Experience design principles unique to the mission of Logistics Information Systems, as well as Web and Component Design Standards to create a more unified and consistent digital presence while enabling more rapid planning and deployment of future digital initiatives. These Web and Component Design Standards will include guidelines for typography, color and other style parameters in addition to reusable design patterns for common components such as navigation, buttons, alerts and form controls. The User Experience Playbook will serve as a \u201cliving document,\u201d delivering value in the present while providing a foundation for future updates and enhancements as the state of technology and the needs of stakeholders and users served by Logistics Information Systems continually evolve. For those seeking further resources and materials to help you advance along the path the understanding User Experience practices and methods, please see the Resources page within the Appendix.","title":"1.1 Purpose"},{"location":"ux/1-2-audience/","text":"1.2 Audience While this playbook can provide value to all personnel involved in a software development project, the primary audience for this playbook are those individuals who are responsible for the planning, management and development of projects that employ or might benefit from User Experience methodologies. For program managers and those in similar roles, this playbook will provide a foundation of knowledge to enable more effective planning and support for programs where User Experience methodologies might be employed. It will also establish a framework by which the quality and effectiveness of Logistics Information Systems\u2019 digital experiences can be evaluated. For software engineers and those in similar roles, this playbook will provide a valuable resource for design patterns and guidelines from which new digital experiences can be created, enabling faster design decisions and more rapid deployment while ensuring consistency and quality across all digital experiences delivered by Logistics Information Systems. For software development project teams as a whole, this playbook will help establish key concepts and methods that will enable more consistent delivery of solutions that meet the needs of end users and the broader organization. These operational benefits will translate to a more efficient and effective workforce, equipped with digital tools that reliably enhance their ability to achieve mission-critical goals.","title":"1.2 Audience"},{"location":"ux/1-2-audience/#12-audience","text":"While this playbook can provide value to all personnel involved in a software development project, the primary audience for this playbook are those individuals who are responsible for the planning, management and development of projects that employ or might benefit from User Experience methodologies. For program managers and those in similar roles, this playbook will provide a foundation of knowledge to enable more effective planning and support for programs where User Experience methodologies might be employed. It will also establish a framework by which the quality and effectiveness of Logistics Information Systems\u2019 digital experiences can be evaluated. For software engineers and those in similar roles, this playbook will provide a valuable resource for design patterns and guidelines from which new digital experiences can be created, enabling faster design decisions and more rapid deployment while ensuring consistency and quality across all digital experiences delivered by Logistics Information Systems. For software development project teams as a whole, this playbook will help establish key concepts and methods that will enable more consistent delivery of solutions that meet the needs of end users and the broader organization. These operational benefits will translate to a more efficient and effective workforce, equipped with digital tools that reliably enhance their ability to achieve mission-critical goals.","title":"1.2 Audience"},{"location":"ux/1-3-benefits/","text":"1.3 Benefits of UX While a number of disciplines have long paved the way for User Experience \u2013 including cognitive psychology, library science and human-computer interaction (HCI) \u2013 the need for organizations to create and adopt this unique discipline arose at the outset of the internet era as society became increasingly dependent on user interfaces to facilitate daily tasks. The aim of this new discipline was to establish more reliable practices and methods to deliver user-centered software solutions, putting aside biases in the way project teams imagine user needs to create a more firm foundation of evidence upon which to base design decisions. The practices and methods offered by the User Experience discipline ultimately help create greater efficiency on the project team while promoting greater effectiveness and satisfaction among end users. Studies consistently reinforce these benefits. A frequently cited report by IEEE states that: 70% of projects fail due to lack of user acceptance Investment in User Experience results in 33\u201350% reduced development time by having better upfront definition of requirements and avoiding rework Every dollar invested in UX brings 100 dollars in return Among the most common factors for project failure are: Unrealistic or unarticulated project goals Inaccurate estimates of needed resources Badly defined system requirements\u00b4 Poor communication among customers, developers and users Inability to handle the project\u2019s complexity User Experience is the key to addressing many of these factors. A well-defined User Experience process can help to establish and clarify project goals, balancing the needs of the people and organizations that stand to benefit most from digital products. The process also helps validate and refine system requirements by facilitating their analysis through the lens of user value, organizational value and complexity, identifying and resolving areas of ambiguity prior to the initiation of development cycles. These benefits leads to stronger project plans, better estimation and increased chances for project success. The USAF is unique in its challenges and opportunities as they relate to incorporating User Experience practices and methods within its daily operations. The USAF became reliant on technology systems far in advance of the general public, and many of those early systems were deployed at a larger scale than comparable commercial systems. In addition, these software systems support highly mission-critical operations where effectiveness cannot be compromised. This makes the effort associated with updating legacy systems and modernizing practices much more challenging and sensitive than are typically found. However these factors also reinforce the need to make these changes, taking advantage of better technological capabilities and User Experience practices that yield greater effectiveness across the organization and its workforce, ensuring greater success and reliability for these mission-critical operations. Reference Why Software Fails https://spectrum.ieee.org/computing/software/why-software-fails","title":"1.3 Benefits of UX"},{"location":"ux/1-3-benefits/#13-benefits-of-ux","text":"While a number of disciplines have long paved the way for User Experience \u2013 including cognitive psychology, library science and human-computer interaction (HCI) \u2013 the need for organizations to create and adopt this unique discipline arose at the outset of the internet era as society became increasingly dependent on user interfaces to facilitate daily tasks. The aim of this new discipline was to establish more reliable practices and methods to deliver user-centered software solutions, putting aside biases in the way project teams imagine user needs to create a more firm foundation of evidence upon which to base design decisions. The practices and methods offered by the User Experience discipline ultimately help create greater efficiency on the project team while promoting greater effectiveness and satisfaction among end users.","title":"1.3 Benefits of UX"},{"location":"ux/1-3-benefits/#studies-consistently-reinforce-these-benefits-a-frequently-cited-report-by-ieee-states-that","text":"70% of projects fail due to lack of user acceptance Investment in User Experience results in 33\u201350% reduced development time by having better upfront definition of requirements and avoiding rework Every dollar invested in UX brings 100 dollars in return","title":"Studies consistently reinforce these benefits. A frequently cited report by IEEE states that:"},{"location":"ux/1-3-benefits/#among-the-most-common-factors-for-project-failure-are","text":"Unrealistic or unarticulated project goals Inaccurate estimates of needed resources Badly defined system requirements\u00b4 Poor communication among customers, developers and users Inability to handle the project\u2019s complexity User Experience is the key to addressing many of these factors. A well-defined User Experience process can help to establish and clarify project goals, balancing the needs of the people and organizations that stand to benefit most from digital products. The process also helps validate and refine system requirements by facilitating their analysis through the lens of user value, organizational value and complexity, identifying and resolving areas of ambiguity prior to the initiation of development cycles. These benefits leads to stronger project plans, better estimation and increased chances for project success. The USAF is unique in its challenges and opportunities as they relate to incorporating User Experience practices and methods within its daily operations. The USAF became reliant on technology systems far in advance of the general public, and many of those early systems were deployed at a larger scale than comparable commercial systems. In addition, these software systems support highly mission-critical operations where effectiveness cannot be compromised. This makes the effort associated with updating legacy systems and modernizing practices much more challenging and sensitive than are typically found. However these factors also reinforce the need to make these changes, taking advantage of better technological capabilities and User Experience practices that yield greater effectiveness across the organization and its workforce, ensuring greater success and reliability for these mission-critical operations.","title":"Among the most common factors for project failure are:"},{"location":"ux/1-3-benefits/#reference","text":"Why Software Fails https://spectrum.ieee.org/computing/software/why-software-fails","title":"Reference"},{"location":"ux/1-4-maturity/","text":"1.4 Levels of Design Maturity Integration of user experience as a core strategic pillar within your organization is not a binary proposition. Every organization will have differences in the extent to which they have adopted design into their business process. For this reason, InVision defined a maturity model for experience design adoption. In this model, organizations are rated at one of five levels. It\u2019s important to be cognizant of your current maturity level within this model, and to appreciate that moving up this model only increases the overall return on investment to your mission, your stakeholders, and your end users. For example, per InVision\u2019s research, organizations are five times more likely to report having an impact on cost savings, when functioning at a Level 5 versus a Level 1. This number only increases when examining time to release. 1.4.1 Leveling Up Increasing your organization\u2019s strategic alignment around design and adoption of design practices can be done by adhering to a few guiding principles. Within each principle below are statements that, per InVision\u2019s findings, hold true at least twice as often in Level 5 organizations as in Level 1. Top-down belief in the value of design Executives talk about the value of design internally. Design shares priorities and goals with key partners. Executives prioritize decisions that lead to the best design/customer experience. Involve design throughout the software development process Design is well-integrated in the product development process. Design leaders are peers with product management and engineering leaders. Value user research Employees participate in user/customer research. Employees understand why human-centered design is valuable. Share design research and decision-making across disciplines Design work is shared in all hands meetings, important executive meetings, and other influential gatherings. Design has joint working sessions with key partners (e.g., workshops, stand-ups, etc.). Product/feature ideas are jointly developed and owned between design and key partners. Reference The New Design Frontier https://www.invisionapp.com/design-better/design-maturity-model/","title":"1.4 Levels of Design Maturity"},{"location":"ux/1-4-maturity/#14-levels-of-design-maturity","text":"Integration of user experience as a core strategic pillar within your organization is not a binary proposition. Every organization will have differences in the extent to which they have adopted design into their business process. For this reason, InVision defined a maturity model for experience design adoption. In this model, organizations are rated at one of five levels. It\u2019s important to be cognizant of your current maturity level within this model, and to appreciate that moving up this model only increases the overall return on investment to your mission, your stakeholders, and your end users. For example, per InVision\u2019s research, organizations are five times more likely to report having an impact on cost savings, when functioning at a Level 5 versus a Level 1. This number only increases when examining time to release.","title":"1.4 Levels of Design Maturity"},{"location":"ux/1-4-maturity/#141-leveling-up","text":"Increasing your organization\u2019s strategic alignment around design and adoption of design practices can be done by adhering to a few guiding principles. Within each principle below are statements that, per InVision\u2019s findings, hold true at least twice as often in Level 5 organizations as in Level 1.","title":"1.4.1 Leveling Up"},{"location":"ux/1-4-maturity/#top-down-belief-in-the-value-of-design","text":"Executives talk about the value of design internally. Design shares priorities and goals with key partners. Executives prioritize decisions that lead to the best design/customer experience.","title":"Top-down belief in the value of design"},{"location":"ux/1-4-maturity/#involve-design-throughout-the-software-development-process","text":"Design is well-integrated in the product development process. Design leaders are peers with product management and engineering leaders.","title":"Involve design throughout the software development process"},{"location":"ux/1-4-maturity/#value-user-research","text":"Employees participate in user/customer research. Employees understand why human-centered design is valuable.","title":"Value user research"},{"location":"ux/1-4-maturity/#share-design-research-and-decision-making-across-disciplines","text":"Design work is shared in all hands meetings, important executive meetings, and other influential gatherings. Design has joint working sessions with key partners (e.g., workshops, stand-ups, etc.). Product/feature ideas are jointly developed and owned between design and key partners.","title":"Share design research and decision-making across disciplines"},{"location":"ux/1-4-maturity/#reference","text":"The New Design Frontier https://www.invisionapp.com/design-better/design-maturity-model/","title":"Reference"},{"location":"ux/2-1-what-is-ux/","text":"2.1 What is User Experience? Defined broadly, \u201cUser Experience\u201d is about how people experience and interact with an organization\u2019s varied products, systems and services. The focus of the User Experience craft is typically on interactive, digital experiences, but UX practitioners tend to take a broader, more holistic view, looking not only at a given product or service itself but at its users\u2019 expectations and the context in which it is used \u2013 which can include other products, non-digital communications and even people with whom the user communicates while using a product or service. This holistic view yields insights that help us to design digital experiences that fit more seamlessly into users\u2019 lives while helping them achieve more successful and satisfactory outcomes. In many cases, the insights and recommendations uncovered by the UX process can extend to those very same people and non-digital communications that surround an experience as well. 2.1.1 The \u201cThree Circles\u201d of User Experience This holistic focus is conveyed well via the classic \u201cThree Circles\u201d diagram first proposed by Peter Morville, considered one of the \u201cfounding fathers\u201d of the modern UX field: The meaning of each circle in this diagram is as follows: Content : This refers not just to images and copy, but to all the content, features and functions that comprise the experience. It\u2019s important to ensure that all aspects of the experience are included thoughtfully and purposefully to serve the needs of the system\u2019s users. Context : This refers to all of the external factors that a user may consider or encounter while using a given product, system or service. These factors vary greatly depending on the system and can include anything from distractions in the environment, to the device on which the experience is viewed, to other tools and systems that a system\u2019s user might be using simultaneously. Users : This refers to the people who use a given product, system or service. In most cases, the goal is to move beyond a singular, demographic-driven view of the user to establish a more nuanced understanding of distinct user profiles along with the unique needs, mindset, motivations, behaviors and limitations of each one. A deep understanding of the needs and context for each user profile results in more insightful design solutions that align more closely with the way that the system\u2019s users think and work. 2.1.2 Important Qualities of User Experience Usability is often the first aspect that comes to mind when considering the focus of user experience, but there are in actuality a number of other factors that UX practitioners take into account when shaping a product, system or service. This is captured well by another classic model created by Peter Morville: Each hexagon within this Honeycomb diagram is defined as follows: Useful : This refers to the relevance of content and features within a given system to its users. Content and features on a given interface are typically positioned and emphasized based on their level of usefulness or relevance in relation to other content on the screen. Usable : This refers to the ease of use of a given product or service, which includes the clarity of how features are presented as well as how reliably they behave. Desirable : This refers to emotional aspects of a system\u2019s design, which can include not only the sense of satisfaction experienced when a system aligns seamlessly with the users\u2019 needs, but also whether the organizational values reflected in the design resonate with those of the system\u2019s users. Findable : This means ensuring that navigation and content are designed and positioned in ways that are intuitive to end-users, allowing them to quickly find what they need in the places where they might expect to find them. Accessible : This means ensuring that products and services are usable for people with disabilities, which can include anything from hearing or vision impairment to color-blindness. Credible : This means designing so that users can feel that they can trust and rely on the information being presented within an experience, having confidence in the data integrity and system status presented therein. Valuable : This means ensuring that a product or service effectively advances the goals of the organization that sponsors it, whether the nature of those goals is educational, operational or otherwise. UX practitioners can design more effective digital experiences through careful and balanced consideration of these factors. References The Definition of User Experience (UX) https://www.nngroup.com/articles/definition-user-experience User Experience Design http://semanticstudios.com/user_experience_design","title":"2.1 What is UX?"},{"location":"ux/2-1-what-is-ux/#21-what-is-user-experience","text":"Defined broadly, \u201cUser Experience\u201d is about how people experience and interact with an organization\u2019s varied products, systems and services. The focus of the User Experience craft is typically on interactive, digital experiences, but UX practitioners tend to take a broader, more holistic view, looking not only at a given product or service itself but at its users\u2019 expectations and the context in which it is used \u2013 which can include other products, non-digital communications and even people with whom the user communicates while using a product or service. This holistic view yields insights that help us to design digital experiences that fit more seamlessly into users\u2019 lives while helping them achieve more successful and satisfactory outcomes. In many cases, the insights and recommendations uncovered by the UX process can extend to those very same people and non-digital communications that surround an experience as well.","title":"2.1  What is User Experience?"},{"location":"ux/2-1-what-is-ux/#211-the-three-circles-of-user-experience","text":"This holistic focus is conveyed well via the classic \u201cThree Circles\u201d diagram first proposed by Peter Morville, considered one of the \u201cfounding fathers\u201d of the modern UX field: The meaning of each circle in this diagram is as follows: Content : This refers not just to images and copy, but to all the content, features and functions that comprise the experience. It\u2019s important to ensure that all aspects of the experience are included thoughtfully and purposefully to serve the needs of the system\u2019s users. Context : This refers to all of the external factors that a user may consider or encounter while using a given product, system or service. These factors vary greatly depending on the system and can include anything from distractions in the environment, to the device on which the experience is viewed, to other tools and systems that a system\u2019s user might be using simultaneously. Users : This refers to the people who use a given product, system or service. In most cases, the goal is to move beyond a singular, demographic-driven view of the user to establish a more nuanced understanding of distinct user profiles along with the unique needs, mindset, motivations, behaviors and limitations of each one. A deep understanding of the needs and context for each user profile results in more insightful design solutions that align more closely with the way that the system\u2019s users think and work.","title":"2.1.1 The \u201cThree Circles\u201d of User Experience"},{"location":"ux/2-1-what-is-ux/#212-important-qualities-of-user-experience","text":"Usability is often the first aspect that comes to mind when considering the focus of user experience, but there are in actuality a number of other factors that UX practitioners take into account when shaping a product, system or service. This is captured well by another classic model created by Peter Morville: Each hexagon within this Honeycomb diagram is defined as follows: Useful : This refers to the relevance of content and features within a given system to its users. Content and features on a given interface are typically positioned and emphasized based on their level of usefulness or relevance in relation to other content on the screen. Usable : This refers to the ease of use of a given product or service, which includes the clarity of how features are presented as well as how reliably they behave. Desirable : This refers to emotional aspects of a system\u2019s design, which can include not only the sense of satisfaction experienced when a system aligns seamlessly with the users\u2019 needs, but also whether the organizational values reflected in the design resonate with those of the system\u2019s users. Findable : This means ensuring that navigation and content are designed and positioned in ways that are intuitive to end-users, allowing them to quickly find what they need in the places where they might expect to find them. Accessible : This means ensuring that products and services are usable for people with disabilities, which can include anything from hearing or vision impairment to color-blindness. Credible : This means designing so that users can feel that they can trust and rely on the information being presented within an experience, having confidence in the data integrity and system status presented therein. Valuable : This means ensuring that a product or service effectively advances the goals of the organization that sponsors it, whether the nature of those goals is educational, operational or otherwise. UX practitioners can design more effective digital experiences through careful and balanced consideration of these factors.","title":"2.1.2 Important Qualities of User Experience"},{"location":"ux/2-1-what-is-ux/#references","text":"The Definition of User Experience (UX) https://www.nngroup.com/articles/definition-user-experience User Experience Design http://semanticstudios.com/user_experience_design","title":"References"},{"location":"ux/2-2-disciplines/","text":"2.2 UX Disciplines & Project Roles In one sense, all participants in the software development lifecycle play a critical role in ensuring the quality of a digital product\u2019s user experience. Everyone from developers to program managers to visual designers and content creators must take into account how their decisions might affect the experience of the system\u2019s users. But for those who specialize in user experience, the core disciplines that comprise their skillset might be summarized as follows: User Research : A truly user-centered design cannot be delivered without first assembling a sufficient body of research on the system\u2019s target users. This research can consist of quantitative data such as analytics, or qualitative insights gained from observing or interviewing current or potential users. Experience Strategy : Once the users\u2019 needs, expectations and challenges are well understood, those insights must then be synthesized and reconciled with the goals of the organization sponsoring the experience. This means ensuring that the value the organization is looking to provide aligns with the value the system\u2019s users seek. Once the core values driving the experience are established, those can then be used as the foundation for determining what kinds of content and features should be incorporated and how they should be prioritized, then translated into strategically-driven product roadmaps and other plans that lay a clear path to realizing value for both the users of a system and the organization that sponsored its creation. Information Architecture : This discipline entails organizing information within an experience in a way that is meaningful and intuitive to users. Prior to beginning screen designs, UX practitioners spend time considering the structure of the experience and the relationships between different types of content within it. They consider where features and content should fall within that structure in order to meet users\u2019 needs. This typically results in maps of the system\u2019s architecture and key processes within it, and often extends to content strategy documentation that outlines what content can be reused or refined versus what needs to be newly created. Interaction Design : Once user research, experience strategy and information architecture (IA) have been established, UX practitioners can then begin designing screens within the experience to align with established needs and goals. Content and features are incorporated into user interfaces (UI) that align with the target devices through which an experience will be accessed. At this point, the presentation and behavior of all interactive elements such as buttons, controls, transitions and animations are defined and documented in detail to prepare for technical implementation of the system. 2.2.1 Planning User Experience Teams While personnel from all disciplines share some level of responsibility for delivering products that meet user needs, at least one User Experience specialist should be incorporated within each software development team. This ensures that end users have clear and consistent advocacy within the software development lifecycle, uninhibited by conflicting objectives that may be held by developers, visual designers, program managers and others that may be involved in project delivery. The balance of these roles helps to ensure that each one\u2019s objectives become transparent in the planning and execution of the design, yielding better communication and decision-making overall. Specializations do exist for each of the separate core disciplines outlined above, however many User Experience professionals carry a sufficient balance of these skills to effectively manage all of them at once. A strong User Experience practitioner will be skilled in user research methods, navigation structure and process analysis in addition to interaction design. The more senior-level practitioners will be proficient in product strategy and planning, and should be involved by Program Management teams at the earliest stages of a project\u2019s conception to ensure the right research, design and testing approach through all stages of the project. Enlisting strong UX generalists rather than specialized research and design functions is generally more cost-effective, and will help to maintain a cohesive product vision while ensuring a high degree of consistency, adaptability and agility within a dynamic project environment. While User Experience practitioners do establish the foundations for interface design, the craft of creating pixel-perfect page layouts are often managed by a separate Visual Design function who holds a much deeper understanding of factors such as color, typography and creative asset production. Some practitioners may have both skillsets, but considering these as separate functions will increase the likelihood of identifying talent that holds sufficient depth of experience, skill and perspective for each role. Similarly, while User Experience practitioners should be competent in prototyping and understanding the implications of technical factors identified by development teams, true front-end development skills should be considered a separate role within the project team. References What Is User Experience (UX) Design? Everything You Need To Know To Get Started https://careerfoundry.com/en/blog/ux-design/what-is-user-experience-ux-design-everything-you-need-to-know-to-get-started","title":"2.2 UX Disciplines & Project Roles"},{"location":"ux/2-2-disciplines/#22-ux-disciplines-project-roles","text":"In one sense, all participants in the software development lifecycle play a critical role in ensuring the quality of a digital product\u2019s user experience. Everyone from developers to program managers to visual designers and content creators must take into account how their decisions might affect the experience of the system\u2019s users. But for those who specialize in user experience, the core disciplines that comprise their skillset might be summarized as follows: User Research : A truly user-centered design cannot be delivered without first assembling a sufficient body of research on the system\u2019s target users. This research can consist of quantitative data such as analytics, or qualitative insights gained from observing or interviewing current or potential users. Experience Strategy : Once the users\u2019 needs, expectations and challenges are well understood, those insights must then be synthesized and reconciled with the goals of the organization sponsoring the experience. This means ensuring that the value the organization is looking to provide aligns with the value the system\u2019s users seek. Once the core values driving the experience are established, those can then be used as the foundation for determining what kinds of content and features should be incorporated and how they should be prioritized, then translated into strategically-driven product roadmaps and other plans that lay a clear path to realizing value for both the users of a system and the organization that sponsored its creation. Information Architecture : This discipline entails organizing information within an experience in a way that is meaningful and intuitive to users. Prior to beginning screen designs, UX practitioners spend time considering the structure of the experience and the relationships between different types of content within it. They consider where features and content should fall within that structure in order to meet users\u2019 needs. This typically results in maps of the system\u2019s architecture and key processes within it, and often extends to content strategy documentation that outlines what content can be reused or refined versus what needs to be newly created. Interaction Design : Once user research, experience strategy and information architecture (IA) have been established, UX practitioners can then begin designing screens within the experience to align with established needs and goals. Content and features are incorporated into user interfaces (UI) that align with the target devices through which an experience will be accessed. At this point, the presentation and behavior of all interactive elements such as buttons, controls, transitions and animations are defined and documented in detail to prepare for technical implementation of the system.","title":"2.2 UX Disciplines &amp; Project Roles"},{"location":"ux/2-2-disciplines/#221-planning-user-experience-teams","text":"While personnel from all disciplines share some level of responsibility for delivering products that meet user needs, at least one User Experience specialist should be incorporated within each software development team. This ensures that end users have clear and consistent advocacy within the software development lifecycle, uninhibited by conflicting objectives that may be held by developers, visual designers, program managers and others that may be involved in project delivery. The balance of these roles helps to ensure that each one\u2019s objectives become transparent in the planning and execution of the design, yielding better communication and decision-making overall. Specializations do exist for each of the separate core disciplines outlined above, however many User Experience professionals carry a sufficient balance of these skills to effectively manage all of them at once. A strong User Experience practitioner will be skilled in user research methods, navigation structure and process analysis in addition to interaction design. The more senior-level practitioners will be proficient in product strategy and planning, and should be involved by Program Management teams at the earliest stages of a project\u2019s conception to ensure the right research, design and testing approach through all stages of the project. Enlisting strong UX generalists rather than specialized research and design functions is generally more cost-effective, and will help to maintain a cohesive product vision while ensuring a high degree of consistency, adaptability and agility within a dynamic project environment. While User Experience practitioners do establish the foundations for interface design, the craft of creating pixel-perfect page layouts are often managed by a separate Visual Design function who holds a much deeper understanding of factors such as color, typography and creative asset production. Some practitioners may have both skillsets, but considering these as separate functions will increase the likelihood of identifying talent that holds sufficient depth of experience, skill and perspective for each role. Similarly, while User Experience practitioners should be competent in prototyping and understanding the implications of technical factors identified by development teams, true front-end development skills should be considered a separate role within the project team.","title":"2.2.1 Planning User Experience Teams"},{"location":"ux/2-2-disciplines/#references","text":"What Is User Experience (UX) Design? Everything You Need To Know To Get Started https://careerfoundry.com/en/blog/ux-design/what-is-user-experience-ux-design-everything-you-need-to-know-to-get-started","title":"References"},{"location":"ux/2-3-lifecycle/","text":"2.3 UX in Software Lifecycle Methodologies When planning a software development project, the process typically breaks down into four discrete stages, each of which will be described in greater detail within section 3.0. Research & Discovery : This is the stage where initial research activities will be performed and insights will be synthesized based on the research findings. A high-level strategy should be established at this stage that includes experience goals, tactics, plans and content/feature recommendations which can then be used as a foundation for Conceptual Design. Conceptual Design : During this stage, key aspects of the experience will be modeled and discussed in order to establish alignment around the overall approach and design framework for the experience. The conceptual models produced during this stage can describe things like site architecture, processes, navigation patterns and key page layouts to help lay the foundations for an efficient Detailed Design process. A key goal of this stage is to establish clear expectations about how a digital experience will be presented at a high level in order to minimize the need for significant rework in the future. Detailed Design : Once foundations for the experience have been established, the Detailed Design stage then builds on those foundations, designing and specifying detailed elements and interactions for each section and component of the experience. This provides the basis for visual design comps to be created in addition to a clear and detailed plan that developers can follow in order to implement the experience. Support : During this stage, UX practitioners play a supporting role to the visual design and technical development teams who are producing the experience. This can involve addressing questions to clarify intended interactive behaviors, revising designs to resolve technical implementation challenges, and assisting with the testing and delivery of the implemented experience. These stages can sometimes be named or broken down differently, but in essence outline the stages of a typical user experience process. However it is important to note that some of these stages may not be required at times depending on the nature and goals of the project. For example, if a project entails targeted refinements and enhancements to an existing experience, full Research/Discovery and Conceptual Design stages may not be needed; however in this scenario, some level of user input should be collected and reviewed in order to determine what refinements should be made. Similarly, a Conceptual Design stage can in some case proceed without a Research/Discovery stage, assuming that existing research sufficiently addresses key questions relevant to the design problem at hand. The approach to planning and managing each of these stages can vary considerably depending on the software development process leveraged by the project team as a whole, described in further detail below. 2.3.1 Waterfall Process Considerations In a \u201cwaterfall\u201d process, each of these stages happens in sequential order, with the output of each stage being typically reviewed, finalized and approved in its entirety before proceeding to the next stage. In this type of process methodology, the typical expectation is that all documentation is comprehensive and exhaustive, addressing all possible questions and scenarios prior to any development being performed. Representative members of each project team can be incorporated into each stage to ensure that proper communication occurs and relevant questions are identified early in the process, but often this is not the case in a classic waterfall process. Instead, each project team often works independently from each other, with the resulting documentation being tossed \u201cover the wall\u201d so to speak to other project teams in order to guide their efforts for the next process stage. The main advantage of this approach is the upfront clarity and stability of plans and specifications prior to the beginning of technical implementation. However there are several disadvantages to the waterfall approach: Reduced communication and collaboration between project teams Heavy and often unwieldy design documentation Longer overall project timelines Working software not available for evaluation until late in the project lifecycle Inflexibility to changing requirements and circumstances surrounding the project Cross-team communication challenges can sometimes be circumvented in a waterfall process through adopting daily \u201cstand-up\u201d or \u201cscrum\u201d meeting that are typical in an Agile process; however daily scrum meetings alone do not make a project truly \u201cAgile, and leave many of the other disadvantages of the Waterfall methodology unaddressed. 2.3.2 Agile Process Considerations The Agile methodology was devised in order to address many of these disadvantages of waterfall processes. However the classic Agile methodology focuses more so on technical development aspects and less so on user experience or design aspects of the software development lifecycle. In order for user experience and design teams to work efficiently and effectively within the process, some additional process considerations must be made. The core Agile values are what remain consistent regardless of project role or the type of Agile process being employed. These core values are: Individuals and interactions over processes and tools Working software over comprehensive documentation Customer collaboration over contract negotiation Responding to change over following a plan Putting these values into context, things like plans, processes, tools and documentation are still relied upon heavily in an Agile environment, however all of these are secondary to effective communication, collaboration and adaptability among members of the project team. As such, there are many crucial \u201cceremonies\u201d that also remain consistent within an Agile process, playing a key role in ensuring that the values and benefits of the Agile methodology are realized. A daily \u201cscrum\u201d or \u201cstandup\u201d meeting is one type of Agile ceremony that ensures that team members across disciplines have a daily view of what features are in focus today and what key challenges might impact their completion. Teams typically work from a \u201cproduct backlog\u201d \u2013 a prioritized list of all features and capabilities that will ultimately be part of the final experience \u2013 and break that product backlog down into \u201csprints\u201d, typically 2 weeks in duration, where a portion of the items from the backlog are expected to be complete. As changes occur or new requirements are realized, this product backlog is continually \u201cgroomed\u201d to ensure that the project plan is optimized for the best outcome, and planning sessions at the start of each new sprint offer an opportunity to put the adjusted plan into effect. See the Agile Playbook for further details on typical Agile ceremonies, tools and methods. In addition to values and ceremonies, the Agile management framework is another aspect of the methodology that remains relatively unchanged when design teams are incorporated, however slight amendments must be made to define a clear role for User Experience. The Agile management framework identifies two key roles \u2013 the Scrum Master and the Product Owner \u2013 where the Scrum Master\u2019s primary role is to keep the project team focused on its goals for the sprint, and the Product Owner\u2019s primary role is to keep the team\u2019s work aligned with the overall product vision. While the Agile methodology does not directly prescribe a role for User Experience teams, their role is best considered as an extension to the Product Owner role. This means helping to establish the product context during the Research/Discovery stage, summarizing core user needs and business requirements. This also entails modeling the product vision during the Conceptual Design stage, creating preliminary wireframes and user stories that describe the expected user needs and behaviors that each feature of the experience will be designed to support. Lastly, this means leading the creation, prioritization and ongoing \u201cgrooming\u201d of the product backlog, ensuring that the most valuable features are delivered first and meet the expectations of the users for whom they are designed. As a project moves into detailed design, the task of backlog grooming is often shared with members of the Project Management team, with members of the UX team offering significant guidance while focusing mainly on the creation of design specifications for each item in the backlog. The aspect in which the classic Agile methodology is least prescriptive for User Experience teams is its guidance around planning each of the process stages and determining what user experience practices to incorporate within them. Many of the Agile ceremonies outlined above are more relevant to the Detailed Design and Support stages than they are to the initial Research/Discovery and Conceptual Design stages. In Agile terms, these first two stages are often combined and named \u201cSprint Zero\u201d, continuing to resemble a waterfall process in many ways. This allows for solid research foundations and a clear product vision to be established in advance of working iteratively on detailed design specifications. Cross-team input is valuable at any stage, and daily scrums may be incorporated to facilitate this, but beginning front-end development before a clear product vision is set can result in lost development cycles and a significant amount of rework. Design teams should focus first on establishing a clear product architecture and basic alignment around how features might be translated into layouts before the team begins iterations of detailed design and prototyping. Once a product backlog is created at the end of Conceptual Design, the project team is prepared to adopt all other typical Agile ceremonies for the remainder of the project. Detailed Design and Support are the stages where the Agile methodology is utilized in its truest form. In an ideal scenario, all project teams are focused on the same set of features at once, working in unison to arrive at a functioning prototype of a specific feature. User Experience teams leads their counterparts through the iterative process for each feature: Understanding and clarifying requirements Defining key criteria for the design solution based user needs, business goals and technology capabilities Exploring different design solutions that meet the requirements and criteria Deciding which solution to pursue based on input from across team functions Prototyping the solution Testing, validating and potentially refining the solution This constant communication and direct translation of designs into working code can significantly reduce the amount documentation created or needed within a project, a documentation approach often referred to as \u201cLean\u201d wherein design specifications are only as exhaustive as necessitated by the project team. In an ideal Agile scenario where all teams can focus on the same set of features at once, the cycle described above can take anywhere from 2-5 days to complete, assuming that cross-disciplinary teams are well-integrated and that features in the product backlog are broken into small, manageable chunks that are achievable within that time frame. By the end of the sprint, the expectation is that all backlog items that have been initiated are completed, reviewed and approved, however selective items can move into later sprints if necessary. 2.3.3 UX Process at Different Stages of the \u201cAgility Spectrum\u201d However some organizations may have difficulty achieving this level of integration, and even then, readiness for these methods may vary between different teams or projects within an organization, falling at different points of the \u201cagility spectrum\u201d. The approach to planning the Detailed Design stage may vary depending on the degree of \u201cagility\u201d at which any given team or project is ready to operate. Limiting factors to project agility often go beyond the team or organization\u2019s level of training and experience, and can extend to project-specific factors such as resource availability, vendor involvement, project budget and ability to co-locate. In these scenarios, there are a number of ways that a project approach can be adjusted to ensure successful collaboration and delivery. One approach is to rely on a User Experience team\u2019s competency in creating clickable prototypes using software such as InVision, Sketch or Axure. This reduces reliance on the development team to arrive at a working model and significantly simplifies the level of effort required to produce it. Development teams still need to stay closely involved in its creation and will certainly have additional tasks to perform such as data sourcing and more complex interactivity, but creating clickable prototypes in software allows project teams to effectively validate the usability and flow of a given feature while enabling development teams to work independently and in parallel on those additional tasks. This also allows the user experience team to work at a faster velocity on a range of features while development teams work at their own pace determining the optimal implementation approach. The key here is ensuring that both teams have a portion of their velocity (typically 10-20% of available bandwidth) dedicated to addressing questions and collaborating across teams on different pieces of work. The length of each sprint is another variable to consider. While a 2-week sprint cycle is considered standard, lengthening that cycle to 3 or even 4 weeks can help ensure sprint objectives are successfully achieved. In the scenario described above where project teams spend some portion of their time working independently and in parallel, a longer sprint cycle can ensure that all teams have sufficient time to reconvene, discuss their findings and make adjustments so that all sprint output is delivered on time, with exhaustive validation and alignment across stakeholders, further reducing the potential need for rework throughout the project timeline. Another approach is to anticipate \u201cstaggered sprints\u201d from the outset. This means that the user experience team always working one full sprint ahead of the development team. To do this, the notion must be applied here of having each team\u2019s velocity account for a portion of time dedicate to collaborating across teams on different pieces of work. While it would be expected that the same staggered sprint duration would be maintained throughout the project lifecycle, this approach does allow one team to stay on track with delivery in the event that another team faces challenges with maintaining that pace. The need for additional time to work through complex issues is a common challenge for development teams, but can be a challenge for user experience teams as well. If this type of challenge is foreseen during the project\u2019s planning activities, something called a \u201cdesign spike\u201d can be instituted to alleviate this issue. A design spike is an activity that can take place of a regular sprint to resolve big unknowns and work through complexities that would otherwise impact the efficiency of the project team. During this time, development teams are typically assigned non-dependent work while the user experience team gathers additional requirements and potential explore concepts for specific feature areas within the product. While these practices may not be considered Agile in its truest form, they may become essential in managing a team successfully through limiting factors to agility such as resource availability, vendor involvement, project budget and ability to co-locate. These practices might also be considered stepping stones to more advanced and integrated Agile practices, enabling project teams to abide by the value and reap the benefits of the Agile methodology while ensuring project plans and output are optimized for success. 2.3.4 Practical Considerations When planning a project or estimating work that involves a User Experience component within an Agile setting, there are a few key guidelines or common practices to consider. One common practice is aligning team size and levels across disciplines. A general rule of thumb is to match every user experience designer with a visual designer and a front-end developer, also taking the levels of seniority and team hierarchy into account. This helps to ensure that the output of one team aligns with the production capacity of other teams on the project. This may require adjustment based on project-specific factors, but is often a good place to start. This rule of thumb does not typically apply to project management or back-end technology functions, who each have unique considerations to take into account when estimating team size. Another common practice is ensuring that separate user stories are written for user experience versus technology teams. There may be significant overlap or even replication of stories between teams but this is not always the case. For one thing, development teams have different considerations to make when estimating the level of effort associated with a user story, which can result in the development effort associated with a story being much larger than the design effort. In these cases, user stories may be decomposed into smaller, more manageable stories from a development perspective. Likewise, development teams often approach work differently from an efficiency standpoint, resulting in a different view of how user stories should be structured. For example, user experience stories are typically focused on the front-end experience, but many features may have a significant back-end component in common, resulting in greater development efficiency if those stories are in some way grouped together. The vast majority of user stories will be shared across both teams, but there are frequently cases where differentiating a handful of these stories will result in a more efficient development process. One final consideration is around estimating the \u201csize\u201d or level of effort associated with a user story. While any design problem may have a number of viable solutions, the Conceptual Design stage helps narrow down the range of possible solutions which in turn makes estimation easier. On this basis, the User Experience team should first discuss what successful completion of each user story might entail, then determine how complex and time-consuming the delivery of that solution might be. To approximate and track this, each item in the product backlog is often assigned a number from the Fibonacci sequence (e.g. 1, 2, 3, 5, 8, 13, 21, etc.) which represents the complexity of the design problem in increasing orders of magnitude. These \u201cstory points\u201d are often used to track a team\u2019s velocity, with a baseline established in the first sprint as to how many story points the User Experience team can successfully deliver. Particularly large values are usually an indication that a story needs to be decomposed into smaller, more manageable stories, particularly when those values are higher than the baseline velocity that an individual team member can deliver in one sprint. These story point values can be taken into account when planning what items to take into a sprint, and can be adjusted at the start of each sprint as new requirements or complexities are learned. Further details about each stage of the User Experience process, including the key methods and activities conducted at each stage, will be outlined in the UX Principles & Methodologies section of this playbook. References Agile Manifesto https://agilemanifesto.org Scrum https://www.atlassian.com/agile/scrum Design Sprint Kit https://designsprintkit.withgoogle.com/introduction/overview Benefits of Partnering Closely with Your Product Owner https://www.uxmatters.com/mt/archives/2018/08/benefits-of-partnering-closely-with-your-product-owner.php A Spectrum of Approaches to Project Agility https://www.pmi.org/-/media/pmi/documents/public/pdf/learning/thought-leadership/achieving-greater-agility-series/spectrum-approaches.pdf Spikes https://www.scaledagileframework.com/spikes Fitting Big-Picture UX Into Agile Development https://www.smashingmagazine.com/2012/11/design-spikes-fit-big-picture-ux-agile-development","title":"2.3 UX in Software Lifecycle Methodologies"},{"location":"ux/2-3-lifecycle/#23-ux-in-software-lifecycle-methodologies","text":"When planning a software development project, the process typically breaks down into four discrete stages, each of which will be described in greater detail within section 3.0. Research & Discovery : This is the stage where initial research activities will be performed and insights will be synthesized based on the research findings. A high-level strategy should be established at this stage that includes experience goals, tactics, plans and content/feature recommendations which can then be used as a foundation for Conceptual Design. Conceptual Design : During this stage, key aspects of the experience will be modeled and discussed in order to establish alignment around the overall approach and design framework for the experience. The conceptual models produced during this stage can describe things like site architecture, processes, navigation patterns and key page layouts to help lay the foundations for an efficient Detailed Design process. A key goal of this stage is to establish clear expectations about how a digital experience will be presented at a high level in order to minimize the need for significant rework in the future. Detailed Design : Once foundations for the experience have been established, the Detailed Design stage then builds on those foundations, designing and specifying detailed elements and interactions for each section and component of the experience. This provides the basis for visual design comps to be created in addition to a clear and detailed plan that developers can follow in order to implement the experience. Support : During this stage, UX practitioners play a supporting role to the visual design and technical development teams who are producing the experience. This can involve addressing questions to clarify intended interactive behaviors, revising designs to resolve technical implementation challenges, and assisting with the testing and delivery of the implemented experience. These stages can sometimes be named or broken down differently, but in essence outline the stages of a typical user experience process. However it is important to note that some of these stages may not be required at times depending on the nature and goals of the project. For example, if a project entails targeted refinements and enhancements to an existing experience, full Research/Discovery and Conceptual Design stages may not be needed; however in this scenario, some level of user input should be collected and reviewed in order to determine what refinements should be made. Similarly, a Conceptual Design stage can in some case proceed without a Research/Discovery stage, assuming that existing research sufficiently addresses key questions relevant to the design problem at hand. The approach to planning and managing each of these stages can vary considerably depending on the software development process leveraged by the project team as a whole, described in further detail below.","title":"2.3 UX in Software Lifecycle Methodologies"},{"location":"ux/2-3-lifecycle/#231-waterfall-process-considerations","text":"In a \u201cwaterfall\u201d process, each of these stages happens in sequential order, with the output of each stage being typically reviewed, finalized and approved in its entirety before proceeding to the next stage. In this type of process methodology, the typical expectation is that all documentation is comprehensive and exhaustive, addressing all possible questions and scenarios prior to any development being performed. Representative members of each project team can be incorporated into each stage to ensure that proper communication occurs and relevant questions are identified early in the process, but often this is not the case in a classic waterfall process. Instead, each project team often works independently from each other, with the resulting documentation being tossed \u201cover the wall\u201d so to speak to other project teams in order to guide their efforts for the next process stage. The main advantage of this approach is the upfront clarity and stability of plans and specifications prior to the beginning of technical implementation. However there are several disadvantages to the waterfall approach: Reduced communication and collaboration between project teams Heavy and often unwieldy design documentation Longer overall project timelines Working software not available for evaluation until late in the project lifecycle Inflexibility to changing requirements and circumstances surrounding the project Cross-team communication challenges can sometimes be circumvented in a waterfall process through adopting daily \u201cstand-up\u201d or \u201cscrum\u201d meeting that are typical in an Agile process; however daily scrum meetings alone do not make a project truly \u201cAgile, and leave many of the other disadvantages of the Waterfall methodology unaddressed.","title":"2.3.1 Waterfall Process Considerations"},{"location":"ux/2-3-lifecycle/#232-agile-process-considerations","text":"The Agile methodology was devised in order to address many of these disadvantages of waterfall processes. However the classic Agile methodology focuses more so on technical development aspects and less so on user experience or design aspects of the software development lifecycle. In order for user experience and design teams to work efficiently and effectively within the process, some additional process considerations must be made. The core Agile values are what remain consistent regardless of project role or the type of Agile process being employed. These core values are: Individuals and interactions over processes and tools Working software over comprehensive documentation Customer collaboration over contract negotiation Responding to change over following a plan Putting these values into context, things like plans, processes, tools and documentation are still relied upon heavily in an Agile environment, however all of these are secondary to effective communication, collaboration and adaptability among members of the project team. As such, there are many crucial \u201cceremonies\u201d that also remain consistent within an Agile process, playing a key role in ensuring that the values and benefits of the Agile methodology are realized. A daily \u201cscrum\u201d or \u201cstandup\u201d meeting is one type of Agile ceremony that ensures that team members across disciplines have a daily view of what features are in focus today and what key challenges might impact their completion. Teams typically work from a \u201cproduct backlog\u201d \u2013 a prioritized list of all features and capabilities that will ultimately be part of the final experience \u2013 and break that product backlog down into \u201csprints\u201d, typically 2 weeks in duration, where a portion of the items from the backlog are expected to be complete. As changes occur or new requirements are realized, this product backlog is continually \u201cgroomed\u201d to ensure that the project plan is optimized for the best outcome, and planning sessions at the start of each new sprint offer an opportunity to put the adjusted plan into effect. See the Agile Playbook for further details on typical Agile ceremonies, tools and methods. In addition to values and ceremonies, the Agile management framework is another aspect of the methodology that remains relatively unchanged when design teams are incorporated, however slight amendments must be made to define a clear role for User Experience. The Agile management framework identifies two key roles \u2013 the Scrum Master and the Product Owner \u2013 where the Scrum Master\u2019s primary role is to keep the project team focused on its goals for the sprint, and the Product Owner\u2019s primary role is to keep the team\u2019s work aligned with the overall product vision. While the Agile methodology does not directly prescribe a role for User Experience teams, their role is best considered as an extension to the Product Owner role. This means helping to establish the product context during the Research/Discovery stage, summarizing core user needs and business requirements. This also entails modeling the product vision during the Conceptual Design stage, creating preliminary wireframes and user stories that describe the expected user needs and behaviors that each feature of the experience will be designed to support. Lastly, this means leading the creation, prioritization and ongoing \u201cgrooming\u201d of the product backlog, ensuring that the most valuable features are delivered first and meet the expectations of the users for whom they are designed. As a project moves into detailed design, the task of backlog grooming is often shared with members of the Project Management team, with members of the UX team offering significant guidance while focusing mainly on the creation of design specifications for each item in the backlog. The aspect in which the classic Agile methodology is least prescriptive for User Experience teams is its guidance around planning each of the process stages and determining what user experience practices to incorporate within them. Many of the Agile ceremonies outlined above are more relevant to the Detailed Design and Support stages than they are to the initial Research/Discovery and Conceptual Design stages. In Agile terms, these first two stages are often combined and named \u201cSprint Zero\u201d, continuing to resemble a waterfall process in many ways. This allows for solid research foundations and a clear product vision to be established in advance of working iteratively on detailed design specifications. Cross-team input is valuable at any stage, and daily scrums may be incorporated to facilitate this, but beginning front-end development before a clear product vision is set can result in lost development cycles and a significant amount of rework. Design teams should focus first on establishing a clear product architecture and basic alignment around how features might be translated into layouts before the team begins iterations of detailed design and prototyping. Once a product backlog is created at the end of Conceptual Design, the project team is prepared to adopt all other typical Agile ceremonies for the remainder of the project. Detailed Design and Support are the stages where the Agile methodology is utilized in its truest form. In an ideal scenario, all project teams are focused on the same set of features at once, working in unison to arrive at a functioning prototype of a specific feature. User Experience teams leads their counterparts through the iterative process for each feature: Understanding and clarifying requirements Defining key criteria for the design solution based user needs, business goals and technology capabilities Exploring different design solutions that meet the requirements and criteria Deciding which solution to pursue based on input from across team functions Prototyping the solution Testing, validating and potentially refining the solution This constant communication and direct translation of designs into working code can significantly reduce the amount documentation created or needed within a project, a documentation approach often referred to as \u201cLean\u201d wherein design specifications are only as exhaustive as necessitated by the project team. In an ideal Agile scenario where all teams can focus on the same set of features at once, the cycle described above can take anywhere from 2-5 days to complete, assuming that cross-disciplinary teams are well-integrated and that features in the product backlog are broken into small, manageable chunks that are achievable within that time frame. By the end of the sprint, the expectation is that all backlog items that have been initiated are completed, reviewed and approved, however selective items can move into later sprints if necessary.","title":"2.3.2 Agile Process Considerations"},{"location":"ux/2-3-lifecycle/#233-ux-process-at-different-stages-of-the-agility-spectrum","text":"However some organizations may have difficulty achieving this level of integration, and even then, readiness for these methods may vary between different teams or projects within an organization, falling at different points of the \u201cagility spectrum\u201d. The approach to planning the Detailed Design stage may vary depending on the degree of \u201cagility\u201d at which any given team or project is ready to operate. Limiting factors to project agility often go beyond the team or organization\u2019s level of training and experience, and can extend to project-specific factors such as resource availability, vendor involvement, project budget and ability to co-locate. In these scenarios, there are a number of ways that a project approach can be adjusted to ensure successful collaboration and delivery. One approach is to rely on a User Experience team\u2019s competency in creating clickable prototypes using software such as InVision, Sketch or Axure. This reduces reliance on the development team to arrive at a working model and significantly simplifies the level of effort required to produce it. Development teams still need to stay closely involved in its creation and will certainly have additional tasks to perform such as data sourcing and more complex interactivity, but creating clickable prototypes in software allows project teams to effectively validate the usability and flow of a given feature while enabling development teams to work independently and in parallel on those additional tasks. This also allows the user experience team to work at a faster velocity on a range of features while development teams work at their own pace determining the optimal implementation approach. The key here is ensuring that both teams have a portion of their velocity (typically 10-20% of available bandwidth) dedicated to addressing questions and collaborating across teams on different pieces of work. The length of each sprint is another variable to consider. While a 2-week sprint cycle is considered standard, lengthening that cycle to 3 or even 4 weeks can help ensure sprint objectives are successfully achieved. In the scenario described above where project teams spend some portion of their time working independently and in parallel, a longer sprint cycle can ensure that all teams have sufficient time to reconvene, discuss their findings and make adjustments so that all sprint output is delivered on time, with exhaustive validation and alignment across stakeholders, further reducing the potential need for rework throughout the project timeline. Another approach is to anticipate \u201cstaggered sprints\u201d from the outset. This means that the user experience team always working one full sprint ahead of the development team. To do this, the notion must be applied here of having each team\u2019s velocity account for a portion of time dedicate to collaborating across teams on different pieces of work. While it would be expected that the same staggered sprint duration would be maintained throughout the project lifecycle, this approach does allow one team to stay on track with delivery in the event that another team faces challenges with maintaining that pace. The need for additional time to work through complex issues is a common challenge for development teams, but can be a challenge for user experience teams as well. If this type of challenge is foreseen during the project\u2019s planning activities, something called a \u201cdesign spike\u201d can be instituted to alleviate this issue. A design spike is an activity that can take place of a regular sprint to resolve big unknowns and work through complexities that would otherwise impact the efficiency of the project team. During this time, development teams are typically assigned non-dependent work while the user experience team gathers additional requirements and potential explore concepts for specific feature areas within the product. While these practices may not be considered Agile in its truest form, they may become essential in managing a team successfully through limiting factors to agility such as resource availability, vendor involvement, project budget and ability to co-locate. These practices might also be considered stepping stones to more advanced and integrated Agile practices, enabling project teams to abide by the value and reap the benefits of the Agile methodology while ensuring project plans and output are optimized for success.","title":"2.3.3 UX Process at Different Stages of the \u201cAgility Spectrum\u201d"},{"location":"ux/2-3-lifecycle/#234-practical-considerations","text":"When planning a project or estimating work that involves a User Experience component within an Agile setting, there are a few key guidelines or common practices to consider. One common practice is aligning team size and levels across disciplines. A general rule of thumb is to match every user experience designer with a visual designer and a front-end developer, also taking the levels of seniority and team hierarchy into account. This helps to ensure that the output of one team aligns with the production capacity of other teams on the project. This may require adjustment based on project-specific factors, but is often a good place to start. This rule of thumb does not typically apply to project management or back-end technology functions, who each have unique considerations to take into account when estimating team size. Another common practice is ensuring that separate user stories are written for user experience versus technology teams. There may be significant overlap or even replication of stories between teams but this is not always the case. For one thing, development teams have different considerations to make when estimating the level of effort associated with a user story, which can result in the development effort associated with a story being much larger than the design effort. In these cases, user stories may be decomposed into smaller, more manageable stories from a development perspective. Likewise, development teams often approach work differently from an efficiency standpoint, resulting in a different view of how user stories should be structured. For example, user experience stories are typically focused on the front-end experience, but many features may have a significant back-end component in common, resulting in greater development efficiency if those stories are in some way grouped together. The vast majority of user stories will be shared across both teams, but there are frequently cases where differentiating a handful of these stories will result in a more efficient development process. One final consideration is around estimating the \u201csize\u201d or level of effort associated with a user story. While any design problem may have a number of viable solutions, the Conceptual Design stage helps narrow down the range of possible solutions which in turn makes estimation easier. On this basis, the User Experience team should first discuss what successful completion of each user story might entail, then determine how complex and time-consuming the delivery of that solution might be. To approximate and track this, each item in the product backlog is often assigned a number from the Fibonacci sequence (e.g. 1, 2, 3, 5, 8, 13, 21, etc.) which represents the complexity of the design problem in increasing orders of magnitude. These \u201cstory points\u201d are often used to track a team\u2019s velocity, with a baseline established in the first sprint as to how many story points the User Experience team can successfully deliver. Particularly large values are usually an indication that a story needs to be decomposed into smaller, more manageable stories, particularly when those values are higher than the baseline velocity that an individual team member can deliver in one sprint. These story point values can be taken into account when planning what items to take into a sprint, and can be adjusted at the start of each sprint as new requirements or complexities are learned. Further details about each stage of the User Experience process, including the key methods and activities conducted at each stage, will be outlined in the UX Principles & Methodologies section of this playbook.","title":"2.3.4 Practical Considerations"},{"location":"ux/2-3-lifecycle/#references","text":"Agile Manifesto https://agilemanifesto.org Scrum https://www.atlassian.com/agile/scrum Design Sprint Kit https://designsprintkit.withgoogle.com/introduction/overview Benefits of Partnering Closely with Your Product Owner https://www.uxmatters.com/mt/archives/2018/08/benefits-of-partnering-closely-with-your-product-owner.php A Spectrum of Approaches to Project Agility https://www.pmi.org/-/media/pmi/documents/public/pdf/learning/thought-leadership/achieving-greater-agility-series/spectrum-approaches.pdf Spikes https://www.scaledagileframework.com/spikes Fitting Big-Picture UX Into Agile Development https://www.smashingmagazine.com/2012/11/design-spikes-fit-big-picture-ux-agile-development","title":"References"},{"location":"ux/3-1-principles/","text":"3.1 UX Principles Establishing anchoring user experience principles allows us a mechanism to instill consistency across the breadth of USAF Logistics Information Systems\u2019 portfolio. These principles represent goals at a high, abstract level of the user\u2019s experience, but should be reflected in the design and execution of specific capabilities wherever applicable. In this way, we can know that even where specific interactions and interface components will differ per the respective requirements of a given system, as long as they are successfully fulfilling the same larger UX principle, an overarching consistency of purpose, experience, and mission support is being met. Referring to these principles consistently over time offers decision-making guidance, from strategic efforts (e.g. prioritizing features) to tactical execution (e.g. determining specific interactions and system flows). To best serve the logistics community and its missions, systems should adhere to the following eight interrelated UX principles: 3.1.1 Enable Clear and Immediate Decision Support Paramount to a system\u2019s ability to support its mission is highly effective user enablement, especially for logistics information systems in the area of decision-making. To that end, a system\u2019s design should first and foremost support this critical function. Proper enablement includes: Intuitive and easily repeatable task flows for arriving at decision-supporting information. Formatting and presenting decision-supporting information in a way that facilitates subsequent steps (either within or outside of the system). Providing appropriate descriptions, context, and caveats with data to better inform decision-making. Favoring recognition over recall by the user. The user can select from clearly presented options instead of recalling codes or meanings. Options are reduced to only those that are applicable in context. Plain-text descriptions are offered wherever data is coded. All of this requires that users and their key tasks and decisions are understood deeply by the system owners and designers. 3.1.2 Promote Data Integrity and Accuracy Data lies at the heart of logistics information systems and processes. Faithful representation and management of this data, then, is crucial to mission success for their users. For users to have confidence in a system\u2019s data, they must understand how the system works. Further, a policy of data transparency will improve the data\u2019s accuracy over time and build trust in its systems. Enterprise-wide data integrity and accuracy can be ensured by: Never obscuring or correcting problematic source data. Instead, at a minimum, intend to notify the user of possible issues and, optimally, offer means to resolving issues. Indicating from where data is sourced and, where relevant, the business logic applied by the system to create the displayed data. Minimizing the possibility of human error. Use structured inputs wherever possible, reduce options to only those that are allowable by business rules, and offer rich type-ahead suggestions where a user needs more open-ended data entry or query. This principle requires system owners and designers to have thorough expertise in relevant technical orders and their system\u2019s contributing source systems. 3.1.3 Streamline Common Tasks and Analyses Making common tasks and analyses more efficient for the end user allows the system to maximize its value to the user and to the mission overall. The system\u2019s design should favor its users\u2019 most frequent needs by streamlining its flows and interfaces toward those ends. Some means toward that goal include: Emphasizing most commonly used functions. Allowing users to individually save settings, custom searches, and tool preferences. Providing smart defaults wherever possible. This ranges from pre-selecting the most common form field options to displaying the most commonly used features or sections upon initial login. Successfully following this principle requires comprehensively researching your users\u2019 intents and monitoring system usage. 3.1.4 Tailor Tasks and Interfaces to Roles Too often, DoD systems err by attempting to do everything for everyone, and by doing so, ultimately impede the mission. Offering users the right content and functionality for their needs and assignments is a meaningful way to support the above UX principles, but also to aid mission attainment. Ways to tailor the experience include: Emphasizing or exposing interfaces or portions of interfaces, especially for data entry and transactional functions, to only those user roles with a need and authority for said functionality. Designing task flows and interfaces so that they at a minimum support, but optimally increase the efficiency of, the larger assignment beyond the system for key end-user groups. Reducing data inputs, and options within those inputs, to only those that are meaningful and relevant to the user\u2019s role and their current context. 3.1.5 Provide Clear System Status Clear, consistent status indication and feedback is a certain measure for instilling trust in users and providing a sense of confidence and control within the system. With this sense of control, users do not have to split their attention between managing the system and contextualizing its responses and outputs, and can instead focus entirely on quickly and accurately accomplishing their end goal. A system should clearly report its status by: Indicating the recency and providing a detailed status of its source systems. Providing up-to-date context for the content and data presented by the system. Utilizing consistent enterprise metaphors for indicating status, such as stoplight coloring, at both the system and individual data record levels. Telling the user when it is busy processing their request, and, if the response is expected to take longer than a few seconds, offering an indication of how much longer the response will take or what percent of the process is complete. 3.1.6 Prioritize Alerts and Exceptions As the nature of many tasks within the logistics community includes attending to conspicuous, outlying data and \u201cgreening up\u201d problem areas, many users can greatly benefit by having these situations brought to their attention systematically, instead of by someone up or down the chain of command, or by hunting through data themselves. Where appropriate then, systems should prioritize alerts and exceptions to support this type of task by: Alerting users where problems may exist in the source data or in calculations made by the system with uncertain source data. Highlighting metrics that are below target goals, limits, or enterprise standards. Offering deeper assessments with explanations, breakdowns, or drivers of metrics and calculations. 3.1.7 Ensure Fast Findability Due to the inherent urgency of some mission support tasks, systems should be designed to allow users to quickly find information. Some means to assist this include: Providing smart defaults on interface configurations and search parameters. Tailoring data reports to include unrequested but meaningful supporting data alongside the user\u2019s requested information, sparing the user a second query or deeper analysis. Allowing users to individually save settings, custom searches, and tool preferences. Adhering to visual design best practices for clear presentation of content and data. 3.1.8 Reduce Reliance on Externalities Fragmented experiences are too often the norm, whether it be requiring users to jump between systems, or referencing paper documents or print materials. Pulling as much information and as many sub-tasks as makes sense into a user\u2019s primary system of need can consolidate work and reduce fragmentation. The end result is an allowance for the user to give more attention to their actual end goals and less on managing tools, thereby supporting the above principles of faster, more streamlined decision-making. Providing plain-text descriptions of acronyms and codes. Optimizing system task flows to seamlessly fit into larger external processes. Digitizing paper processes, especially where immediate transcription back into a digital system is expected.","title":"3.1 UX Principles"},{"location":"ux/3-1-principles/#31-ux-principles","text":"Establishing anchoring user experience principles allows us a mechanism to instill consistency across the breadth of USAF Logistics Information Systems\u2019 portfolio. These principles represent goals at a high, abstract level of the user\u2019s experience, but should be reflected in the design and execution of specific capabilities wherever applicable. In this way, we can know that even where specific interactions and interface components will differ per the respective requirements of a given system, as long as they are successfully fulfilling the same larger UX principle, an overarching consistency of purpose, experience, and mission support is being met. Referring to these principles consistently over time offers decision-making guidance, from strategic efforts (e.g. prioritizing features) to tactical execution (e.g. determining specific interactions and system flows). To best serve the logistics community and its missions, systems should adhere to the following eight interrelated UX principles:","title":"3.1 UX Principles"},{"location":"ux/3-1-principles/#311-enable-clear-and-immediate-decision-support","text":"Paramount to a system\u2019s ability to support its mission is highly effective user enablement, especially for logistics information systems in the area of decision-making. To that end, a system\u2019s design should first and foremost support this critical function. Proper enablement includes: Intuitive and easily repeatable task flows for arriving at decision-supporting information. Formatting and presenting decision-supporting information in a way that facilitates subsequent steps (either within or outside of the system). Providing appropriate descriptions, context, and caveats with data to better inform decision-making. Favoring recognition over recall by the user. The user can select from clearly presented options instead of recalling codes or meanings. Options are reduced to only those that are applicable in context. Plain-text descriptions are offered wherever data is coded. All of this requires that users and their key tasks and decisions are understood deeply by the system owners and designers.","title":"3.1.1 Enable Clear and Immediate Decision Support"},{"location":"ux/3-1-principles/#312-promote-data-integrity-and-accuracy","text":"Data lies at the heart of logistics information systems and processes. Faithful representation and management of this data, then, is crucial to mission success for their users. For users to have confidence in a system\u2019s data, they must understand how the system works. Further, a policy of data transparency will improve the data\u2019s accuracy over time and build trust in its systems. Enterprise-wide data integrity and accuracy can be ensured by: Never obscuring or correcting problematic source data. Instead, at a minimum, intend to notify the user of possible issues and, optimally, offer means to resolving issues. Indicating from where data is sourced and, where relevant, the business logic applied by the system to create the displayed data. Minimizing the possibility of human error. Use structured inputs wherever possible, reduce options to only those that are allowable by business rules, and offer rich type-ahead suggestions where a user needs more open-ended data entry or query. This principle requires system owners and designers to have thorough expertise in relevant technical orders and their system\u2019s contributing source systems.","title":"3.1.2 Promote Data Integrity and Accuracy"},{"location":"ux/3-1-principles/#313-streamline-common-tasks-and-analyses","text":"Making common tasks and analyses more efficient for the end user allows the system to maximize its value to the user and to the mission overall. The system\u2019s design should favor its users\u2019 most frequent needs by streamlining its flows and interfaces toward those ends. Some means toward that goal include: Emphasizing most commonly used functions. Allowing users to individually save settings, custom searches, and tool preferences. Providing smart defaults wherever possible. This ranges from pre-selecting the most common form field options to displaying the most commonly used features or sections upon initial login. Successfully following this principle requires comprehensively researching your users\u2019 intents and monitoring system usage.","title":"3.1.3 Streamline Common Tasks and Analyses"},{"location":"ux/3-1-principles/#314-tailor-tasks-and-interfaces-to-roles","text":"Too often, DoD systems err by attempting to do everything for everyone, and by doing so, ultimately impede the mission. Offering users the right content and functionality for their needs and assignments is a meaningful way to support the above UX principles, but also to aid mission attainment. Ways to tailor the experience include: Emphasizing or exposing interfaces or portions of interfaces, especially for data entry and transactional functions, to only those user roles with a need and authority for said functionality. Designing task flows and interfaces so that they at a minimum support, but optimally increase the efficiency of, the larger assignment beyond the system for key end-user groups. Reducing data inputs, and options within those inputs, to only those that are meaningful and relevant to the user\u2019s role and their current context.","title":"3.1.4 Tailor Tasks and Interfaces to Roles"},{"location":"ux/3-1-principles/#315-provide-clear-system-status","text":"Clear, consistent status indication and feedback is a certain measure for instilling trust in users and providing a sense of confidence and control within the system. With this sense of control, users do not have to split their attention between managing the system and contextualizing its responses and outputs, and can instead focus entirely on quickly and accurately accomplishing their end goal. A system should clearly report its status by: Indicating the recency and providing a detailed status of its source systems. Providing up-to-date context for the content and data presented by the system. Utilizing consistent enterprise metaphors for indicating status, such as stoplight coloring, at both the system and individual data record levels. Telling the user when it is busy processing their request, and, if the response is expected to take longer than a few seconds, offering an indication of how much longer the response will take or what percent of the process is complete.","title":"3.1.5 Provide Clear System Status"},{"location":"ux/3-1-principles/#316-prioritize-alerts-and-exceptions","text":"As the nature of many tasks within the logistics community includes attending to conspicuous, outlying data and \u201cgreening up\u201d problem areas, many users can greatly benefit by having these situations brought to their attention systematically, instead of by someone up or down the chain of command, or by hunting through data themselves. Where appropriate then, systems should prioritize alerts and exceptions to support this type of task by: Alerting users where problems may exist in the source data or in calculations made by the system with uncertain source data. Highlighting metrics that are below target goals, limits, or enterprise standards. Offering deeper assessments with explanations, breakdowns, or drivers of metrics and calculations.","title":"3.1.6 Prioritize Alerts and Exceptions"},{"location":"ux/3-1-principles/#317-ensure-fast-findability","text":"Due to the inherent urgency of some mission support tasks, systems should be designed to allow users to quickly find information. Some means to assist this include: Providing smart defaults on interface configurations and search parameters. Tailoring data reports to include unrequested but meaningful supporting data alongside the user\u2019s requested information, sparing the user a second query or deeper analysis. Allowing users to individually save settings, custom searches, and tool preferences. Adhering to visual design best practices for clear presentation of content and data.","title":"3.1.7 Ensure Fast Findability"},{"location":"ux/3-1-principles/#318-reduce-reliance-on-externalities","text":"Fragmented experiences are too often the norm, whether it be requiring users to jump between systems, or referencing paper documents or print materials. Pulling as much information and as many sub-tasks as makes sense into a user\u2019s primary system of need can consolidate work and reduce fragmentation. The end result is an allowance for the user to give more attention to their actual end goals and less on managing tools, thereby supporting the above principles of faster, more streamlined decision-making. Providing plain-text descriptions of acronyms and codes. Optimizing system task flows to seamlessly fit into larger external processes. Digitizing paper processes, especially where immediate transcription back into a digital system is expected.","title":"3.1.8 Reduce Reliance on Externalities"},{"location":"ux/3-2-methodology/","text":"3.2 UX Methodology Successful user experience design requires accomplishing a number of successive, iterative steps, each helping drive towards a higher level of specificity and fidelity than the prior.","title":"3.2 UX Methodology"},{"location":"ux/3-2-methodology/#32-ux-methodology","text":"Successful user experience design requires accomplishing a number of successive, iterative steps, each helping drive towards a higher level of specificity and fidelity than the prior.","title":"3.2 UX Methodology"},{"location":"ux/3-3-1-1-interviews/","text":"Back to Phase 1: Research & Discovery 3.3.1.1 Interviews Direct interviews are typically the single most effective method for uncovering insight. There are different types of interviews, each serving a different purpose. When to use Use interviews to examine users, subject matter experts or stakeholders closely, especially to: Investigate more deeply into users\u2019 needs or behaviors, e.g. motivations, expectations, concerns, and frustrations. Gain subject-matter expertise on mission current state, best practices, and problem articulation. Understand mission objectives, current opportunities and obstacles, and project success criteria. Requirements Access to actual end users. (User proxies, or representatives, can be helpful, but frequently do not have a full picture of real end users\u2019 process, context or usage scenarios.) Ability to record and transcribe later, or have a second team member listening for note taking. (It is generally inefficient to lead the interview while also taking notes.) Variations Contextual/field study versus direct discussion Contextual interviews are conducted in the environment in which the user would typically interact with your system (e.g. at their workplace), and generally involves the participant allowing the interviewer to watch them use the system for actual purposes (i.e. while \u201cdoing their day job\u201d). This method allows the interviewer to learn, from direct observation, about details that frequently go unaccounted for, or unremembered by the user when out of setting, such as necessary supplemental materials (e.g. a field manual kept next to the computer). It also allows for pain points that have been normalized by the user over time to be noticed and discussed. Inviting a participant to a direct session (either away from the user\u2019s work station or even just via a dedicated phone call or online meeting) can remove the burden and distraction of routine job obligations and keep the focus on your topics. Group versus individual Interviewing a large group of users together is best used to get a wide range of input quickly and a shallow exploration of the problem space. This method is best done as an initial session, prior to breaking out individual or small-group interviews. In individual interviews, topics can be explored in depth, without concern for a \u201cloudest voice\u201d or highest ranking presence inhibiting equitable input. Structured versus unstructured Structured interviews should be used when you have a specific set of questions to be addressed with your participant, especially when the ability to compare responses between multiple participants is desired. In unstructured interviews the topics are only broadly laid out. The interviewer does much more listening than interacting, sometimes speaking only to prompt the participant to continue or explain something further. These interviews are effective when the desire is to cover touchy subjects without directly asking about them, or to probe deeper emotional reactions by offering a neutral, safe conversational space. Product/output Findings and even direct quotes from user interviews become the backbone of personas , user scenarios and user journeys , and drive the definition of use cases . Input from subject matter experts and stakeholders form the primary depiction of staff/support actions and processes in service blueprints and user journeys . References & Resources Reference Complete Beginner\u2019s Guide to UX Research https://www.uxbooth.com/articles/complete-beginners-guide-to-design-research Resources How to Conduct User Interviews https://www.interaction-design.org/literature/article/how-to-conduct-user-interviews Listening Tips https://indiyoung.com/listening-tips/#more-7443","title":"Interviews"},{"location":"ux/3-3-1-1-interviews/#3311-interviews","text":"Direct interviews are typically the single most effective method for uncovering insight. There are different types of interviews, each serving a different purpose.","title":"3.3.1.1 Interviews"},{"location":"ux/3-3-1-1-interviews/#when-to-use","text":"Use interviews to examine users, subject matter experts or stakeholders closely, especially to: Investigate more deeply into users\u2019 needs or behaviors, e.g. motivations, expectations, concerns, and frustrations. Gain subject-matter expertise on mission current state, best practices, and problem articulation. Understand mission objectives, current opportunities and obstacles, and project success criteria.","title":"When to use"},{"location":"ux/3-3-1-1-interviews/#requirements","text":"Access to actual end users. (User proxies, or representatives, can be helpful, but frequently do not have a full picture of real end users\u2019 process, context or usage scenarios.) Ability to record and transcribe later, or have a second team member listening for note taking. (It is generally inefficient to lead the interview while also taking notes.)","title":"Requirements"},{"location":"ux/3-3-1-1-interviews/#variations","text":"","title":"Variations"},{"location":"ux/3-3-1-1-interviews/#contextualfield-study-versus-direct-discussion","text":"Contextual interviews are conducted in the environment in which the user would typically interact with your system (e.g. at their workplace), and generally involves the participant allowing the interviewer to watch them use the system for actual purposes (i.e. while \u201cdoing their day job\u201d). This method allows the interviewer to learn, from direct observation, about details that frequently go unaccounted for, or unremembered by the user when out of setting, such as necessary supplemental materials (e.g. a field manual kept next to the computer). It also allows for pain points that have been normalized by the user over time to be noticed and discussed. Inviting a participant to a direct session (either away from the user\u2019s work station or even just via a dedicated phone call or online meeting) can remove the burden and distraction of routine job obligations and keep the focus on your topics.","title":"Contextual/field study versus direct discussion"},{"location":"ux/3-3-1-1-interviews/#group-versus-individual","text":"Interviewing a large group of users together is best used to get a wide range of input quickly and a shallow exploration of the problem space. This method is best done as an initial session, prior to breaking out individual or small-group interviews. In individual interviews, topics can be explored in depth, without concern for a \u201cloudest voice\u201d or highest ranking presence inhibiting equitable input.","title":"Group versus individual"},{"location":"ux/3-3-1-1-interviews/#structured-versus-unstructured","text":"Structured interviews should be used when you have a specific set of questions to be addressed with your participant, especially when the ability to compare responses between multiple participants is desired. In unstructured interviews the topics are only broadly laid out. The interviewer does much more listening than interacting, sometimes speaking only to prompt the participant to continue or explain something further. These interviews are effective when the desire is to cover touchy subjects without directly asking about them, or to probe deeper emotional reactions by offering a neutral, safe conversational space.","title":"Structured versus unstructured"},{"location":"ux/3-3-1-1-interviews/#productoutput","text":"Findings and even direct quotes from user interviews become the backbone of personas , user scenarios and user journeys , and drive the definition of use cases . Input from subject matter experts and stakeholders form the primary depiction of staff/support actions and processes in service blueprints and user journeys .","title":"Product/output"},{"location":"ux/3-3-1-1-interviews/#references-resources","text":"","title":"References &amp; Resources"},{"location":"ux/3-3-1-1-interviews/#reference","text":"Complete Beginner\u2019s Guide to UX Research https://www.uxbooth.com/articles/complete-beginners-guide-to-design-research","title":"Reference"},{"location":"ux/3-3-1-1-interviews/#resources","text":"How to Conduct User Interviews https://www.interaction-design.org/literature/article/how-to-conduct-user-interviews Listening Tips https://indiyoung.com/listening-tips/#more-7443","title":"Resources"},{"location":"ux/3-3-1-2-surveys/","text":"Back to Phase 1: Research & Discovery 3.3.1.2 Surveys Conducting surveys is a simple, quick tool for gathering a large amount of input. And since this method is generally simpler and easier than others, it can frequently be a default recommendation. However, it is important to consider the advantages and limitations of surveying before selecting this method. When to use Use to quickly gather shallow feedback from a large number of respondents, or from a set of respondents who have anonymity concerns. Requirements Ability to distribute the survey to your desired participants. Ability to collect its results (if anonymous, a third party must be used). Variations Identified-response versus anonymous With identified-response surveys, the responses are tied to the identify of the responder, either through requiring the responder to provide that information, or by issuing uniquely identified surveys to each responder. This level of control can have a suppressing effect on the survey, both in terms of total responses, and in amount of honest criticism offered in answers. On the other hand, slanderous or other malevolent responses will be largely avoided. Be mindful of your subject matter and its prevailing sentiment among your audience. Identified-response surveys also have the advantage of optionally asking respondents to avail themselves for follow-up surveys or interviews. Anonymous surveys offer the ability to collect more honest responses when you expect the topic to skew negative, and will generally receive higher participation because of the lower responsibility or risk to the respondent. Product/output Categorizing responses to open-ended questions can provide rough quantitative measures around frequency of similar responses. (For example, answers like \u201cSite is slow\u201d and \u201cPages take too long to load\u201d could both be categorized as \u201cSite performance does not meet expectations.\u201d) Close-ended questions frequently already lend themselves to this sort aggregate quantifying. This data can then provide support and evidence to personas and user scenarios , as well as help prioritize remediation efforts for major existing deficiencies. The individual open-ended responses themselves can uncover topics for follow-up user interview sessions around intents, motivations, and frustrations, especially if your survey\u2019s respondents allow their identities to be known and agree to participate further. Practical considerations Consider distributing the survey to a couple test participants prior to wider distribution, to uncover questions that can be misinterpreted and should be rewritten, and to ensure you receive the types of responses you are expecting. Ask questions that you have reason to believe your respondents are willing and able to answer sincerely. Be aware that requiring respondent identification will reduce your response rate. Avoid using Likert scales (i.e. 1-10 ratings) or similar structures for quantifying opinions. These tend to rely on subjective or arbitrary value assignments by your respondents, which devalues their responses. References & Resources References Online Surveys https://www.usability.gov/how-to-and-tools/methods/online-surveys.html Complete Beginner\u2019s Guide to UX Research https://www.uxbooth.com/articles/complete-beginners-guide-to-design-research Resources On Surveys https://medium.com/mule-design/on-surveys-5a73dda5e9a0","title":"Surveys"},{"location":"ux/3-3-1-2-surveys/#3312-surveys","text":"Conducting surveys is a simple, quick tool for gathering a large amount of input. And since this method is generally simpler and easier than others, it can frequently be a default recommendation. However, it is important to consider the advantages and limitations of surveying before selecting this method.","title":"3.3.1.2 Surveys"},{"location":"ux/3-3-1-2-surveys/#when-to-use","text":"Use to quickly gather shallow feedback from a large number of respondents, or from a set of respondents who have anonymity concerns.","title":"When to use"},{"location":"ux/3-3-1-2-surveys/#requirements","text":"Ability to distribute the survey to your desired participants. Ability to collect its results (if anonymous, a third party must be used).","title":"Requirements"},{"location":"ux/3-3-1-2-surveys/#variations","text":"","title":"Variations"},{"location":"ux/3-3-1-2-surveys/#identified-response-versus-anonymous","text":"With identified-response surveys, the responses are tied to the identify of the responder, either through requiring the responder to provide that information, or by issuing uniquely identified surveys to each responder. This level of control can have a suppressing effect on the survey, both in terms of total responses, and in amount of honest criticism offered in answers. On the other hand, slanderous or other malevolent responses will be largely avoided. Be mindful of your subject matter and its prevailing sentiment among your audience. Identified-response surveys also have the advantage of optionally asking respondents to avail themselves for follow-up surveys or interviews. Anonymous surveys offer the ability to collect more honest responses when you expect the topic to skew negative, and will generally receive higher participation because of the lower responsibility or risk to the respondent.","title":"Identified-response versus anonymous"},{"location":"ux/3-3-1-2-surveys/#productoutput","text":"Categorizing responses to open-ended questions can provide rough quantitative measures around frequency of similar responses. (For example, answers like \u201cSite is slow\u201d and \u201cPages take too long to load\u201d could both be categorized as \u201cSite performance does not meet expectations.\u201d) Close-ended questions frequently already lend themselves to this sort aggregate quantifying. This data can then provide support and evidence to personas and user scenarios , as well as help prioritize remediation efforts for major existing deficiencies. The individual open-ended responses themselves can uncover topics for follow-up user interview sessions around intents, motivations, and frustrations, especially if your survey\u2019s respondents allow their identities to be known and agree to participate further.","title":"Product/output"},{"location":"ux/3-3-1-2-surveys/#practical-considerations","text":"Consider distributing the survey to a couple test participants prior to wider distribution, to uncover questions that can be misinterpreted and should be rewritten, and to ensure you receive the types of responses you are expecting. Ask questions that you have reason to believe your respondents are willing and able to answer sincerely. Be aware that requiring respondent identification will reduce your response rate. Avoid using Likert scales (i.e. 1-10 ratings) or similar structures for quantifying opinions. These tend to rely on subjective or arbitrary value assignments by your respondents, which devalues their responses.","title":"Practical considerations"},{"location":"ux/3-3-1-2-surveys/#references-resources","text":"","title":"References &amp; Resources"},{"location":"ux/3-3-1-2-surveys/#references","text":"Online Surveys https://www.usability.gov/how-to-and-tools/methods/online-surveys.html Complete Beginner\u2019s Guide to UX Research https://www.uxbooth.com/articles/complete-beginners-guide-to-design-research","title":"References"},{"location":"ux/3-3-1-2-surveys/#resources","text":"On Surveys https://medium.com/mule-design/on-surveys-5a73dda5e9a0","title":"Resources"},{"location":"ux/3-3-1-3-metrics/","text":"Back to Phase 1: Research & Discovery 3.3.1.3 Site Metrics Analysis When a pre-existing capability is available in a live environment, and has an integrated data analytics tool, time should be spent assessing the data that has been captured to learn more about your users\u2019 actual usage and behavior patterns. When to use When a pre-existing capability is available in a live environment, and has an integrated data analytics tool, time should be spent assessing the data that has been captured to learn more about your users\u2019 actual usage and behavior patterns. Requirements Data captured through an analytics tool or similar. Product/output Behavioral patterns discerned through metrics analysis can be used to: Inform customer journeys and service design blueprints . Provide quantitative support and evidence to personas or user scenarios . Practical considerations Look for click paths (page-to-page navigational flows) that have drop-offs or exits you would not expect, to identify pain points or unknown user intents. Carefully consider sample sizes and occurrence percentages to avoid fixating on edge cases. Avoid attempting to characterize user intent/goals solely by assessing their behavior. Reference 8 Usability Metrics Tech Teams Can Use To Analyze User Behaviour https://www.shopify.com/partners/blog/9-usability-metrics-tech-teams-can-use-to-analyze-user-behaviour","title":"Site metrics analysis"},{"location":"ux/3-3-1-3-metrics/#3313-site-metrics-analysis","text":"When a pre-existing capability is available in a live environment, and has an integrated data analytics tool, time should be spent assessing the data that has been captured to learn more about your users\u2019 actual usage and behavior patterns.","title":"3.3.1.3 Site Metrics Analysis"},{"location":"ux/3-3-1-3-metrics/#when-to-use","text":"When a pre-existing capability is available in a live environment, and has an integrated data analytics tool, time should be spent assessing the data that has been captured to learn more about your users\u2019 actual usage and behavior patterns.","title":"When to use"},{"location":"ux/3-3-1-3-metrics/#requirements","text":"Data captured through an analytics tool or similar.","title":"Requirements"},{"location":"ux/3-3-1-3-metrics/#productoutput","text":"Behavioral patterns discerned through metrics analysis can be used to: Inform customer journeys and service design blueprints . Provide quantitative support and evidence to personas or user scenarios .","title":"Product/output"},{"location":"ux/3-3-1-3-metrics/#practical-considerations","text":"Look for click paths (page-to-page navigational flows) that have drop-offs or exits you would not expect, to identify pain points or unknown user intents. Carefully consider sample sizes and occurrence percentages to avoid fixating on edge cases. Avoid attempting to characterize user intent/goals solely by assessing their behavior.","title":"Practical considerations"},{"location":"ux/3-3-1-3-metrics/#reference","text":"8 Usability Metrics Tech Teams Can Use To Analyze User Behaviour https://www.shopify.com/partners/blog/9-usability-metrics-tech-teams-can-use-to-analyze-user-behaviour","title":"Reference"},{"location":"ux/3-3-1-4-diary/","text":"Back to Phase 1: Research & Discovery 3.3.1.4 Diary Studies In this method, study participants are asked to keep a diary and log specific information over time about activities being studied. The diary itself can range from a simple paper journal to a digital recording device, and the input to be collected in the log entries can range from highly structured responses (like duration of the activity in seconds) to open-ended responses like detailed depictions of thoughts or emotions. When to use Use to collect qualitative data about user behaviors, activities and experiences over time, especially when field study or \u201cshadowing\u201d is not possible, such as when the duration of the activities to be studied is longer than a day or two, or involves the user traveling to multiple geographic locations, or requires round-the-clock monitoring. Requirements Access to actual end users who are willing to earnestly and accurately report their own thoughts and actions. A means of collecting diaries at the end of the reporting period. Product/output Depending on the types of input your study requested of its participants, the diary entries can be decomposed and categorized or aggregated into quantitative measures to inform and support personas, or can be used for qualitative inputs to populate personas , user scenarios , journey maps and service design blueprints . Reference Diary Studies: Understanding Long-Term User Behavior and Experiences https://www.nngroup.com/articles/diary-studies","title":"Diary studies"},{"location":"ux/3-3-1-4-diary/#3314-diary-studies","text":"In this method, study participants are asked to keep a diary and log specific information over time about activities being studied. The diary itself can range from a simple paper journal to a digital recording device, and the input to be collected in the log entries can range from highly structured responses (like duration of the activity in seconds) to open-ended responses like detailed depictions of thoughts or emotions.","title":"3.3.1.4 Diary Studies"},{"location":"ux/3-3-1-4-diary/#when-to-use","text":"Use to collect qualitative data about user behaviors, activities and experiences over time, especially when field study or \u201cshadowing\u201d is not possible, such as when the duration of the activities to be studied is longer than a day or two, or involves the user traveling to multiple geographic locations, or requires round-the-clock monitoring.","title":"When to use"},{"location":"ux/3-3-1-4-diary/#requirements","text":"Access to actual end users who are willing to earnestly and accurately report their own thoughts and actions. A means of collecting diaries at the end of the reporting period.","title":"Requirements"},{"location":"ux/3-3-1-4-diary/#productoutput","text":"Depending on the types of input your study requested of its participants, the diary entries can be decomposed and categorized or aggregated into quantitative measures to inform and support personas, or can be used for qualitative inputs to populate personas , user scenarios , journey maps and service design blueprints .","title":"Product/output"},{"location":"ux/3-3-1-4-diary/#reference","text":"Diary Studies: Understanding Long-Term User Behavior and Experiences https://www.nngroup.com/articles/diary-studies","title":"Reference"},{"location":"ux/3-3-2-1-personas/","text":"Back to Phase 1: Research & Discovery 3.3.2.1 Personas Personas are realistic amalgamated representations of your key audience segments. Their humanistic details offer your team the ability to better empathize with your user types. They attempt to help your team understand, relate to, and remember the end user throughout the development process. When to use Use to humanize your users, especially in order to create rich scenarios and journey maps . Referring back to a well-crafted persona that contains specific human concerns and feelings, can help identify opportunities for your system to better match their needs. Requirements Deep understanding of your users, including an understanding of how to break them up into meaningfully different segments, from direct user input collected in your research. Product/output The output should be a single page or card, that fully characterizes its represented user segment, including goals and motivations, challenges and frustrations, behaviors and habits, attitudes and concerns. Personas are used as the insight to generate scenarios and journey maps that include a comprehensive user perspective. Practical considerations Avoid adding details to a persona when they are not relevant to helping achieve that user segment\u2019s goals. Examples could be demographic information like gender and age or lifestyle information like occupation or hobbies. In some instances, those details may be meaningful, but generally are not. Instead, those details allow for bias, unconscious or otherwise, to affect our consideration of the user segment. Write personas such that they are always sympathetic, meaning they are always able to elicit fondness and sympathy from your team. Creating opportunity for antagonism in a persona will only serve to alienate your team from a valid user segment. References Describing Personas https://medium.com/inclusive-software/describing-personas-af992e3fc527 Personas Make Users Memorable for Product Team Members https://www.nngroup.com/articles/persona","title":"Personas"},{"location":"ux/3-3-2-1-personas/#3321-personas","text":"Personas are realistic amalgamated representations of your key audience segments. Their humanistic details offer your team the ability to better empathize with your user types. They attempt to help your team understand, relate to, and remember the end user throughout the development process.","title":"3.3.2.1 Personas"},{"location":"ux/3-3-2-1-personas/#when-to-use","text":"Use to humanize your users, especially in order to create rich scenarios and journey maps . Referring back to a well-crafted persona that contains specific human concerns and feelings, can help identify opportunities for your system to better match their needs.","title":"When to use"},{"location":"ux/3-3-2-1-personas/#requirements","text":"Deep understanding of your users, including an understanding of how to break them up into meaningfully different segments, from direct user input collected in your research.","title":"Requirements"},{"location":"ux/3-3-2-1-personas/#productoutput","text":"The output should be a single page or card, that fully characterizes its represented user segment, including goals and motivations, challenges and frustrations, behaviors and habits, attitudes and concerns. Personas are used as the insight to generate scenarios and journey maps that include a comprehensive user perspective.","title":"Product/output"},{"location":"ux/3-3-2-1-personas/#practical-considerations","text":"Avoid adding details to a persona when they are not relevant to helping achieve that user segment\u2019s goals. Examples could be demographic information like gender and age or lifestyle information like occupation or hobbies. In some instances, those details may be meaningful, but generally are not. Instead, those details allow for bias, unconscious or otherwise, to affect our consideration of the user segment. Write personas such that they are always sympathetic, meaning they are always able to elicit fondness and sympathy from your team. Creating opportunity for antagonism in a persona will only serve to alienate your team from a valid user segment.","title":"Practical considerations"},{"location":"ux/3-3-2-1-personas/#references","text":"Describing Personas https://medium.com/inclusive-software/describing-personas-af992e3fc527 Personas Make Users Memorable for Product Team Members https://www.nngroup.com/articles/persona","title":"References"},{"location":"ux/3-3-2-2-scenarios/","text":"Back to Phase 1: Research & Discovery 3.3.2.2 User Scenarios User scenarios are essentially short expositions, used to explore why a specific user or user group would use your system within a narrow context and purpose. They detail the user\u2019s motivations, goals and concerns, and are perfect setups for journey maps . When comparing different scenarios that have the same user goals, opportunities for the system to be more helpful in accomplishing those goals can be discovered. When to use Use scenarios to explore how users might approach your system in specific contexts or environments. This exploration should foster insight into ways to optimize the system for these scenarios, thus increasing the value your system offers its users. Requirements Scenarios should be crafted around actual, real-world circumstances, and require realistic user perspectives. As such, having direct input from end users is crucial for accurate scenarios. Working from their actual stories, provided in interviews or even possibly surveys , grounds your scenarios in reality. While not a hard and fast requirement, starting from personas is a good way to ensure your scenarios are focused on your key user types and their needs. Product/output Scenarios are typically written as simple prose, like the start of a short story. They form the input for user journey maps , where the scenario can be played out and interactions with your system can be explored. Scenarios should also become the foundation of your concept testing and usability testing \u2013 having your test participants attempt to accomplish the goals laid out in your scenarios is an effective method of design validation. Tim is preparing to report at the quarterly WSR on his weapon system\u2019s availability in the previous quarter. When doing this analysis previously, he\u2019s relied on downloading ESR data then importing it into a spreadsheet created years ago by a colleague. This quarter, however, his CO has told him his analysis needs to go further and identify the 10 maintenance issues most limiting overall availability and at what locations they are most frequently occurring. This has Tim worried, as he doesn\u2019t believe he can break down the data and prepare it for presentation in the time he has before the conference.\u201d Practical considerations It\u2019s impossible to craft every scenario your users would have for using your system. Instead, start by drawing some of the most common scenarios out of your user research. Be willing to explore highly specific scenarios. Unexpected insights come from examining how your users\u2019 behaviors and needs change as their context does. Resource When It Comes To Personas, The Real Value Is In The Scenarios https://articles.uie.com/when-it-comes-to-personas-the-real-value-is-in-the-scenarios","title":"User Scenarios"},{"location":"ux/3-3-2-2-scenarios/#3322-user-scenarios","text":"User scenarios are essentially short expositions, used to explore why a specific user or user group would use your system within a narrow context and purpose. They detail the user\u2019s motivations, goals and concerns, and are perfect setups for journey maps . When comparing different scenarios that have the same user goals, opportunities for the system to be more helpful in accomplishing those goals can be discovered.","title":"3.3.2.2 User Scenarios"},{"location":"ux/3-3-2-2-scenarios/#when-to-use","text":"Use scenarios to explore how users might approach your system in specific contexts or environments. This exploration should foster insight into ways to optimize the system for these scenarios, thus increasing the value your system offers its users.","title":"When to use"},{"location":"ux/3-3-2-2-scenarios/#requirements","text":"Scenarios should be crafted around actual, real-world circumstances, and require realistic user perspectives. As such, having direct input from end users is crucial for accurate scenarios. Working from their actual stories, provided in interviews or even possibly surveys , grounds your scenarios in reality. While not a hard and fast requirement, starting from personas is a good way to ensure your scenarios are focused on your key user types and their needs.","title":"Requirements"},{"location":"ux/3-3-2-2-scenarios/#productoutput","text":"Scenarios are typically written as simple prose, like the start of a short story. They form the input for user journey maps , where the scenario can be played out and interactions with your system can be explored. Scenarios should also become the foundation of your concept testing and usability testing \u2013 having your test participants attempt to accomplish the goals laid out in your scenarios is an effective method of design validation. Tim is preparing to report at the quarterly WSR on his weapon system\u2019s availability in the previous quarter. When doing this analysis previously, he\u2019s relied on downloading ESR data then importing it into a spreadsheet created years ago by a colleague. This quarter, however, his CO has told him his analysis needs to go further and identify the 10 maintenance issues most limiting overall availability and at what locations they are most frequently occurring. This has Tim worried, as he doesn\u2019t believe he can break down the data and prepare it for presentation in the time he has before the conference.\u201d","title":"Product/output"},{"location":"ux/3-3-2-2-scenarios/#practical-considerations","text":"It\u2019s impossible to craft every scenario your users would have for using your system. Instead, start by drawing some of the most common scenarios out of your user research. Be willing to explore highly specific scenarios. Unexpected insights come from examining how your users\u2019 behaviors and needs change as their context does.","title":"Practical considerations"},{"location":"ux/3-3-2-2-scenarios/#resource","text":"When It Comes To Personas, The Real Value Is In The Scenarios https://articles.uie.com/when-it-comes-to-personas-the-real-value-is-in-the-scenarios","title":"Resource"},{"location":"ux/3-3-2-3-journey/","text":"Back to Phase 1: Research & Discovery 3.3.2.3 Journey Maps A journey map is a narrative-driven graph that describes the typical journey of an end user by representing the different touchpoints and interactions that encompass their experience with the system and surrounding environment. When to use Use journey maps to explore how users might use your system as part of accomplishing the goals laid out in specific scenarios . This exploration should foster insight into ways to optimize the system\u2019s design to best support these journeys, thus increasing the value your system offers its users. Requirements An understanding of meaningful scenarios to map, and the user insights to map them, from user research ( interviews in particular). Product/output The map product is a timeline-based chart, typically split into two primary sections. The first section, the lens, depicts the user\u2019s perspective as they become oriented to the activities and requirements of each new stage of the journey. The journey section then captures the specific activities and interactions in which the user is involved at each stage, along with typical thoughts and feelings they might have along the way. This layer of thoughts and feelings provides context for ideas about better ways to support their experience. The process of depicting a user journey will uncover insights and opportunities to better align the current experience with user needs, or introduce new capabilities to fill gaps in the experience. These ideas should next be explored in conceptual designs , evaluated for feasibility, and finally be roadmapped for implementation. If the journey occurs as part of a larger service offering from your system, a next step could be exploring the full details of the service execution in a service design blueprint . Resource Journey Mapping in Real Life: A Survey of UX Practitioners https://www.nngroup.com/articles/journey-mapping-ux-practitioners","title":"Journey maps"},{"location":"ux/3-3-2-3-journey/#3323-journey-maps","text":"A journey map is a narrative-driven graph that describes the typical journey of an end user by representing the different touchpoints and interactions that encompass their experience with the system and surrounding environment.","title":"3.3.2.3 Journey Maps"},{"location":"ux/3-3-2-3-journey/#when-to-use","text":"Use journey maps to explore how users might use your system as part of accomplishing the goals laid out in specific scenarios . This exploration should foster insight into ways to optimize the system\u2019s design to best support these journeys, thus increasing the value your system offers its users.","title":"When to use"},{"location":"ux/3-3-2-3-journey/#requirements","text":"An understanding of meaningful scenarios to map, and the user insights to map them, from user research ( interviews in particular).","title":"Requirements"},{"location":"ux/3-3-2-3-journey/#productoutput","text":"The map product is a timeline-based chart, typically split into two primary sections. The first section, the lens, depicts the user\u2019s perspective as they become oriented to the activities and requirements of each new stage of the journey. The journey section then captures the specific activities and interactions in which the user is involved at each stage, along with typical thoughts and feelings they might have along the way. This layer of thoughts and feelings provides context for ideas about better ways to support their experience. The process of depicting a user journey will uncover insights and opportunities to better align the current experience with user needs, or introduce new capabilities to fill gaps in the experience. These ideas should next be explored in conceptual designs , evaluated for feasibility, and finally be roadmapped for implementation. If the journey occurs as part of a larger service offering from your system, a next step could be exploring the full details of the service execution in a service design blueprint .","title":"Product/output"},{"location":"ux/3-3-2-3-journey/#resource","text":"Journey Mapping in Real Life: A Survey of UX Practitioners https://www.nngroup.com/articles/journey-mapping-ux-practitioners","title":"Resource"},{"location":"ux/3-3-2-4-blueprints/","text":"Back to Phase 1: Research & Discovery 3.3.2.4 Service Design Blueprints A service design blueprint is detailed diagram that captures key processes in the delivery of a service, starting from the end user\u2019s actions and system touchpoints and layering in the front- and backstage support personnel\u2019s tasks and roles and the additional systems that all ultimately enable the service. When to use Use blueprinting to model solutions that address specific process inefficiencies and pain points. This is especially useful in cases where a provided service is complex, such as when multiple support roles or systems are required to facilitate accomplishing the end user\u2019s goal. Requirements Service design blueprints work best when layered onto an existing journey map , but can be executed independently. A thorough understanding of the service delivery process is necessary to accurately capture it. Because complex services can require the efforts of a host of different support personnel and systems, elicit direct input from all participants through interviews or a collaborative workshop session . Product/output A service design blueprint should be a time-based diagram, with swim lanes for the end user\u2019s actions, system touchpoints, frontstage support personnel actions, backstage actions, and additional support processes/additional required systems. The detailed analysis inherit in creating a service blueprint will uncover gaps, inefficiencies, and pain points in your current service delivery model that your conceptual designs can address. Resources Service Blueprints: Laying the Foundation https://www.cooper.com/journal/2014/08/service-blueprints-laying-the-foundation The difference between a journey map and a service blueprint https://blog.practicalservicedesign.com/the-difference-between-a-journey-map-and-a-service-blueprint-31a6e24c4a6c","title":"Service design blueprints"},{"location":"ux/3-3-2-4-blueprints/#3324-service-design-blueprints","text":"A service design blueprint is detailed diagram that captures key processes in the delivery of a service, starting from the end user\u2019s actions and system touchpoints and layering in the front- and backstage support personnel\u2019s tasks and roles and the additional systems that all ultimately enable the service.","title":"3.3.2.4 Service Design Blueprints"},{"location":"ux/3-3-2-4-blueprints/#when-to-use","text":"Use blueprinting to model solutions that address specific process inefficiencies and pain points. This is especially useful in cases where a provided service is complex, such as when multiple support roles or systems are required to facilitate accomplishing the end user\u2019s goal.","title":"When to use"},{"location":"ux/3-3-2-4-blueprints/#requirements","text":"Service design blueprints work best when layered onto an existing journey map , but can be executed independently. A thorough understanding of the service delivery process is necessary to accurately capture it. Because complex services can require the efforts of a host of different support personnel and systems, elicit direct input from all participants through interviews or a collaborative workshop session .","title":"Requirements"},{"location":"ux/3-3-2-4-blueprints/#productoutput","text":"A service design blueprint should be a time-based diagram, with swim lanes for the end user\u2019s actions, system touchpoints, frontstage support personnel actions, backstage actions, and additional support processes/additional required systems. The detailed analysis inherit in creating a service blueprint will uncover gaps, inefficiencies, and pain points in your current service delivery model that your conceptual designs can address.","title":"Product/output"},{"location":"ux/3-3-2-4-blueprints/#resources","text":"Service Blueprints: Laying the Foundation https://www.cooper.com/journal/2014/08/service-blueprints-laying-the-foundation The difference between a journey map and a service blueprint https://blog.practicalservicedesign.com/the-difference-between-a-journey-map-and-a-service-blueprint-31a6e24c4a6c","title":"Resources"},{"location":"ux/3-3-2-5-usecase/","text":"Back to Phase 1: Research & Discovery 3.3.2.5 Use Cases A use case is a simple depiction of a sequence of actions in a system that result in a meaningful outcome for the actor (who is typically an end user of the system, but could also be support personnel in the case of a service). When to use Create use cases to identify and catalog all of the tasks and sub-tasks that the system needs to support in order to fulfill your users\u2019 needs within it. They can be derived by decomposing stakeholder requirements around system purpose and intent, or by decomposing journey maps into discrete tasks and system interactions. Product/output Use cases are typically written in a two-column format. The left column lists the intention of the actor and the right column lists the system's responsibilities and responses. In this way, the use case can resemble a dialogue between actor and system. Any prerequisites should be identified and noted. Use cases are good building blocks for user stories and helpful references while identifying a scope of work for a given sprint or release. Practical considerations Use cases function most effectively when they are written simply, technology-free and implementation-agnostic. That is, they are written unaware of their eventual technical solutions. This keeps the focus on task accomplishment from the user\u2019s perspective, and keeps the cases relevant even as the system\u2019s technology and architecture change. Reference Essential (Abstract) Use Cases: An Agile Introduction http://agilemodeling.com/artifacts/essentialUseCase.htm","title":"Use cases"},{"location":"ux/3-3-2-5-usecase/#3325-use-cases","text":"A use case is a simple depiction of a sequence of actions in a system that result in a meaningful outcome for the actor (who is typically an end user of the system, but could also be support personnel in the case of a service).","title":"3.3.2.5 Use Cases"},{"location":"ux/3-3-2-5-usecase/#when-to-use","text":"Create use cases to identify and catalog all of the tasks and sub-tasks that the system needs to support in order to fulfill your users\u2019 needs within it. They can be derived by decomposing stakeholder requirements around system purpose and intent, or by decomposing journey maps into discrete tasks and system interactions.","title":"When to use"},{"location":"ux/3-3-2-5-usecase/#productoutput","text":"Use cases are typically written in a two-column format. The left column lists the intention of the actor and the right column lists the system's responsibilities and responses. In this way, the use case can resemble a dialogue between actor and system. Any prerequisites should be identified and noted. Use cases are good building blocks for user stories and helpful references while identifying a scope of work for a given sprint or release.","title":"Product/output"},{"location":"ux/3-3-2-5-usecase/#practical-considerations","text":"Use cases function most effectively when they are written simply, technology-free and implementation-agnostic. That is, they are written unaware of their eventual technical solutions. This keeps the focus on task accomplishment from the user\u2019s perspective, and keeps the cases relevant even as the system\u2019s technology and architecture change.","title":"Practical considerations"},{"location":"ux/3-3-2-5-usecase/#reference","text":"Essential (Abstract) Use Cases: An Agile Introduction http://agilemodeling.com/artifacts/essentialUseCase.htm","title":"Reference"},{"location":"ux/3-3-research/","text":"3.3 Phase 1: Research & Discovery \u201cUX without user research is not UX.\u201d \u2013 Nielsen Norman Group Designing a product or system that meets the needs of its users is simply not possible without first understanding those users. This principle is core to the purpose of user experience design. The research phase, then, is critical to success and becomes the foundation upon which design decisions should be made for a user-centered product. And while examining and understanding your users is the primary goal, comprehensive research should also include gaining a clear understanding of mission and stakeholder objectives and requirements, and in commercial settings, would also include market or landscape assessments. Successful designs bridge both user and stakeholder needs. The crux of conducting relevant, impactful research is having clear objectives at the outset. You should be setting these research objectives by having a clear purpose for how and where you intend to turn your research findings into inputs for later activities and methods. Only then can you know which research methods to employ and how best to employ them. In other words, start with the questions you wish to have answered and select methods best suited to answering them. Continue reading below to learn about some of the research methods most appropriate for logistics information systems and how they can be leveraged to inform subsequent work. 3.3.1 Methods for Gathering Information Interviews Direct communication in the form of interviews is a reliable way to gather user and business needs. Surveys Surveys are an easy way to gather a large amount of information in minimal time. Site metrics analysis Data-driven analysis of site usage and user behavior can provide important context and theory validation. Diary studies Logs of activities by users as they occur offer insights like context and environment details, real-time needs and behaviors. 3.3.2 Methods for Synthesizing Insights Personas Personas are realistic amalgamated representations of your key audience segments. Their humanistic details offer your team the ability to better empathize with your user types. User Scenarios These narratives explore why a specific user or user group would use your system within a narrow context and purpose, focusing on their motivations, goals, and concerns. Journey maps Visualizations that depict the typical journey for users accomplishing a task or going through a scenario, including their thoughts, system interactions, and reactions along the way. Service design blueprints Detailed diagrams that capture key processes in service delivery across all touchpoints, including the actions of the end user and the roles and tasks of any support personnel. Use cases As simple descriptions of a user\u2019s intent and the system\u2019s desired responses to satisfy, use cases help quickly catalog all of a system\u2019s necessary features and interactions. Reference UX Without User Research Is Not UX https://www.nngroup.com/articles/ux-without-user-research","title":"3.3 Phase 1: Research & Discovery"},{"location":"ux/3-3-research/#33-phase-1-research-discovery","text":"\u201cUX without user research is not UX.\u201d \u2013 Nielsen Norman Group Designing a product or system that meets the needs of its users is simply not possible without first understanding those users. This principle is core to the purpose of user experience design. The research phase, then, is critical to success and becomes the foundation upon which design decisions should be made for a user-centered product. And while examining and understanding your users is the primary goal, comprehensive research should also include gaining a clear understanding of mission and stakeholder objectives and requirements, and in commercial settings, would also include market or landscape assessments. Successful designs bridge both user and stakeholder needs. The crux of conducting relevant, impactful research is having clear objectives at the outset. You should be setting these research objectives by having a clear purpose for how and where you intend to turn your research findings into inputs for later activities and methods. Only then can you know which research methods to employ and how best to employ them. In other words, start with the questions you wish to have answered and select methods best suited to answering them. Continue reading below to learn about some of the research methods most appropriate for logistics information systems and how they can be leveraged to inform subsequent work.","title":"3.3 Phase 1: Research &amp; Discovery"},{"location":"ux/3-3-research/#331-methods-for-gathering-information","text":"","title":"3.3.1 Methods for Gathering Information"},{"location":"ux/3-3-research/#interviews","text":"Direct communication in the form of interviews is a reliable way to gather user and business needs.","title":"Interviews"},{"location":"ux/3-3-research/#surveys","text":"Surveys are an easy way to gather a large amount of information in minimal time.","title":"Surveys"},{"location":"ux/3-3-research/#site-metrics-analysis","text":"Data-driven analysis of site usage and user behavior can provide important context and theory validation.","title":"Site metrics analysis"},{"location":"ux/3-3-research/#diary-studies","text":"Logs of activities by users as they occur offer insights like context and environment details, real-time needs and behaviors.","title":"Diary studies"},{"location":"ux/3-3-research/#332-methods-for-synthesizing-insights","text":"","title":"3.3.2 Methods for Synthesizing Insights"},{"location":"ux/3-3-research/#personas","text":"Personas are realistic amalgamated representations of your key audience segments. Their humanistic details offer your team the ability to better empathize with your user types.","title":"Personas"},{"location":"ux/3-3-research/#user-scenarios","text":"These narratives explore why a specific user or user group would use your system within a narrow context and purpose, focusing on their motivations, goals, and concerns.","title":"User Scenarios"},{"location":"ux/3-3-research/#journey-maps","text":"Visualizations that depict the typical journey for users accomplishing a task or going through a scenario, including their thoughts, system interactions, and reactions along the way.","title":"Journey maps"},{"location":"ux/3-3-research/#service-design-blueprints","text":"Detailed diagrams that capture key processes in service delivery across all touchpoints, including the actions of the end user and the roles and tasks of any support personnel.","title":"Service design blueprints"},{"location":"ux/3-3-research/#use-cases","text":"As simple descriptions of a user\u2019s intent and the system\u2019s desired responses to satisfy, use cases help quickly catalog all of a system\u2019s necessary features and interactions.","title":"Use cases"},{"location":"ux/3-3-research/#reference","text":"UX Without User Research Is Not UX https://www.nngroup.com/articles/ux-without-user-research","title":"Reference"},{"location":"ux/3-4-1-1-wireframe/","text":"Back to Phase 2: Conceptual Design 3.4.1.1 Wireframing/Sketching Low-fidelity wireframing or sketching is a method for quickly translating requirements into interface designs by using simple line drawings to depict the interface. The low fidelity is a feature, as it helps signify the design is in its early stages, and that adjusting to feedback at this stage is still very low-effort and anticipated. Product/output At the initial stage, the wireframes or sketches need only be detailed enough to communicate their intent. The larger goal is to explore a range of possibilities, not to explore the nuances of any single possibility. Resource How to design a low-fi wireframe https://www.creativebloq.com/wireframes/how-design-low-fi-wireframe-91516934","title":"Wireframing/sketching"},{"location":"ux/3-4-1-1-wireframe/#3411-wireframingsketching","text":"Low-fidelity wireframing or sketching is a method for quickly translating requirements into interface designs by using simple line drawings to depict the interface. The low fidelity is a feature, as it helps signify the design is in its early stages, and that adjusting to feedback at this stage is still very low-effort and anticipated.","title":"3.4.1.1 Wireframing/Sketching"},{"location":"ux/3-4-1-1-wireframe/#productoutput","text":"At the initial stage, the wireframes or sketches need only be detailed enough to communicate their intent. The larger goal is to explore a range of possibilities, not to explore the nuances of any single possibility.","title":"Product/output"},{"location":"ux/3-4-1-1-wireframe/#resource","text":"How to design a low-fi wireframe https://www.creativebloq.com/wireframes/how-design-low-fi-wireframe-91516934","title":"Resource"},{"location":"ux/3-4-1-2-ia/","text":"Back to Phase 2: Conceptual Design 3.4.1.2 Information Architecture Information architecture (IA) is the scheme and structure defining the organization of your system\u2019s content and features. An effective IA allows users to find information and complete tasks efficiently. Scheme refers to how content is categorized or segmented. Schemes can either be objective (e.g. alphabetical, or chronological), or subjective (e.g. by topic, or by audience type). Structure refers to the relationship between content. Two common structures are hierarchical (top-down), where initial categories are broad, and lead to more specific later categories; and sequential, where content is presented along a linear progression. Methods Cardsorting In this method, participants organize topics into categories that make sense to them. In an open card sort, the participants can assign their own labels to their groupings. In a closed sort, participants are only allowed to categorize within set categories. This method is an effective means of identifying or validating an appropriate topical architecture scheme. Tree testing To validate a hierarchical structure for your content, use tree testing. In this test, a participant is asked to choose a path along the hierarchy, or \u201ctree,\u201d they would follow in order to find a given piece of information. Practical considerations Consider long-term implications of your architecture. Future content additions should be easily placeable within the architecture. Avoid structures that make for very shallow schemes (i.e. many top-level categories) or very deep schemes (i.e. many levels of categorization). References Complete Beginner\u2019s Guide to UX Research https://www.uxbooth.com/articles/complete-beginners-guide-to-design-research Organization Schemes https://www.usability.gov/how-to-and-tools/methods/organization-schemes.html Organization Structures https://www.usability.gov/how-to-and-tools/methods/organization-structures.html The Encyclopedia of Human-Computer Interaction, 2nd Ed. \u2013 Card Sorting https://www.interaction-design.org/literature/book/the-encyclopedia-of-human-computer-interaction-2nd-ed/card-sorting Tree Testing: Fast, Iterative Evaluation of Menu Labels and Categories https://www.interaction-design.org/literature/book/the-encyclopedia-of-human-computer-interaction-2nd-ed/card-sorting","title":"Information architecture"},{"location":"ux/3-4-1-2-ia/#3412-information-architecture","text":"Information architecture (IA) is the scheme and structure defining the organization of your system\u2019s content and features. An effective IA allows users to find information and complete tasks efficiently. Scheme refers to how content is categorized or segmented. Schemes can either be objective (e.g. alphabetical, or chronological), or subjective (e.g. by topic, or by audience type). Structure refers to the relationship between content. Two common structures are hierarchical (top-down), where initial categories are broad, and lead to more specific later categories; and sequential, where content is presented along a linear progression.","title":"3.4.1.2 Information Architecture"},{"location":"ux/3-4-1-2-ia/#methods","text":"","title":"Methods"},{"location":"ux/3-4-1-2-ia/#cardsorting","text":"In this method, participants organize topics into categories that make sense to them. In an open card sort, the participants can assign their own labels to their groupings. In a closed sort, participants are only allowed to categorize within set categories. This method is an effective means of identifying or validating an appropriate topical architecture scheme.","title":"Cardsorting"},{"location":"ux/3-4-1-2-ia/#tree-testing","text":"To validate a hierarchical structure for your content, use tree testing. In this test, a participant is asked to choose a path along the hierarchy, or \u201ctree,\u201d they would follow in order to find a given piece of information.","title":"Tree testing"},{"location":"ux/3-4-1-2-ia/#practical-considerations","text":"Consider long-term implications of your architecture. Future content additions should be easily placeable within the architecture. Avoid structures that make for very shallow schemes (i.e. many top-level categories) or very deep schemes (i.e. many levels of categorization).","title":"Practical considerations"},{"location":"ux/3-4-1-2-ia/#references","text":"Complete Beginner\u2019s Guide to UX Research https://www.uxbooth.com/articles/complete-beginners-guide-to-design-research Organization Schemes https://www.usability.gov/how-to-and-tools/methods/organization-schemes.html Organization Structures https://www.usability.gov/how-to-and-tools/methods/organization-structures.html The Encyclopedia of Human-Computer Interaction, 2nd Ed. \u2013 Card Sorting https://www.interaction-design.org/literature/book/the-encyclopedia-of-human-computer-interaction-2nd-ed/card-sorting Tree Testing: Fast, Iterative Evaluation of Menu Labels and Categories https://www.interaction-design.org/literature/book/the-encyclopedia-of-human-computer-interaction-2nd-ed/card-sorting","title":"References"},{"location":"ux/3-4-1-3-participatory/","text":"Back to Phase 2: Conceptual Design 3.4.1.3 Participatory Design Participatory design is typically done in a group workshop setting but can ultimately take many forms and span a broad range of activities. The real constant is the involvement of \u201cnon-designers\u201d in idea generation. This direct collaboration with stakeholders, subject matter experts, and end users allows for ideas to be validated as they are created, speeding up the design process and increasing the chances of user adoption. When to use Use when you have access to subject matter experts or end users who are willing to engage in the design process, and preferably have some amount of stake in your design outcomes. Participating in the design can help develop a sense of ownership, so this method is particularly effective with end user of systems with smaller user communities. Offering your end users early investment and ownership in the process can also help reduce their resistance to the changes the designs will require of them, once implemented. Activities There is a large range of activities that can be conducted as part of a participatory design workshop. In fact, many of the methods covered in this playbook work well in a workshop session, such as journey mapping , service blueprinting and card sorting . As your participants are largely not going to immediately be comfortable taking on the role of \u201cdesigner,\u201d it\u2019s important that chosen activities grant them opportunities to participate in multiple ways. Further, the order of activities should prime your participants, such that they are introduced and immersed into the problem space, then subsequently given a chance to provide solutions. Practical considerations Limit workshops to about 4\u20135 activities over a few hours or less to keep participants focused. If the scope of design requires more time collaborating, consider splitting into multiple workshops by topic or theme. To ensure the session is productive, the facilitator should keep the group in agreement on meaningful insights and outcomes as they are generated. References Bringing Users into Your Process Through Participatory Design https://www.slideshare.net/frogdesign/bringing-users-into-your-process-through-participatory-design","title":"Participatory design"},{"location":"ux/3-4-1-3-participatory/#3413-participatory-design","text":"Participatory design is typically done in a group workshop setting but can ultimately take many forms and span a broad range of activities. The real constant is the involvement of \u201cnon-designers\u201d in idea generation. This direct collaboration with stakeholders, subject matter experts, and end users allows for ideas to be validated as they are created, speeding up the design process and increasing the chances of user adoption.","title":"3.4.1.3 Participatory Design"},{"location":"ux/3-4-1-3-participatory/#when-to-use","text":"Use when you have access to subject matter experts or end users who are willing to engage in the design process, and preferably have some amount of stake in your design outcomes. Participating in the design can help develop a sense of ownership, so this method is particularly effective with end user of systems with smaller user communities. Offering your end users early investment and ownership in the process can also help reduce their resistance to the changes the designs will require of them, once implemented.","title":"When to use"},{"location":"ux/3-4-1-3-participatory/#activities","text":"There is a large range of activities that can be conducted as part of a participatory design workshop. In fact, many of the methods covered in this playbook work well in a workshop session, such as journey mapping , service blueprinting and card sorting . As your participants are largely not going to immediately be comfortable taking on the role of \u201cdesigner,\u201d it\u2019s important that chosen activities grant them opportunities to participate in multiple ways. Further, the order of activities should prime your participants, such that they are introduced and immersed into the problem space, then subsequently given a chance to provide solutions.","title":"Activities"},{"location":"ux/3-4-1-3-participatory/#practical-considerations","text":"Limit workshops to about 4\u20135 activities over a few hours or less to keep participants focused. If the scope of design requires more time collaborating, consider splitting into multiple workshops by topic or theme. To ensure the session is productive, the facilitator should keep the group in agreement on meaningful insights and outcomes as they are generated.","title":"Practical considerations"},{"location":"ux/3-4-1-3-participatory/#references","text":"Bringing Users into Your Process Through Participatory Design https://www.slideshare.net/frogdesign/bringing-users-into-your-process-through-participatory-design","title":"References"},{"location":"ux/3-4-1-4-concept/","text":"Back to Phase 2: Conceptual Design 3.4.1.4 Concept Testing Testing early and often is the most certain way to ensure the ultimately implemented solution completely meets your users\u2019 needs. To that end, testing should commence as soon as concepts are ready. When to use Start validating with end users as soon as conceptual designs are mature enough to demonstrate their intent. With this type of testing, early beats late in terms of benefit. Since the conceptual phase typically revolves around exploring a range of solutions, this phase of testing functions well as a means of identifying \u201cbest of breed\u201d ideas that warrant further pursuit. Requirements Designs that are meant to fulfill at least one complete user task. Access to end users willing to validate the designs. A test script or plan, for the participant to follow, that covers the user tasks the designs fulfill. An ability to collect feedback. Practical considerations See usability testing Resources Paper Prototyping: Getting User Data Before You Code https://www.nngroup.com/articles/paper-prototyping","title":"Concept testing"},{"location":"ux/3-4-1-4-concept/#3414-concept-testing","text":"Testing early and often is the most certain way to ensure the ultimately implemented solution completely meets your users\u2019 needs. To that end, testing should commence as soon as concepts are ready.","title":"3.4.1.4 Concept Testing"},{"location":"ux/3-4-1-4-concept/#when-to-use","text":"Start validating with end users as soon as conceptual designs are mature enough to demonstrate their intent. With this type of testing, early beats late in terms of benefit. Since the conceptual phase typically revolves around exploring a range of solutions, this phase of testing functions well as a means of identifying \u201cbest of breed\u201d ideas that warrant further pursuit.","title":"When to use"},{"location":"ux/3-4-1-4-concept/#requirements","text":"Designs that are meant to fulfill at least one complete user task. Access to end users willing to validate the designs. A test script or plan, for the participant to follow, that covers the user tasks the designs fulfill. An ability to collect feedback.","title":"Requirements"},{"location":"ux/3-4-1-4-concept/#practical-considerations","text":"See usability testing","title":"Practical considerations"},{"location":"ux/3-4-1-4-concept/#resources","text":"Paper Prototyping: Getting User Data Before You Code https://www.nngroup.com/articles/paper-prototyping","title":"Resources"},{"location":"ux/3-4-2-1-roadmapping/","text":"Back to Phase 2: Conceptual Design 3.4.2.1 Roadmapping The process of roadmapping is done to translate the totality of the collected requirements and conceptual design direction into an actionable plan for phased implementation. First, the requirements and designs are decomposed into an itemization of features and development efforts. Each item is then prioritized by assessing a combination of its user value, organizational value and implementation readiness. Finally, the prioritized items are strategically grouped and sequenced into releases that deliver the greatest incremental value to all parties. When to use Use roadmapping when the scope of the designed solution looks likely to exceed the capacity of a single development increment (release or similar), or valuable features require additional planning and preparation to be ready to implement. Requirements An itemization of designed features to be roadmapped. Stakeholder input on user and business value, and implementation readiness, of itemized features. Participation from all disciplines, from product ownership through the development team, to ensure the targets are realistic. Product/output The roadmap itself can take many forms, but a common one is a timeline-based chart that reflects the team\u2019s plan for implementation. It shows a target execution period or completion date for each thread of currently identified and prioritized features. Resources Roadmap Basics https://www.productplan.com/roadmap-basics","title":"Roadmapping"},{"location":"ux/3-4-2-1-roadmapping/#3421-roadmapping","text":"The process of roadmapping is done to translate the totality of the collected requirements and conceptual design direction into an actionable plan for phased implementation. First, the requirements and designs are decomposed into an itemization of features and development efforts. Each item is then prioritized by assessing a combination of its user value, organizational value and implementation readiness. Finally, the prioritized items are strategically grouped and sequenced into releases that deliver the greatest incremental value to all parties.","title":"3.4.2.1 Roadmapping"},{"location":"ux/3-4-2-1-roadmapping/#when-to-use","text":"Use roadmapping when the scope of the designed solution looks likely to exceed the capacity of a single development increment (release or similar), or valuable features require additional planning and preparation to be ready to implement.","title":"When to use"},{"location":"ux/3-4-2-1-roadmapping/#requirements","text":"An itemization of designed features to be roadmapped. Stakeholder input on user and business value, and implementation readiness, of itemized features. Participation from all disciplines, from product ownership through the development team, to ensure the targets are realistic.","title":"Requirements"},{"location":"ux/3-4-2-1-roadmapping/#productoutput","text":"The roadmap itself can take many forms, but a common one is a timeline-based chart that reflects the team\u2019s plan for implementation. It shows a target execution period or completion date for each thread of currently identified and prioritized features.","title":"Product/output"},{"location":"ux/3-4-2-1-roadmapping/#resources","text":"Roadmap Basics https://www.productplan.com/roadmap-basics","title":"Resources"},{"location":"ux/3-4-2-2-story/","text":"Back to Phase 2: Conceptual Design 3.4.2.2 Story Writing Story writing is the process of breaking down a feature or group of features that are to be built into discrete, achieveable implementation tasks. In Agile terminology, a story may be also called a Product Backlog Item. The term \u201cstory\u201d is frequently used, however, because of the important notion that these implementation tasks be oriented around the user and achieving value for the user. Thus, the task is very loosely written in the format of a \u201cstory\u201d of which the target user is the main character. Use cases are ideal building blocks for stories. The user\u2019s need becomes the focus of the story. The sequence of actions, and the system functionality necessary to support those actions, form the basis for the story\u2019s acceptance criteria. Product/output A typical user story starts with a statement in the format of \u201cAs a [user type], I need [functionality] so that [goal/task to be accomplished].\u201d From there, additional description can be included, as well as any already defined requirements, conceptual designs for fulfilling the identified user need, and known technological leverage or constraint. As the story matures as is prepped for execution, complete requirements, in the form of acceptance criteria, should be captured and included. These acceptance criteria should form the basis for later quality assurance, including acceptance testing . Practical considerations Stories are best written and reviewed by the entire team, across disciplines. Stories can be split into experience-led and technology-led versions if implementation efficiencies will be gained by doing so. Initially, the story should have little clarity around how the need it raises will be fulfilled. In other words, the story identifies the problem, but not its solution. Resources User Stories: An Agile Introduction http://agilemodeling.com/artifacts/userStory.htm","title":"Story writing"},{"location":"ux/3-4-2-2-story/#3422-story-writing","text":"Story writing is the process of breaking down a feature or group of features that are to be built into discrete, achieveable implementation tasks. In Agile terminology, a story may be also called a Product Backlog Item. The term \u201cstory\u201d is frequently used, however, because of the important notion that these implementation tasks be oriented around the user and achieving value for the user. Thus, the task is very loosely written in the format of a \u201cstory\u201d of which the target user is the main character. Use cases are ideal building blocks for stories. The user\u2019s need becomes the focus of the story. The sequence of actions, and the system functionality necessary to support those actions, form the basis for the story\u2019s acceptance criteria.","title":"3.4.2.2 Story Writing"},{"location":"ux/3-4-2-2-story/#productoutput","text":"A typical user story starts with a statement in the format of \u201cAs a [user type], I need [functionality] so that [goal/task to be accomplished].\u201d From there, additional description can be included, as well as any already defined requirements, conceptual designs for fulfilling the identified user need, and known technological leverage or constraint. As the story matures as is prepped for execution, complete requirements, in the form of acceptance criteria, should be captured and included. These acceptance criteria should form the basis for later quality assurance, including acceptance testing .","title":"Product/output"},{"location":"ux/3-4-2-2-story/#practical-considerations","text":"Stories are best written and reviewed by the entire team, across disciplines. Stories can be split into experience-led and technology-led versions if implementation efficiencies will be gained by doing so. Initially, the story should have little clarity around how the need it raises will be fulfilled. In other words, the story identifies the problem, but not its solution.","title":"Practical considerations"},{"location":"ux/3-4-2-2-story/#resources","text":"User Stories: An Agile Introduction http://agilemodeling.com/artifacts/userStory.htm","title":"Resources"},{"location":"ux/3-4-conceptual/","text":"3.4 Phase 2: Conceptual Design Once the research and discovery work has provided a clear understanding of the problem space and your target users, the next step is translating those insights into a range of testable concepts for a system architecture and initial, low-fidelity interface designs; validating and honing in on the most successful concepts (eventually narrowing to the single best concept); and finally devising a workable strategy for implementation of the selected direction. 3.4.1 Activities for Conceptual Design Wireframing/sketching Using quick, low-fidelity drawings helps explore a broader range of potential solutions in less time. Information architecture Create an organizational structure for your system\u2019s features and content in ways that are most meaningful to your users. Participatory design Collaborating on initial design with subject matter experts and end users reduces risk and rework. Concept testing Test early concepts with actual users to gauge effectiveness. 3.4.2 Methods for Strategic Planning Roadmapping A strategic roadmap provides the team alignment on a plan for implementation. Story writing Break up requirements and conceptual designs into discrete, independently executable pieces.","title":"3.4 Phase 2: Conceptual Design"},{"location":"ux/3-4-conceptual/#34-phase-2-conceptual-design","text":"Once the research and discovery work has provided a clear understanding of the problem space and your target users, the next step is translating those insights into a range of testable concepts for a system architecture and initial, low-fidelity interface designs; validating and honing in on the most successful concepts (eventually narrowing to the single best concept); and finally devising a workable strategy for implementation of the selected direction.","title":"3.4 Phase 2: Conceptual Design"},{"location":"ux/3-4-conceptual/#341-activities-for-conceptual-design","text":"","title":"3.4.1 Activities for Conceptual Design"},{"location":"ux/3-4-conceptual/#wireframingsketching","text":"Using quick, low-fidelity drawings helps explore a broader range of potential solutions in less time.","title":"Wireframing/sketching"},{"location":"ux/3-4-conceptual/#information-architecture","text":"Create an organizational structure for your system\u2019s features and content in ways that are most meaningful to your users.","title":"Information architecture"},{"location":"ux/3-4-conceptual/#participatory-design","text":"Collaborating on initial design with subject matter experts and end users reduces risk and rework.","title":"Participatory design"},{"location":"ux/3-4-conceptual/#concept-testing","text":"Test early concepts with actual users to gauge effectiveness.","title":"Concept testing"},{"location":"ux/3-4-conceptual/#342-methods-for-strategic-planning","text":"","title":"3.4.2 Methods for Strategic Planning"},{"location":"ux/3-4-conceptual/#roadmapping","text":"A strategic roadmap provides the team alignment on a plan for implementation.","title":"Roadmapping"},{"location":"ux/3-4-conceptual/#story-writing","text":"Break up requirements and conceptual designs into discrete, independently executable pieces.","title":"Story writing"},{"location":"ux/3-5-1-1-visualdesign/","text":"Back to Phase 3: Detailed Design 3.5.1.1 High-Fidelity Wireframing & Visual Design High-fidelity wireframing and the creation of visual design comps are activities done to create a fully detailed articulation of the interface (plus all its attendant states necessary to support a functionality). Like low-fidelity wireframes, high-fidelity wireframes are still mostly line renderings representing the interface. But the additional detail at this stage should offer language, labels and fully depicted interactions including success and error cases, and provide guidance for how to handle not just primary use cases but also all edge cases. Visual design comps should offer presentation details such as how individual components should be visually styled, and how the components relate to one another in the interface. Requirements A determination of which conceptual design direction to pursue in detail. A scope of design work to be detailed, in user stories or similar. Product/output High-fidelity wireframes and comps serve as the foundation for communicating the final design. For lean documentation practices and usability testing , these artifacts will serve as the design source for a prototype . They also serve as the source material for writing full developer specifications . Practical considerations Visual design comps can organize the interface differently than the wireframes, so long as there is no contradiction between the two. In cases where the comps do not match the wireframes exactly, the comps should be followed for form and the wireframes should be followed for function. Frequently high-fidelity wireframes and visual design comps are created in parallel, and help inform one another. In this process, the best ideas from each are combined in the final solution. Resources What is Interaction Design? https://www.interaction-design.org/literature/article/what-is-interaction-design Adaptive vs. Responsive Design https://www.interaction-design.org/literature/article/adaptive-vs-responsive-design","title":"High-fidelity wireframing and visual design"},{"location":"ux/3-5-1-1-visualdesign/#3511-high-fidelity-wireframing-visual-design","text":"High-fidelity wireframing and the creation of visual design comps are activities done to create a fully detailed articulation of the interface (plus all its attendant states necessary to support a functionality). Like low-fidelity wireframes, high-fidelity wireframes are still mostly line renderings representing the interface. But the additional detail at this stage should offer language, labels and fully depicted interactions including success and error cases, and provide guidance for how to handle not just primary use cases but also all edge cases. Visual design comps should offer presentation details such as how individual components should be visually styled, and how the components relate to one another in the interface.","title":"3.5.1.1 High-Fidelity Wireframing &amp; Visual Design"},{"location":"ux/3-5-1-1-visualdesign/#requirements","text":"A determination of which conceptual design direction to pursue in detail. A scope of design work to be detailed, in user stories or similar.","title":"Requirements"},{"location":"ux/3-5-1-1-visualdesign/#productoutput","text":"High-fidelity wireframes and comps serve as the foundation for communicating the final design. For lean documentation practices and usability testing , these artifacts will serve as the design source for a prototype . They also serve as the source material for writing full developer specifications .","title":"Product/output"},{"location":"ux/3-5-1-1-visualdesign/#practical-considerations","text":"Visual design comps can organize the interface differently than the wireframes, so long as there is no contradiction between the two. In cases where the comps do not match the wireframes exactly, the comps should be followed for form and the wireframes should be followed for function. Frequently high-fidelity wireframes and visual design comps are created in parallel, and help inform one another. In this process, the best ideas from each are combined in the final solution.","title":"Practical considerations"},{"location":"ux/3-5-1-1-visualdesign/#resources","text":"What is Interaction Design? https://www.interaction-design.org/literature/article/what-is-interaction-design Adaptive vs. Responsive Design https://www.interaction-design.org/literature/article/adaptive-vs-responsive-design","title":"Resources"},{"location":"ux/3-5-1-2-prototyping/","text":"Back to Phase 3: Detailed Design 3.5.1.2 Prototyping A prototype is a simulation of the intended presentation and functionality of the system. It allows user validation prior to committing to build, and demonstrates its intended functionality to the development team. In the detailed design phase, a prototype would typically be based on high-fidelity wireframes or comps. Requirements High-fidelity wireframes or comps to act as source material. Use cases the prototype is meant to simulate. User stories or similar for detailed requirements or acceptance criteria the prototype should follow. Product/output A high-fidelity prototype should be digital and interactive, even if only for highly specific click-paths. The fidelity at this point should be close enough to an exact representation of the intended final product, that testing of the prototype should result in reasonably accurate user performance data (e.g. time to perform a given task). Practical considerations If usability testing goals do not require the prototype to dynamically respond to user inputs, a prototype comprised simply of clickable static interface images (i.e. visual design comps) can suffice. It is generally better to prototype small units of an experience such as key pages, sections or features than to manage one comprehensive prototype. This keeps prototypes manageable and focused, and minimizes need for additional time in refactoring patterns and interactions across a larger whole, especially when the delivery focus is narrower. Reference Prototyping https://www.usability.gov/how-to-and-tools/methods/prototyping.html","title":"Prototyping"},{"location":"ux/3-5-1-2-prototyping/#3512-prototyping","text":"A prototype is a simulation of the intended presentation and functionality of the system. It allows user validation prior to committing to build, and demonstrates its intended functionality to the development team. In the detailed design phase, a prototype would typically be based on high-fidelity wireframes or comps.","title":"3.5.1.2 Prototyping"},{"location":"ux/3-5-1-2-prototyping/#requirements","text":"High-fidelity wireframes or comps to act as source material. Use cases the prototype is meant to simulate. User stories or similar for detailed requirements or acceptance criteria the prototype should follow.","title":"Requirements"},{"location":"ux/3-5-1-2-prototyping/#productoutput","text":"A high-fidelity prototype should be digital and interactive, even if only for highly specific click-paths. The fidelity at this point should be close enough to an exact representation of the intended final product, that testing of the prototype should result in reasonably accurate user performance data (e.g. time to perform a given task).","title":"Product/output"},{"location":"ux/3-5-1-2-prototyping/#practical-considerations","text":"If usability testing goals do not require the prototype to dynamically respond to user inputs, a prototype comprised simply of clickable static interface images (i.e. visual design comps) can suffice. It is generally better to prototype small units of an experience such as key pages, sections or features than to manage one comprehensive prototype. This keeps prototypes manageable and focused, and minimizes need for additional time in refactoring patterns and interactions across a larger whole, especially when the delivery focus is narrower.","title":"Practical considerations"},{"location":"ux/3-5-1-2-prototyping/#reference","text":"Prototyping https://www.usability.gov/how-to-and-tools/methods/prototyping.html","title":"Reference"},{"location":"ux/3-5-1-3-usability/","text":"Back to Phase 3: Detailed Design 3.5.1.3 Usability Testing Compared to earlier concept testing , the testing done at this phase should not be centered on exploring solutions so much as refining and optimizing specifics. In much the same fashion as earlier testing, however, participants should be given scenarios or tasks, and asked to complete them through use of the prototyped interfaces, while also thinking aloud to express their thoughts and feelings of the experience as they proceed. Requirements A test script based on real scenarios or use cases . A prototype that allows for the completion of the tasks in the test script. The ability to recruit target end users to participate. An ability to record the tests, or to take careful notes during the sessions. Variations Moderated versus unmoderated With moderated testing, a moderator sits with the participant to guide the session. This involves giving the participant an introduction to the test and its goals, then giving tasks to the tester as the session progresses, prompting the user to explain their thoughts and actions, and helping prevent the tester from getting completely off track. In this method, the moderator must be careful to avoid offering any guidance on how to complete the test tasks, even if the tester begs off a difficult task, until the entire test is done. If the tester has veered so far off as to make their continuation unproductive, the moderator may elect to gently direct the tester to start over or move to the next task. The moderator must also be extremely careful not to ask leading or biased questions about the design in an attempt to elicit specific feedback. The moderator should exhibit neutral attitudes regarding the design and the test tasks. In unmoderated testing, the efficacy of the test relies on the participants to clearly understand the goals of the test and their instructions for properly participating, including a desire to have them continuously thinking aloud for the duration of the session. Because that behavior is fairly unnatural, it is easily forgotten about when attempting a particularly challenging task, which is exactly when that vocalization is most beneficial to the design team. Unmoderated testing is typically done via an online testing tool, where the test can be done at the participant\u2019s convenience on their own computer, which makes participation easier. Formal versus informal/guerilla In formal testing, the test is conducted in a lab setting with a moderator on hand to lead the sessions. In highly formal lab settings, there is frequently an observation area separated from the test area by a one-way mirror. This keeps the participant and moderator able to focus on the test, while allowing stakeholders and other team members to observe the sessions firsthand. Formal settings often have advanced technologies for recording the test, including video of the participant, screen capture of the participant\u2019s on-screen actions, and even eye tracking. In this way, formal tests can yield highly nuanced findings. Informal or guerilla-style testing is conducted in any environment where testers can find participants. This can be simply at the participant\u2019s desk, or a conference room. The test need not be conducted in a highly controlled setting in order to elicit meaningful feedback. Product/output Ideally, your setup allows for a recording of the test sessions (most importantly audio recording and screen capture) for later analysis and detailed reporting. If that is not possible, having someone available to take notes during the sessions, such as key quotes from the testers and details around specific difficulties, will suffice. The findings should inform iterations of the detailed designs and prototype, and a repeat of testing until the designs satisfy user needs. Practical considerations Make sure the test participants are made aware that they are not being tested \u2013 the effectiveness of the design is. Any difficulty the participant has completing the test script is a reflection of a deficient design, not of their abilities. If recording is not available, make sure there is a second person available to take notes. The moderator should be able to focus on the participant and their test tasks, not on documenting findings. References & Resources References Running a Usability Test https://www.usability.gov/how-to-and-tools/methods/running-usability-tests.html Complete Beginner\u2019s Guide to UX Research https://www.uxbooth.com/articles/complete-beginners-guide-to-design-research Resources Selecting an Online Tool for Unmoderated Remote User Testing https://www.nngroup.com/articles/unmoderated-user-testing-tools","title":"Usability testing"},{"location":"ux/3-5-1-3-usability/#3513-usability-testing","text":"Compared to earlier concept testing , the testing done at this phase should not be centered on exploring solutions so much as refining and optimizing specifics. In much the same fashion as earlier testing, however, participants should be given scenarios or tasks, and asked to complete them through use of the prototyped interfaces, while also thinking aloud to express their thoughts and feelings of the experience as they proceed.","title":"3.5.1.3 Usability Testing"},{"location":"ux/3-5-1-3-usability/#requirements","text":"A test script based on real scenarios or use cases . A prototype that allows for the completion of the tasks in the test script. The ability to recruit target end users to participate. An ability to record the tests, or to take careful notes during the sessions.","title":"Requirements"},{"location":"ux/3-5-1-3-usability/#variations","text":"","title":"Variations"},{"location":"ux/3-5-1-3-usability/#moderated-versus-unmoderated","text":"With moderated testing, a moderator sits with the participant to guide the session. This involves giving the participant an introduction to the test and its goals, then giving tasks to the tester as the session progresses, prompting the user to explain their thoughts and actions, and helping prevent the tester from getting completely off track. In this method, the moderator must be careful to avoid offering any guidance on how to complete the test tasks, even if the tester begs off a difficult task, until the entire test is done. If the tester has veered so far off as to make their continuation unproductive, the moderator may elect to gently direct the tester to start over or move to the next task. The moderator must also be extremely careful not to ask leading or biased questions about the design in an attempt to elicit specific feedback. The moderator should exhibit neutral attitudes regarding the design and the test tasks. In unmoderated testing, the efficacy of the test relies on the participants to clearly understand the goals of the test and their instructions for properly participating, including a desire to have them continuously thinking aloud for the duration of the session. Because that behavior is fairly unnatural, it is easily forgotten about when attempting a particularly challenging task, which is exactly when that vocalization is most beneficial to the design team. Unmoderated testing is typically done via an online testing tool, where the test can be done at the participant\u2019s convenience on their own computer, which makes participation easier.","title":"Moderated versus unmoderated"},{"location":"ux/3-5-1-3-usability/#formal-versus-informalguerilla","text":"In formal testing, the test is conducted in a lab setting with a moderator on hand to lead the sessions. In highly formal lab settings, there is frequently an observation area separated from the test area by a one-way mirror. This keeps the participant and moderator able to focus on the test, while allowing stakeholders and other team members to observe the sessions firsthand. Formal settings often have advanced technologies for recording the test, including video of the participant, screen capture of the participant\u2019s on-screen actions, and even eye tracking. In this way, formal tests can yield highly nuanced findings. Informal or guerilla-style testing is conducted in any environment where testers can find participants. This can be simply at the participant\u2019s desk, or a conference room. The test need not be conducted in a highly controlled setting in order to elicit meaningful feedback.","title":"Formal versus informal/guerilla"},{"location":"ux/3-5-1-3-usability/#productoutput","text":"Ideally, your setup allows for a recording of the test sessions (most importantly audio recording and screen capture) for later analysis and detailed reporting. If that is not possible, having someone available to take notes during the sessions, such as key quotes from the testers and details around specific difficulties, will suffice. The findings should inform iterations of the detailed designs and prototype, and a repeat of testing until the designs satisfy user needs.","title":"Product/output"},{"location":"ux/3-5-1-3-usability/#practical-considerations","text":"Make sure the test participants are made aware that they are not being tested \u2013 the effectiveness of the design is. Any difficulty the participant has completing the test script is a reflection of a deficient design, not of their abilities. If recording is not available, make sure there is a second person available to take notes. The moderator should be able to focus on the participant and their test tasks, not on documenting findings.","title":"Practical considerations"},{"location":"ux/3-5-1-3-usability/#references-resources","text":"","title":"References &amp; Resources"},{"location":"ux/3-5-1-3-usability/#references","text":"Running a Usability Test https://www.usability.gov/how-to-and-tools/methods/running-usability-tests.html Complete Beginner\u2019s Guide to UX Research https://www.uxbooth.com/articles/complete-beginners-guide-to-design-research","title":"References"},{"location":"ux/3-5-1-3-usability/#resources","text":"Selecting an Online Tool for Unmoderated Remote User Testing https://www.nngroup.com/articles/unmoderated-user-testing-tools","title":"Resources"},{"location":"ux/3-5-1-4-annotating/","text":"Back to Phase 3: Detailed Design 3.5.1.4 Specifying/Annotating In order to properly support development, all possible states of the interface should be depicted in wireframe or comp, or described in detailed specifications. These specifications and annotations are an opportunity to describe the design\u2019s intent beyond what can be conveyed in a static wireframe of visual design comp alone. This typically includes details like dynamic language, how business rules drive presentation, or exact details on how an interaction should function. Requirements High-fidelity wireframes or visual design comps to start from. Product/output Design specifications are most useful when presented side-by-side with the detailed design artifacts (wireframes or comps). Typically, a numbered marker is placed on top of the artifact, that references its respective annotation. Practical considerations In Agile and Lean environments, verbal and collaborative communication is favored to extensive documentation like fully annotated wireframes. Even so, some level of specification is still generally necessary to fully convey the designs for implementation. Resource Wireframing \u2013 The Perfectionist's Guide https://www.smashingmagazine.com/2016/11/wireframe-perfectionist-guide/#annotating-the-wireframes","title":"Specifying/annotating"},{"location":"ux/3-5-1-4-annotating/#3514-specifyingannotating","text":"In order to properly support development, all possible states of the interface should be depicted in wireframe or comp, or described in detailed specifications. These specifications and annotations are an opportunity to describe the design\u2019s intent beyond what can be conveyed in a static wireframe of visual design comp alone. This typically includes details like dynamic language, how business rules drive presentation, or exact details on how an interaction should function.","title":"3.5.1.4 Specifying/Annotating"},{"location":"ux/3-5-1-4-annotating/#requirements","text":"High-fidelity wireframes or visual design comps to start from.","title":"Requirements"},{"location":"ux/3-5-1-4-annotating/#productoutput","text":"Design specifications are most useful when presented side-by-side with the detailed design artifacts (wireframes or comps). Typically, a numbered marker is placed on top of the artifact, that references its respective annotation.","title":"Product/output"},{"location":"ux/3-5-1-4-annotating/#practical-considerations","text":"In Agile and Lean environments, verbal and collaborative communication is favored to extensive documentation like fully annotated wireframes. Even so, some level of specification is still generally necessary to fully convey the designs for implementation.","title":"Practical considerations"},{"location":"ux/3-5-1-4-annotating/#resource","text":"Wireframing \u2013 The Perfectionist's Guide https://www.smashingmagazine.com/2016/11/wireframe-perfectionist-guide/#annotating-the-wireframes","title":"Resource"},{"location":"ux/3-5-detailed/","text":"3.5 Phase 3: Detailed Design This phase involves building out the conceptual design into fully detailed designs that encompass the entirety of the interfaces and interactions within a given scope of requirements. The final product should be development-ready, including enough specification to explain the how the user may interact with dynamic portions of the designs, and how the designs satisfy their intended requirements or user stories. 3.5.1 Activities for Detailing Designs High-fidelity wireframing and visual design Fully detailed renderings of the system\u2019s relevant interfaces help clearly articulate the system and its capabilities. Prototyping A simulation of the intended presentation and functionality of the system allows user validation prior to committing to build, and demonstrates this intended functionality to the development team. Usability testing Assessing the detailed design at this stage allows the team to more nimbly correct course and enhance the experience prior to development. Specifying/annotating Provide additional details to the development team to explain all of the ways a user can interact with the designs.","title":"3.5 Phase 3: Detailed Design"},{"location":"ux/3-5-detailed/#35-phase-3-detailed-design","text":"This phase involves building out the conceptual design into fully detailed designs that encompass the entirety of the interfaces and interactions within a given scope of requirements. The final product should be development-ready, including enough specification to explain the how the user may interact with dynamic portions of the designs, and how the designs satisfy their intended requirements or user stories.","title":"3.5 Phase 3: Detailed Design"},{"location":"ux/3-5-detailed/#351-activities-for-detailing-designs","text":"","title":"3.5.1 Activities for Detailing Designs"},{"location":"ux/3-5-detailed/#high-fidelity-wireframing-and-visual-design","text":"Fully detailed renderings of the system\u2019s relevant interfaces help clearly articulate the system and its capabilities.","title":"High-fidelity wireframing and visual design"},{"location":"ux/3-5-detailed/#prototyping","text":"A simulation of the intended presentation and functionality of the system allows user validation prior to committing to build, and demonstrates this intended functionality to the development team.","title":"Prototyping"},{"location":"ux/3-5-detailed/#usability-testing","text":"Assessing the detailed design at this stage allows the team to more nimbly correct course and enhance the experience prior to development.","title":"Usability testing"},{"location":"ux/3-5-detailed/#specifyingannotating","text":"Provide additional details to the development team to explain all of the ways a user can interact with the designs.","title":"Specifying/annotating"},{"location":"ux/3-6-support/","text":"3.6 Phase 4: Support User experience involvement should not end at a hand-off to the development team. Continuing to participate in the development, deployment and post-delivery support periods is crucial to ensuring the delivered experience matches the delivered designs and meets stakeholder and end-user expectations. 3.6.1 Activities in Post-Delivery Support Developer communication and design adjustment Collaboration with the development team during build helps cover missed edge cases or ambiguous requirements, and ensures that detailed designs are built as specified. Acceptance testing The product ownership team, including the user experience team, should thoroughly test the built capabilities prior to live deployment. The acceptance criteria collected in user stories serve well as an ad hoc test script. Post-launch, conducting acceptance testing with actual end users serves as another validation of the release\u2019s designs as well as research for future iterations or new additional features.","title":"3.6 Phase 4: Support"},{"location":"ux/3-6-support/#36-phase-4-support","text":"User experience involvement should not end at a hand-off to the development team. Continuing to participate in the development, deployment and post-delivery support periods is crucial to ensuring the delivered experience matches the delivered designs and meets stakeholder and end-user expectations.","title":"3.6 Phase 4: Support"},{"location":"ux/3-6-support/#361-activities-in-post-delivery-support","text":"","title":"3.6.1 Activities in Post-Delivery Support"},{"location":"ux/3-6-support/#developer-communication-and-design-adjustment","text":"Collaboration with the development team during build helps cover missed edge cases or ambiguous requirements, and ensures that detailed designs are built as specified.","title":"Developer communication and design adjustment"},{"location":"ux/3-6-support/#acceptance-testing","text":"The product ownership team, including the user experience team, should thoroughly test the built capabilities prior to live deployment. The acceptance criteria collected in user stories serve well as an ad hoc test script. Post-launch, conducting acceptance testing with actual end users serves as another validation of the release\u2019s designs as well as research for future iterations or new additional features.","title":"Acceptance testing"},{"location":"ux/4-1-introduction/","text":"4.1 Introduction Welcome The visual design section of the BES User Experience Playbook catalogues and describes the best practices of digital visual design, and how they inform logistics information application user experiences. These best practices are illustrated with examples from existing USAF applications, as well as \"hybrid\u201d components where necessary to provide a unique visualization. Purpose This is not a style guide . This content is intended as a teaching tool, to complement technical documentation from other design and development playbooks. The visualizations are not intended as redesigns of existing or new applications, but rather guidelines for making decisions within the context of your own design systems. When designing specific user interface components, first consult your application\u2019s style guide, then refer to this playbook should any questions remain. This content also exists as a living document. As USAF applications, best practices, and use cases continue to evolve, so too will this document and the examples herein.","title":"4.1 Introduction"},{"location":"ux/4-1-introduction/#41-introduction","text":"","title":"4.1 Introduction"},{"location":"ux/4-1-introduction/#welcome","text":"The visual design section of the BES User Experience Playbook catalogues and describes the best practices of digital visual design, and how they inform logistics information application user experiences. These best practices are illustrated with examples from existing USAF applications, as well as \"hybrid\u201d components where necessary to provide a unique visualization.","title":"Welcome"},{"location":"ux/4-1-introduction/#purpose","text":"This is not a style guide . This content is intended as a teaching tool, to complement technical documentation from other design and development playbooks. The visualizations are not intended as redesigns of existing or new applications, but rather guidelines for making decisions within the context of your own design systems. When designing specific user interface components, first consult your application\u2019s style guide, then refer to this playbook should any questions remain. This content also exists as a living document. As USAF applications, best practices, and use cases continue to evolve, so too will this document and the examples herein.","title":"Purpose"},{"location":"ux/4-10-imagery/","text":"4.10 Imagery Worth 1000 Words Images are a highly effective way to communicate information and evoke emotions. In more editorial layouts, images are useful for breaking up copy and reinforcing narrative points. In data visualizations (a matrix of aircraft types, for instance), they can improve scannability and actually increase the efficiency of a platform. Imagery Images aren\u2019t just decoration, but tools for encouraging certain user behavior and response. Whether for orientation or texture, they should follow best practices: Images should have purpose . Though it may be tempting to \u201cdecorate\u201d a text- or data-heavy page, introducing purely decorative images may inadvertently distract from key user tasks and unnecessarily increase the page\u2019s file weight. Images should do what could not be done otherwise, or done more efficiently than if done in text. Reinforce the user experience . The aesthetic of a layout should serve the intent of the UX design, particularly when images can be used as a shorthand in the display of information. Direct the eye toward key layout elements . The subject and composition of an image can help move the eye toward important components, such as mandatory fields and calls-to-action. Human subjects in images should \u201clook at\u201d those components, rather than looking away. Likewise, jets should \u201cfly toward,\u201d lines should \u201cpoint,\u201d etc. Illustrate concepts . The most useful images illustrate concepts too cumbersome to put into words. If text describes at length something in terms of its appearance, that\u2019s often a cue to use an image instead. Avoid placing key copy in an image . From a technical perspective, text in an image introduces an accessibility risk. It is unrecognizable by automated screen readers, and may be made illegible during the image optimization process. It also eliminates searchability and copy-paste functionality. There are exceptions, of course, such as in cases of inline labels and platform limitations. Naturally, images should also follow best practices of contrast, composition, and standards set by USAF design guidelines. Formatting and Optimizing With a few exceptions (such as .SVGs for icons) virtually all digital image assets will be in the RGB raster formats of .JPG, .PNG, and .GIF, which each have slightly different properties and merits. In short: .JPG formats compress image information to a very small file size, but at the cost of a permanent loss in image quality (a \u201clossy\u201d image that results in more noticeable .JPG \u201cartifacts\u201d the smaller the file size). .PNG formats compress image information without a loss in image quality, and also support a transparency layer \u2013 useful for placing the image atop textured or colored backgrounds. .PNGs are most useful for web / digital formats but tend to have a larger file size. .GIF formats compress image information by reducing the total number of colors. .GIFs support transparency, and also multi-frame animations. As an older technology, they are usually only used in limited circumstances. When evaluating your optimization method, consider the nature of the image and most important characteristics. Is the priority the crispness / quality of the image? The smallest possible file size? Is there a production requirement that requires transparency? In what context and on what device will the user be viewing the image? When in doubt, attempt to optimize for the smallest possible file size without an obvious loss in visual quality. References Web Fundamentals from Google https://developers.google.com/web/fundamentals/performance/optimizing-content-efficiency/image-optimization Image Optimizers https://tinypng.com https://kraken.io/web-interface","title":"4.10 Imagery"},{"location":"ux/4-10-imagery/#410-imagery","text":"","title":"4.10 Imagery"},{"location":"ux/4-10-imagery/#worth-1000-words","text":"Images are a highly effective way to communicate information and evoke emotions. In more editorial layouts, images are useful for breaking up copy and reinforcing narrative points. In data visualizations (a matrix of aircraft types, for instance), they can improve scannability and actually increase the efficiency of a platform.","title":"Worth 1000 Words"},{"location":"ux/4-10-imagery/#imagery","text":"Images aren\u2019t just decoration, but tools for encouraging certain user behavior and response. Whether for orientation or texture, they should follow best practices: Images should have purpose . Though it may be tempting to \u201cdecorate\u201d a text- or data-heavy page, introducing purely decorative images may inadvertently distract from key user tasks and unnecessarily increase the page\u2019s file weight. Images should do what could not be done otherwise, or done more efficiently than if done in text. Reinforce the user experience . The aesthetic of a layout should serve the intent of the UX design, particularly when images can be used as a shorthand in the display of information. Direct the eye toward key layout elements . The subject and composition of an image can help move the eye toward important components, such as mandatory fields and calls-to-action. Human subjects in images should \u201clook at\u201d those components, rather than looking away. Likewise, jets should \u201cfly toward,\u201d lines should \u201cpoint,\u201d etc. Illustrate concepts . The most useful images illustrate concepts too cumbersome to put into words. If text describes at length something in terms of its appearance, that\u2019s often a cue to use an image instead. Avoid placing key copy in an image . From a technical perspective, text in an image introduces an accessibility risk. It is unrecognizable by automated screen readers, and may be made illegible during the image optimization process. It also eliminates searchability and copy-paste functionality. There are exceptions, of course, such as in cases of inline labels and platform limitations. Naturally, images should also follow best practices of contrast, composition, and standards set by USAF design guidelines.","title":"Imagery"},{"location":"ux/4-10-imagery/#formatting-and-optimizing","text":"With a few exceptions (such as .SVGs for icons) virtually all digital image assets will be in the RGB raster formats of .JPG, .PNG, and .GIF, which each have slightly different properties and merits. In short: .JPG formats compress image information to a very small file size, but at the cost of a permanent loss in image quality (a \u201clossy\u201d image that results in more noticeable .JPG \u201cartifacts\u201d the smaller the file size). .PNG formats compress image information without a loss in image quality, and also support a transparency layer \u2013 useful for placing the image atop textured or colored backgrounds. .PNGs are most useful for web / digital formats but tend to have a larger file size. .GIF formats compress image information by reducing the total number of colors. .GIFs support transparency, and also multi-frame animations. As an older technology, they are usually only used in limited circumstances. When evaluating your optimization method, consider the nature of the image and most important characteristics. Is the priority the crispness / quality of the image? The smallest possible file size? Is there a production requirement that requires transparency? In what context and on what device will the user be viewing the image? When in doubt, attempt to optimize for the smallest possible file size without an obvious loss in visual quality.","title":"Formatting and Optimizing"},{"location":"ux/4-10-imagery/#references","text":"Web Fundamentals from Google https://developers.google.com/web/fundamentals/performance/optimizing-content-efficiency/image-optimization Image Optimizers https://tinypng.com https://kraken.io/web-interface","title":"References"},{"location":"ux/4-11-buttons/","text":"4.11 Buttons & Controls Creating Custom-Styled Controls Buttons and controls, in the context of this design playbook, are key interactive elements that are custom-styled to match the look & feel of a USAF application. There are many other types of controls (radio buttons, inline scrolls, dropdown menus, etc.), but these most frequently adopt the style of the browser itself, and are thus excluded. Additional styled elements (like navigation and breadcrumbs) can be found within the Component Library of this playbook. Buttons Buttons are a key component of interaction design, and are often the primary call-to-action at the page and component level. They usually \u201ccommit\u201d the action of a user \u2013 submitting a form or piece of data, opening / closing a page element, or navigating to a new content section. Buttons should adhere to the following best practices: Buttons should look like buttons. Unlike links \u2013 which are often just highlighted text \u2013 buttons employ a containing element (usually a square or rounded rectangle) to look like the buttons of the physical world. This helps increase their visual emphasis in a layout, and communicate their clickability. Buttons should be findable. The button location should be logical in relation to the actions of the layout. A \u201csubmit\u201d button, for instance, should be readily visible as the user is completing the last required field of a form. A \u201cnext\u201d button should live near the end of a content section (or universally accessible, if the content is intended to be scanned and not read to completion). Buttons placement should never be a hunting game. Buttons should communicate their action. Button labels should be clear and obvious, indicating the behavior of the button. \u201cSubmit form,\u201d \u201cLearn More,\u201d and \u201cNext\u201d articulate the intention of the button, and relate directly to the content around it. Furthermore, combining buttons labels and buttons styles \u2013 such as in the case with a \u201cCancel\u201d button and an adjacent \u201cOK\u201d button -- enhances the clarity of each button\u2019s action. Button Emphasis & States Buttons often come in primary and secondary styles, both of which follow call-to-action color rules but are treated differently to communicate priority and emphasis. In this case, a primary button style might be used to submit a form, while a secondary button style might be used to cancel the form submission. As interactive elements, buttons often have multiple states to communicate to the user the status of the button. Commonly these states are active, hover (on desktop) / clicked, and disabled. The disabled button state is particularly useful for compelling the user to provide all mandatory data before they can proceed; once all form fields have been completed, the button changes to an active \u2013 thus clickable \u2013 state. Styling custom buttons should follow your application\u2019s design guidelines, and the rules of contrast, legibility, and color mentioned elsewhere in this playbook. The basic button states follow a complementary logic: The active button state reflects the call-to-action color, improving the obviousness of its function. The hover state is a slightly lighter or darker than the active state, further communicating that it is an interactive element. The disabled state is desaturated and reduced in opacity; ideally, it is just visible enough to discern as a button, but not distract from the other active page elements. Button Feedback As a key site interaction, users often want validation that their button-click has performed the intended operation. In some cases, loading the subsequent page will be an adequate response to clicking \u201cLearn More.\u201d In others \u2013 such as when submitting a form \u2013 additional validation may be required. In these cases, consider confirmation and error message styles that are unique and explicit. These feedback elements are a perfect example of the interrelatedness of UX and visual design. Tabs Tabs are a common interactive element \u2013 especially within data tables \u2013 and communicate lateral movement between content of like categories. They are useful in navigation, and also in comparing similar content types on separate layouts. Combine in like categories . Tabbed items should live at the same hierarchical level with one another. They should also have mutually exclusive content. For example, if the tab category is \u201cmedia,\u201d individual tabs could be \u201cMusic,\u201d \u201cMovies,\u201d and \u201cBooks.\u201d Arrange tabs in a logical order . The first (leftmost) tab in the set is usually the default tab, and thus sets the standard for content and behavior of other tabs. This tab should also feature your prioritized content, with less prioritized content as the user moves right. There are some exceptions, of course, such as in the case of tabs that are organized alphabetically or numerically. Mind horizontal space . Since tabs are usually bound by screen width (other than an infinite vertical scroll), your tab solution should be meaningful at the given device\u2019s viewport. That might mean a scalable tab set that goes \u201coffscreen,\u201d using truncated text, or switching from text to icons when users reach the mobile breakpoint. Tabs must maintain their meaning and scannability, at any scale. Similar to buttons and navigation elements, tabs often have states to communicate their status. References Button Design Best Practices https://uxplanet.org/7-basic-rules-for-button-design-63dcdf5676b4 https://www.smashingmagazine.com/2016/11/a-quick-guide-for-designing-better-buttons https://material.io/design/components/buttons.html Table Design Best Practices https://uxplanet.org/tabs-for-mobile-ux-design-d4cc4d9410d1 https://www.justinmind.com/blog/8-tips-to-get-tabs-right-in-web-design https://material.io/design/components/tabs.html","title":"4.11 Buttons & Controls"},{"location":"ux/4-11-buttons/#411-buttons-controls","text":"","title":"4.11 Buttons &amp; Controls"},{"location":"ux/4-11-buttons/#creating-custom-styled-controls","text":"Buttons and controls, in the context of this design playbook, are key interactive elements that are custom-styled to match the look & feel of a USAF application. There are many other types of controls (radio buttons, inline scrolls, dropdown menus, etc.), but these most frequently adopt the style of the browser itself, and are thus excluded. Additional styled elements (like navigation and breadcrumbs) can be found within the Component Library of this playbook.","title":"Creating Custom-Styled Controls"},{"location":"ux/4-11-buttons/#buttons","text":"Buttons are a key component of interaction design, and are often the primary call-to-action at the page and component level. They usually \u201ccommit\u201d the action of a user \u2013 submitting a form or piece of data, opening / closing a page element, or navigating to a new content section. Buttons should adhere to the following best practices: Buttons should look like buttons. Unlike links \u2013 which are often just highlighted text \u2013 buttons employ a containing element (usually a square or rounded rectangle) to look like the buttons of the physical world. This helps increase their visual emphasis in a layout, and communicate their clickability. Buttons should be findable. The button location should be logical in relation to the actions of the layout. A \u201csubmit\u201d button, for instance, should be readily visible as the user is completing the last required field of a form. A \u201cnext\u201d button should live near the end of a content section (or universally accessible, if the content is intended to be scanned and not read to completion). Buttons placement should never be a hunting game. Buttons should communicate their action. Button labels should be clear and obvious, indicating the behavior of the button. \u201cSubmit form,\u201d \u201cLearn More,\u201d and \u201cNext\u201d articulate the intention of the button, and relate directly to the content around it. Furthermore, combining buttons labels and buttons styles \u2013 such as in the case with a \u201cCancel\u201d button and an adjacent \u201cOK\u201d button -- enhances the clarity of each button\u2019s action.","title":"Buttons"},{"location":"ux/4-11-buttons/#button-emphasis-states","text":"Buttons often come in primary and secondary styles, both of which follow call-to-action color rules but are treated differently to communicate priority and emphasis. In this case, a primary button style might be used to submit a form, while a secondary button style might be used to cancel the form submission. As interactive elements, buttons often have multiple states to communicate to the user the status of the button. Commonly these states are active, hover (on desktop) / clicked, and disabled. The disabled button state is particularly useful for compelling the user to provide all mandatory data before they can proceed; once all form fields have been completed, the button changes to an active \u2013 thus clickable \u2013 state. Styling custom buttons should follow your application\u2019s design guidelines, and the rules of contrast, legibility, and color mentioned elsewhere in this playbook. The basic button states follow a complementary logic: The active button state reflects the call-to-action color, improving the obviousness of its function. The hover state is a slightly lighter or darker than the active state, further communicating that it is an interactive element. The disabled state is desaturated and reduced in opacity; ideally, it is just visible enough to discern as a button, but not distract from the other active page elements.","title":"Button Emphasis &amp; States"},{"location":"ux/4-11-buttons/#button-feedback","text":"As a key site interaction, users often want validation that their button-click has performed the intended operation. In some cases, loading the subsequent page will be an adequate response to clicking \u201cLearn More.\u201d In others \u2013 such as when submitting a form \u2013 additional validation may be required. In these cases, consider confirmation and error message styles that are unique and explicit. These feedback elements are a perfect example of the interrelatedness of UX and visual design.","title":"Button Feedback"},{"location":"ux/4-11-buttons/#tabs","text":"Tabs are a common interactive element \u2013 especially within data tables \u2013 and communicate lateral movement between content of like categories. They are useful in navigation, and also in comparing similar content types on separate layouts. Combine in like categories . Tabbed items should live at the same hierarchical level with one another. They should also have mutually exclusive content. For example, if the tab category is \u201cmedia,\u201d individual tabs could be \u201cMusic,\u201d \u201cMovies,\u201d and \u201cBooks.\u201d Arrange tabs in a logical order . The first (leftmost) tab in the set is usually the default tab, and thus sets the standard for content and behavior of other tabs. This tab should also feature your prioritized content, with less prioritized content as the user moves right. There are some exceptions, of course, such as in the case of tabs that are organized alphabetically or numerically. Mind horizontal space . Since tabs are usually bound by screen width (other than an infinite vertical scroll), your tab solution should be meaningful at the given device\u2019s viewport. That might mean a scalable tab set that goes \u201coffscreen,\u201d using truncated text, or switching from text to icons when users reach the mobile breakpoint. Tabs must maintain their meaning and scannability, at any scale. Similar to buttons and navigation elements, tabs often have states to communicate their status.","title":"Tabs"},{"location":"ux/4-11-buttons/#references","text":"Button Design Best Practices https://uxplanet.org/7-basic-rules-for-button-design-63dcdf5676b4 https://www.smashingmagazine.com/2016/11/a-quick-guide-for-designing-better-buttons https://material.io/design/components/buttons.html Table Design Best Practices https://uxplanet.org/tabs-for-mobile-ux-design-d4cc4d9410d1 https://www.justinmind.com/blog/8-tips-to-get-tabs-right-in-web-design https://material.io/design/components/tabs.html","title":"References"},{"location":"ux/4-12-mobile/","text":"4.12 Key Mobile Standards Designing for In-Hand and On-the-Go \u201cMobile first\u201d is a design mantra that helps remind us that an ever-increasing percentage of platform access is via mobile device (both phone and tablet). In some cases, mobile devices make up the majority of access. So while we can\u2019t yet dismiss the importance of desktop devices, it\u2019s a best practice to remember that our users first reach for the computers in their pockets. Mobile is Different Most visual design principles are universal, and applicable across devices. The elements of type and color and imagery employ the same best practices. But two key factors (and many smaller ones) require slightly different design thinking, and a real consideration of user experience. The device is in-hand . Mobile devices have small viewports, obviously. That smaller screen size means that longer copy and larger interactive elements need to be treated with a sort of shorthand. An icon, perhaps, where before there was icon and text. That viewport means a smaller keyboard with smaller keys. They\u2019re often thumb-typed, making long form entry uncomfortable. And to just shrink interactive elements fails the user \u2013 often resulted in the dreaded \u201cfat finger\u201d mis-tap. The user is on-the-go . Mobile access often indicates that the user isn\u2019t at their workstation, is in a remote working environment, or is even working \u201cmulti-screen.\u201d They may be using cellular data and thus reluctant to download large files, if they\u2019re able to at all. They may be in an \u201cin-between time,\u201d waiting in line or walking to their next meeting, hoping for the most efficient task completion, without frills. When translating desktop designs to mobile, don\u2019t just miniaturize it \u2013 consider how the device and context should influence changes. Other mobile best practices include: Hide and reveal content . Text truncation, \u201cshow more\u201d buttons, and accordion controls increase the scannability of a mobile page, and allow the user to reveal content only on demand. Additionally, menus, headers, and footers can hide when scrolling away, and reveal when scrolling toward \u2013 reclaiming that visual real estate for higher priority elements. Remove unnecessary ornamentation . A beautiful brand image might work on a desktop design (and over reliable wifi), but on mobile is a waste of bandwidth. Keep only those images that orient and communicate critical information \u2013 and optimize those to the viewport of your target devices. Embrace scrolling . The most predictable, responsive, mobile designs translate desktop layouts into long, scrollable pages. Components should be prioritized from top to bottom in a logical linear order, and when possible provide an affordance for the component that follows. Test vigorously ! Minor differences between devices tend to disrupt pixel-precise mobile designs. The most reliable way to assure a predictable user experience is to test on your user\u2019s most common devices, within the browser of that device. If the actual phones / tables are unavailable, device emulators and browser plugins can still remove some of the guesswork. Phone vs. Tablet Differences Remember that a mobile device isn\u2019t just a phone. Scalability across devices, more specifically between a phone and tablet, is a common challenge among designers. While the phone and tablet share many similarities, users use tablets, phones, phablets (not small enough to be a phone, not big enough to be a tablet) very differently. Phone Interfaces Mobile interfaces less than 7 inches width should be treated as a phone. Layout should be aligned across these devices as much as possible, and should also leverage native platform guidelines and capabilities when possible. Mobile phone designs should include only necessary information . The phone is a convenient way to consume information on the go. Airmen use a phone to complete quick actions while they are not at their workspace, capture data in the field, view content, and perhaps return to look later on a larger device. Tablet Interfaces Mobile interfaces greater 7 inches width should be treated as a tablet. Layout should be aligned across these devices as much as possible; they do not need to align to phone interfaces. Tablet designs should look and feel like desktop web, but function like a phone (tap / swipe / hold gestures, etc.). Tablets are more likely to be held in landscape view \u2013 approximating the desktop viewport \u2013 and in this way, many users consider the tablet a hybrid device. \u201cHit Area\u201d and Button Design Unlike the pixel-precise cursor on desktop, mobile interactions rely on human fingers of vastly different sizes. The rule of thumb (!) is that any clickable element should be 48px x 48px (accounting for 2x or greater design if necessary) to accommodate the average fingertip. This hit area should also have adequate padding around it; immediately adjacent clickable elements are not recommended. Native Device Capabilities While most USAF platforms are web-based application viewed within the mobile device browser, native OS design (mobile apps) unlocks the potential of an integrated device. Smart phones and tablets have a lot to offer: touch, voice, pressure, location tracking, accelerometer, notifications, etc. You are designing around the device, the platform, the user experience. How can these device features be utilized in products? How can the mobile device benefit users beyond the screen interface in front of them? When designing for native platforms, however, consistently refer to the native OS design guidelines. These constantly evolve with new version releases and system redesigns, so it\u2019s always good practice to stay on top of these guidelines and refresh your memory and knowledge often. References Apple\u2019s Human Interface Guidelines https://developer.apple.com/ios/human-interface-guidelines Google\u2019s Material Design Guidelines https://material.io/guidelines Mobile Design Principles https://medium.com/blueprint-by-intuit/native-mobile-app-design-overall-principles-and-common-patterns-26edee8ced10 https://uxplanet.org/7-rules-for-mobile-ui-button-design-e9cf2ea54556","title":"4.12 Key Mobile Standards"},{"location":"ux/4-12-mobile/#412-key-mobile-standards","text":"","title":"4.12 Key Mobile Standards"},{"location":"ux/4-12-mobile/#designing-for-in-hand-and-on-the-go","text":"\u201cMobile first\u201d is a design mantra that helps remind us that an ever-increasing percentage of platform access is via mobile device (both phone and tablet). In some cases, mobile devices make up the majority of access. So while we can\u2019t yet dismiss the importance of desktop devices, it\u2019s a best practice to remember that our users first reach for the computers in their pockets.","title":"Designing for In-Hand and On-the-Go"},{"location":"ux/4-12-mobile/#mobile-is-different","text":"Most visual design principles are universal, and applicable across devices. The elements of type and color and imagery employ the same best practices. But two key factors (and many smaller ones) require slightly different design thinking, and a real consideration of user experience. The device is in-hand . Mobile devices have small viewports, obviously. That smaller screen size means that longer copy and larger interactive elements need to be treated with a sort of shorthand. An icon, perhaps, where before there was icon and text. That viewport means a smaller keyboard with smaller keys. They\u2019re often thumb-typed, making long form entry uncomfortable. And to just shrink interactive elements fails the user \u2013 often resulted in the dreaded \u201cfat finger\u201d mis-tap. The user is on-the-go . Mobile access often indicates that the user isn\u2019t at their workstation, is in a remote working environment, or is even working \u201cmulti-screen.\u201d They may be using cellular data and thus reluctant to download large files, if they\u2019re able to at all. They may be in an \u201cin-between time,\u201d waiting in line or walking to their next meeting, hoping for the most efficient task completion, without frills. When translating desktop designs to mobile, don\u2019t just miniaturize it \u2013 consider how the device and context should influence changes. Other mobile best practices include: Hide and reveal content . Text truncation, \u201cshow more\u201d buttons, and accordion controls increase the scannability of a mobile page, and allow the user to reveal content only on demand. Additionally, menus, headers, and footers can hide when scrolling away, and reveal when scrolling toward \u2013 reclaiming that visual real estate for higher priority elements. Remove unnecessary ornamentation . A beautiful brand image might work on a desktop design (and over reliable wifi), but on mobile is a waste of bandwidth. Keep only those images that orient and communicate critical information \u2013 and optimize those to the viewport of your target devices. Embrace scrolling . The most predictable, responsive, mobile designs translate desktop layouts into long, scrollable pages. Components should be prioritized from top to bottom in a logical linear order, and when possible provide an affordance for the component that follows. Test vigorously ! Minor differences between devices tend to disrupt pixel-precise mobile designs. The most reliable way to assure a predictable user experience is to test on your user\u2019s most common devices, within the browser of that device. If the actual phones / tables are unavailable, device emulators and browser plugins can still remove some of the guesswork.","title":"Mobile is Different"},{"location":"ux/4-12-mobile/#phone-vs-tablet-differences","text":"Remember that a mobile device isn\u2019t just a phone. Scalability across devices, more specifically between a phone and tablet, is a common challenge among designers. While the phone and tablet share many similarities, users use tablets, phones, phablets (not small enough to be a phone, not big enough to be a tablet) very differently.","title":"Phone vs. Tablet Differences"},{"location":"ux/4-12-mobile/#phone-interfaces","text":"Mobile interfaces less than 7 inches width should be treated as a phone. Layout should be aligned across these devices as much as possible, and should also leverage native platform guidelines and capabilities when possible. Mobile phone designs should include only necessary information . The phone is a convenient way to consume information on the go. Airmen use a phone to complete quick actions while they are not at their workspace, capture data in the field, view content, and perhaps return to look later on a larger device.","title":"Phone Interfaces"},{"location":"ux/4-12-mobile/#tablet-interfaces","text":"Mobile interfaces greater 7 inches width should be treated as a tablet. Layout should be aligned across these devices as much as possible; they do not need to align to phone interfaces. Tablet designs should look and feel like desktop web, but function like a phone (tap / swipe / hold gestures, etc.). Tablets are more likely to be held in landscape view \u2013 approximating the desktop viewport \u2013 and in this way, many users consider the tablet a hybrid device.","title":"Tablet Interfaces"},{"location":"ux/4-12-mobile/#hit-area-and-button-design","text":"Unlike the pixel-precise cursor on desktop, mobile interactions rely on human fingers of vastly different sizes. The rule of thumb (!) is that any clickable element should be 48px x 48px (accounting for 2x or greater design if necessary) to accommodate the average fingertip. This hit area should also have adequate padding around it; immediately adjacent clickable elements are not recommended.","title":"\u201cHit Area\u201d and Button Design"},{"location":"ux/4-12-mobile/#native-device-capabilities","text":"While most USAF platforms are web-based application viewed within the mobile device browser, native OS design (mobile apps) unlocks the potential of an integrated device. Smart phones and tablets have a lot to offer: touch, voice, pressure, location tracking, accelerometer, notifications, etc. You are designing around the device, the platform, the user experience. How can these device features be utilized in products? How can the mobile device benefit users beyond the screen interface in front of them? When designing for native platforms, however, consistently refer to the native OS design guidelines. These constantly evolve with new version releases and system redesigns, so it\u2019s always good practice to stay on top of these guidelines and refresh your memory and knowledge often.","title":"Native Device Capabilities"},{"location":"ux/4-12-mobile/#references","text":"Apple\u2019s Human Interface Guidelines https://developer.apple.com/ios/human-interface-guidelines Google\u2019s Material Design Guidelines https://material.io/guidelines Mobile Design Principles https://medium.com/blueprint-by-intuit/native-mobile-app-design-overall-principles-and-common-patterns-26edee8ced10 https://uxplanet.org/7-rules-for-mobile-ui-button-design-e9cf2ea54556","title":"References"},{"location":"ux/4-13-accessibility/","text":"4.13 Accessibility Accessibility Considers the User Experience for All Users As excerpted from the WCAG Guidelines, \u201cAccessibility defines how to make Web content more accessible to people with disabilities. Accessibility involves a wide range of disabilities, including visual, auditory, physical, speech, cognitive, language, learning, and neurological disabilities.\u201d While this might not be a requirement of your particular application\u2019s users, accessibility standards often reflect the most stringent interpretation of contrast, legibility, and usability best practices \u2013 and thus are of use to all designers. Accessibility Tips for Designers Accessible application design is a collaboration between designers and developers, affecting UI and code alike. In the interest of a useful shorthand for this playbook\u2019s primary users, this section focuses on best practices of visual design & accessibility (quoted heavily from the Web Accessibility Initiative): Provide sufficient contrast between foreground and background . Foreground text needs to have sufficient contrast with background colors. This includes text on images, background gradients, buttons, and other elements. Don\u2019t use color alone to convey information . While color can be useful to convey information, color should not be the only way information is conveyed. When using color to differentiate elements, also provide additional identification that does not rely on color perception. For example, use an asterisk in addition to color to indicate required form fields, and use labels to distinguish areas on graphs. Ensure that interactive elements are easy to identify . Provide distinct styles for interactive elements, such as links and buttons, to make them easy to identify. For example, change the appearance of links on mouse hover, keyboard focus, and touch-screen activation. Ensure that styles and naming for interactive elements are used consistently throughout the website. Provide clear and consistent navigation options . Ensure that navigation across pages within a website has consistent naming, styling, and positioning. Provide more than one method of website navigation, such as a site search or a site map. Help users understand where they are in a website or page by providing orientation cues, such as breadcrumbs and clear headings. Ensure that form elements include clearly associated labels . Ensure that all fields have a descriptive label adjacent to the field. For left-to-right languages, labels are usually positioned to the left or above the field, except for checkboxes and radio buttons where they are usually to the right. Avoid having too much space between labels and fields. Provide easily identifiable feedback . Provide feedback for interactions, such as confirming form submission, alerting the user when something goes wrong, or notifying the user of changes on the page. Instructions should be easy to identify. Important feedback that requires user action should be presented in a prominent style. Use headings and spacing to group related content . Use whitespace and proximity to make relationships between content more apparent. Style headings to group content, reduce clutter, and make it easier to scan and understand. Create designs for different viewport sizes . Consider how page information is presented in different sized viewports, such as mobile phones or zoomed browser windows. Position and presentation of main elements, such as header and navigation can be changed to make best use of the space. Ensure that text size and line width are set to maximize readability and legibility. Include image and media alternatives in your design . Provide a place in your design for alternatives for images and media. For example, you might need: visible links to transcripts of audio, visible links to audio described versions of videos, text along with icons and graphical buttons, and / or captions and descriptions for tables or complex graphs. Work with content authors and developers to provide alternatives for non-text content. Provide controls for content that starts automatically . Provide visible controls to allow users to stop any animations or auto-playing sound. This applies to carousels, image sliders, background sound, and videos. Most government platforms are subject to the Revised Section 508 Standards and thus WCAG 2.0 Level AA Success Criteria. In addition to other requirements, this will most commonly affect a designer\u2019s text & background color (contrast) choices: Text (including images of text) have a contrast ratio of at least 4.5:1. For text and images of that is at least 24px and normal weight or 19px and bold, use a contrast ratio that is at least 3:1. Color contrast for graphics and interactive UI components must be at least 3:1 so that different parts can be distinguished. When providing custom states for elements (e.g. hover, active, focus), color contrast for those states should be at least 3:1. References W3.org Tips & Resources for Designers https://www.w3.org/WAI/tips/designing Web Content Accessibility Guidelines (WCAG) 2.0 https://www.w3.org/TR/WCAG20 US Web Design System https://www.usability.gov/what-and-why/accessibility.html https://designsystem.digital.gov/documentation/accessibility HHS Design Standards https://webstandards.hhs.gov/guidelines","title":"4.13 Accessibility"},{"location":"ux/4-13-accessibility/#413-accessibility","text":"","title":"4.13 Accessibility"},{"location":"ux/4-13-accessibility/#accessibility-considers-the-user-experience-for-all-users","text":"As excerpted from the WCAG Guidelines, \u201cAccessibility defines how to make Web content more accessible to people with disabilities. Accessibility involves a wide range of disabilities, including visual, auditory, physical, speech, cognitive, language, learning, and neurological disabilities.\u201d While this might not be a requirement of your particular application\u2019s users, accessibility standards often reflect the most stringent interpretation of contrast, legibility, and usability best practices \u2013 and thus are of use to all designers.","title":"Accessibility Considers the User Experience for All Users"},{"location":"ux/4-13-accessibility/#accessibility-tips-for-designers","text":"Accessible application design is a collaboration between designers and developers, affecting UI and code alike. In the interest of a useful shorthand for this playbook\u2019s primary users, this section focuses on best practices of visual design & accessibility (quoted heavily from the Web Accessibility Initiative): Provide sufficient contrast between foreground and background . Foreground text needs to have sufficient contrast with background colors. This includes text on images, background gradients, buttons, and other elements. Don\u2019t use color alone to convey information . While color can be useful to convey information, color should not be the only way information is conveyed. When using color to differentiate elements, also provide additional identification that does not rely on color perception. For example, use an asterisk in addition to color to indicate required form fields, and use labels to distinguish areas on graphs. Ensure that interactive elements are easy to identify . Provide distinct styles for interactive elements, such as links and buttons, to make them easy to identify. For example, change the appearance of links on mouse hover, keyboard focus, and touch-screen activation. Ensure that styles and naming for interactive elements are used consistently throughout the website. Provide clear and consistent navigation options . Ensure that navigation across pages within a website has consistent naming, styling, and positioning. Provide more than one method of website navigation, such as a site search or a site map. Help users understand where they are in a website or page by providing orientation cues, such as breadcrumbs and clear headings. Ensure that form elements include clearly associated labels . Ensure that all fields have a descriptive label adjacent to the field. For left-to-right languages, labels are usually positioned to the left or above the field, except for checkboxes and radio buttons where they are usually to the right. Avoid having too much space between labels and fields. Provide easily identifiable feedback . Provide feedback for interactions, such as confirming form submission, alerting the user when something goes wrong, or notifying the user of changes on the page. Instructions should be easy to identify. Important feedback that requires user action should be presented in a prominent style. Use headings and spacing to group related content . Use whitespace and proximity to make relationships between content more apparent. Style headings to group content, reduce clutter, and make it easier to scan and understand. Create designs for different viewport sizes . Consider how page information is presented in different sized viewports, such as mobile phones or zoomed browser windows. Position and presentation of main elements, such as header and navigation can be changed to make best use of the space. Ensure that text size and line width are set to maximize readability and legibility. Include image and media alternatives in your design . Provide a place in your design for alternatives for images and media. For example, you might need: visible links to transcripts of audio, visible links to audio described versions of videos, text along with icons and graphical buttons, and / or captions and descriptions for tables or complex graphs. Work with content authors and developers to provide alternatives for non-text content. Provide controls for content that starts automatically . Provide visible controls to allow users to stop any animations or auto-playing sound. This applies to carousels, image sliders, background sound, and videos. Most government platforms are subject to the Revised Section 508 Standards and thus WCAG 2.0 Level AA Success Criteria. In addition to other requirements, this will most commonly affect a designer\u2019s text & background color (contrast) choices: Text (including images of text) have a contrast ratio of at least 4.5:1. For text and images of that is at least 24px and normal weight or 19px and bold, use a contrast ratio that is at least 3:1. Color contrast for graphics and interactive UI components must be at least 3:1 so that different parts can be distinguished. When providing custom states for elements (e.g. hover, active, focus), color contrast for those states should be at least 3:1.","title":"Accessibility Tips for Designers"},{"location":"ux/4-13-accessibility/#references","text":"W3.org Tips & Resources for Designers https://www.w3.org/WAI/tips/designing Web Content Accessibility Guidelines (WCAG) 2.0 https://www.w3.org/TR/WCAG20 US Web Design System https://www.usability.gov/what-and-why/accessibility.html https://designsystem.digital.gov/documentation/accessibility HHS Design Standards https://webstandards.hhs.gov/guidelines","title":"References"},{"location":"ux/4-2-guidelines/","text":"4.2 General Interface Guidelines Designing the User Interface As the visualized part of the user experience, a thoughtfully designed user interface (UI) is critical to helping end users quickly and efficiently complete tasks. It\u2019s said that \u201cgood design is 99% invisible,\u201d and so should be your choices. When users notice clashing colors, illegible text, or misplaced buttons, they grow frustrated and worse. Design for the User Design is not a purely aesthetic task. Before placing a pixel, orient yourself with any UX and audience findings. You should be able to answer the following questions: What\u2019s your application\u2019s ultimate goal? What tasks do you want users to accomplish? Who is going to use your application? What do they want or need? What are the benchmarks for their success? On what devices will your application be used? In what context (remote, at a workstation, etc.) will they use it? Design for Interaction Digital platforms are not passive environments. While some principles of print design hold true, these pages, platforms, and applications have a fundamentally different relationship with the user. The user is expected to affect the application, to interact and communicate their intent. In return, the platform should be a conversation with the user. Guide the Eye . Users are drawn toward heavier elements of a page, and can wander if there are no distinctions in visual weight between elements. Critical components or calls-to-action should be larger and bolder. Less critical components \u2013 such as supplementary text \u2013 can be smaller and lower contrast. Also, eye tracking studies indicate that people generally scan pages in an F pattern (described in section 4.6), which is a useful starting point. Focus on Task Completion . A design should perform it\u2019s intended task and be beautiful, and is a failure if it\u2019s just beautiful. Each design decision should serve the function of the page, and evaluate decorative elements (like brand images) on their contribution to task completion. Design Light . Bandwidth and download speeds plague users in a need-it-now world. As a designer, you can increase their working speed by minimizing the file size of elements in your design. Text, solid colors, and default browser elements are \u201clight\u201d and can be used liberally. Images (and animations in particular) should only be used when necessary to serve the function of the page. Provide Proactive and Immediate Feedback . Obvious labels, content categorization, and color cues create a proactive shorthand for users. Button hover states and dynamic rollovers allow them to \u201cpeek behind the curtain\u201d and know better what comes after their click (see Buttons & Controls). Progressively Reveal . Only display \u2013 through truncated content and other components \u2013 what the user absolutely needs to know in order to make their next, informed choice. There is a good reason the web is so rife with \u201cLearn More\u201d buttons \u2013 this saves on both page weight and cognitive load. Differentiate Clicks/Taps, Hover, and Scroll . Every time the user interacts with the application, they tell us something about their intent. While a hover is exploratory, a click is committal. Scrolling down a page says \u201cI haven\u2019t found what I\u2019m looking for.\u201d Consider how your design choices respond to the nuanced behaviors of the user. Reference General Assembly\u2019s Guide to UI https://generalassemb.ly/design/visual-design/user-interfaces","title":"4.2 General Interface Guidelines"},{"location":"ux/4-2-guidelines/#42-general-interface-guidelines","text":"","title":"4.2 General Interface Guidelines"},{"location":"ux/4-2-guidelines/#designing-the-user-interface","text":"As the visualized part of the user experience, a thoughtfully designed user interface (UI) is critical to helping end users quickly and efficiently complete tasks. It\u2019s said that \u201cgood design is 99% invisible,\u201d and so should be your choices. When users notice clashing colors, illegible text, or misplaced buttons, they grow frustrated and worse.","title":"Designing the User Interface"},{"location":"ux/4-2-guidelines/#design-for-the-user","text":"Design is not a purely aesthetic task. Before placing a pixel, orient yourself with any UX and audience findings. You should be able to answer the following questions: What\u2019s your application\u2019s ultimate goal? What tasks do you want users to accomplish? Who is going to use your application? What do they want or need? What are the benchmarks for their success? On what devices will your application be used? In what context (remote, at a workstation, etc.) will they use it?","title":"Design for the User"},{"location":"ux/4-2-guidelines/#design-for-interaction","text":"Digital platforms are not passive environments. While some principles of print design hold true, these pages, platforms, and applications have a fundamentally different relationship with the user. The user is expected to affect the application, to interact and communicate their intent. In return, the platform should be a conversation with the user. Guide the Eye . Users are drawn toward heavier elements of a page, and can wander if there are no distinctions in visual weight between elements. Critical components or calls-to-action should be larger and bolder. Less critical components \u2013 such as supplementary text \u2013 can be smaller and lower contrast. Also, eye tracking studies indicate that people generally scan pages in an F pattern (described in section 4.6), which is a useful starting point. Focus on Task Completion . A design should perform it\u2019s intended task and be beautiful, and is a failure if it\u2019s just beautiful. Each design decision should serve the function of the page, and evaluate decorative elements (like brand images) on their contribution to task completion. Design Light . Bandwidth and download speeds plague users in a need-it-now world. As a designer, you can increase their working speed by minimizing the file size of elements in your design. Text, solid colors, and default browser elements are \u201clight\u201d and can be used liberally. Images (and animations in particular) should only be used when necessary to serve the function of the page. Provide Proactive and Immediate Feedback . Obvious labels, content categorization, and color cues create a proactive shorthand for users. Button hover states and dynamic rollovers allow them to \u201cpeek behind the curtain\u201d and know better what comes after their click (see Buttons & Controls). Progressively Reveal . Only display \u2013 through truncated content and other components \u2013 what the user absolutely needs to know in order to make their next, informed choice. There is a good reason the web is so rife with \u201cLearn More\u201d buttons \u2013 this saves on both page weight and cognitive load. Differentiate Clicks/Taps, Hover, and Scroll . Every time the user interacts with the application, they tell us something about their intent. While a hover is exploratory, a click is committal. Scrolling down a page says \u201cI haven\u2019t found what I\u2019m looking for.\u201d Consider how your design choices respond to the nuanced behaviors of the user.","title":"Design for Interaction"},{"location":"ux/4-2-guidelines/#reference","text":"General Assembly\u2019s Guide to UI https://generalassemb.ly/design/visual-design/user-interfaces","title":"Reference"},{"location":"ux/4-3-grids/","text":"4.3 Grids & Breakpoints About the Grid A grid is an organizing system of a layout, consisting of invisible rulers that align layout elements like copy, images, and navigation. Most often, a grid defines a series of equally sized columns with smaller, equally sized columns for padding (\u201cgutters\u201d) in between. The grid behaves differently depending upon the viewport, as defined by the device upon which the layout is viewed. The viewport is essentially the screen size of the device, measured in pixels, and is the amount of visual real estate available for your layout. A thoughtful grid is particularly helpful in responsive design. This type of design is a collaboration with front-end development, and programmatically scales down the layout to be most appropriate for the device / viewport in use. As the browser window shrinks horizontally, the point at which a grid \u201cbreaks\u201d to a fewer-column grid is called the breakpoint. Defining Pixels As device display technology improves, the density of pixels has grown to differ across devices. Names like HD and 4k indicate different numbers of pixels per inch \u2013 all in pursuit of a crisper image. To retain predictability and consistency, designers and developers rely on CSS pixels , which provide a standard definition of pixel size for the web that does not vary based on the device\u2019s pixel density. It is highly recommended that designers work with their front-end developers to define target devices and pixel densities, and use resources that track changing display technology (like https://vizdevices.yesviz.com/ ). Small Viewport The Small Viewport grid is intended to accommodate portrait-view smartphones (approx. 320 px) and many landscape view smartphones. On these devices, the 8-column grid switches to a 4-column structure to create a more comfortable layout on a handheld, as well as to allow for larger tap-targets. This format equates to a maximum width of 767 px , with a minimum width of 320 px . Medium Viewport The Medium Viewport grid is intended to accommodate most portrait-view tablets (768 px), most landscape view phablets (768 px), and larger landscape view smart phones. On these devices, the 12-column grid switches to an 8-column structure to create a more comfortable layout on tablets, as well as to allow for larger tap-targets. This format equates to a maximum width of 1023 px , with a minimum width of 768 px . Large Viewport The Large Viewport grid is intended to accommodate high-resolution monitors, most landscape-view tablets (1042 px), and everything in between. This equates to a maximum width that is infinite , and a minimum width of 1024 px . Responsive Component Reordering As responsive layouts break to smaller viewports, page elements \u2013 components \u2013 should logically reorder and stack vertically to maintain the intent of the layout. This diagram illustrates one solution for component reorganization with respect to responsive layouts. Each component is designed to occupy a minimum of 2 columns and maximum of 6. The Medium Viewport layout includes a potential for additional white space to appear, in the event that a design contains a 3-component row. Being that the use-case is relatively obscure (with respect to hardware, UI, content, and user needs), the choice was made to include the layout as described, thus avoiding either bespoke solutions or arbitrary hierarchical relationships. As a takeaway, design with a minimum of 4 columns / 1 component width for the smallest viewport, and otherwise subdivide the total number of grid columns evenly to create a consistent layout on table and desktop. Reference Interaction Design Foundation https://www.interaction-design.org/literature/article/responsive-design-let-the-device-do-the-work","title":"4.3 Grids & Breakpoints"},{"location":"ux/4-3-grids/#43-grids-breakpoints","text":"","title":"4.3 Grids &amp; Breakpoints"},{"location":"ux/4-3-grids/#about-the-grid","text":"A grid is an organizing system of a layout, consisting of invisible rulers that align layout elements like copy, images, and navigation. Most often, a grid defines a series of equally sized columns with smaller, equally sized columns for padding (\u201cgutters\u201d) in between. The grid behaves differently depending upon the viewport, as defined by the device upon which the layout is viewed. The viewport is essentially the screen size of the device, measured in pixels, and is the amount of visual real estate available for your layout. A thoughtful grid is particularly helpful in responsive design. This type of design is a collaboration with front-end development, and programmatically scales down the layout to be most appropriate for the device / viewport in use. As the browser window shrinks horizontally, the point at which a grid \u201cbreaks\u201d to a fewer-column grid is called the breakpoint.","title":"About the Grid"},{"location":"ux/4-3-grids/#defining-pixels","text":"As device display technology improves, the density of pixels has grown to differ across devices. Names like HD and 4k indicate different numbers of pixels per inch \u2013 all in pursuit of a crisper image. To retain predictability and consistency, designers and developers rely on CSS pixels , which provide a standard definition of pixel size for the web that does not vary based on the device\u2019s pixel density. It is highly recommended that designers work with their front-end developers to define target devices and pixel densities, and use resources that track changing display technology (like https://vizdevices.yesviz.com/ ).","title":"Defining Pixels"},{"location":"ux/4-3-grids/#small-viewport","text":"The Small Viewport grid is intended to accommodate portrait-view smartphones (approx. 320 px) and many landscape view smartphones. On these devices, the 8-column grid switches to a 4-column structure to create a more comfortable layout on a handheld, as well as to allow for larger tap-targets. This format equates to a maximum width of 767 px , with a minimum width of 320 px .","title":"Small Viewport"},{"location":"ux/4-3-grids/#medium-viewport","text":"The Medium Viewport grid is intended to accommodate most portrait-view tablets (768 px), most landscape view phablets (768 px), and larger landscape view smart phones. On these devices, the 12-column grid switches to an 8-column structure to create a more comfortable layout on tablets, as well as to allow for larger tap-targets. This format equates to a maximum width of 1023 px , with a minimum width of 768 px .","title":"Medium Viewport"},{"location":"ux/4-3-grids/#large-viewport","text":"The Large Viewport grid is intended to accommodate high-resolution monitors, most landscape-view tablets (1042 px), and everything in between. This equates to a maximum width that is infinite , and a minimum width of 1024 px .","title":"Large Viewport"},{"location":"ux/4-3-grids/#responsive-component-reordering","text":"As responsive layouts break to smaller viewports, page elements \u2013 components \u2013 should logically reorder and stack vertically to maintain the intent of the layout. This diagram illustrates one solution for component reorganization with respect to responsive layouts. Each component is designed to occupy a minimum of 2 columns and maximum of 6. The Medium Viewport layout includes a potential for additional white space to appear, in the event that a design contains a 3-component row. Being that the use-case is relatively obscure (with respect to hardware, UI, content, and user needs), the choice was made to include the layout as described, thus avoiding either bespoke solutions or arbitrary hierarchical relationships. As a takeaway, design with a minimum of 4 columns / 1 component width for the smallest viewport, and otherwise subdivide the total number of grid columns evenly to create a consistent layout on table and desktop.","title":"Responsive Component Reordering"},{"location":"ux/4-3-grids/#reference","text":"Interaction Design Foundation https://www.interaction-design.org/literature/article/responsive-design-let-the-device-do-the-work","title":"Reference"},{"location":"ux/4-4-branding/","text":"4.4 Branding/Logo Our Identification The Air Force Symbol is the official symbol of the United States Air Force. It honors the heritage of our past and represents the promise of our future. Furthermore, it retains the core elements of our Air Corps heritage, the \"Arnold\" wings and star with circle, and modernizes them to reflect our air and space force of today and tomorrow. The USAF symbol was thoughtfully crafted to represent the Air Force\u2019s history as well as the promise of the future of the armed forces branch. The symbol can take on two different forms depending on how you choose to see it: a medal of valor in service or our nation\u2019s emblem of freedom, an eagle. Some Rules Around the USAF Signature A 15% stand-off space around the Symbol and/or signature is required. The stand-off space takes the shape of a square, not the outline of the Symbol. The U.S. Air Force signature consists of the Air Force Symbol and the logotype (U.S. Air Force) The Symbol can be used with or without the logotype If used with the logotype, the two elements are in a fixed relationship and cannot be altered. The only alternate words permitted \u201cdirectly\u201d under the Symbol are those approved by the Chief of Staff of the Air Force (see Formats) Logo Application The spacing and positioning for the logomark has been carefully considered and optimized to create an ideal optical balance. Because the shape is comprised of sharp, angular points, in tandem with the illusion of a 3-dimensional shape, a sense of symmetry is achieved through subtle repositioning of the mathematical center. Tips on usage White logo on photographic/textural background White or color logo on solid color background Avoid using USAF color logo on full-color photographic image background Spacing The spacing and positioning for the logomark has been carefully considered and optimized to create an ideal optical balance. Because the shape is comprised of sharp, angular points, in tandem with the illusion of a 3-dimensional shape, a sense of symmetry is achieved through subtle repositioning of the mathematical center. Sizing The shape of the mark should maintain its integrity at relatively small sizes. Designers should use discretion and consider the output-media when utilizing the mark at a smaller size. References The Official Website of the Air Force Trademark and Licensing Program https://www.trademark.af.mil/About-Us/The-Air-Force-Symbol/Display-guidelines Meaning behind the mark https://www.airman.af.mil/Portals/17/002%20All%20Products/003%20PACEsetters/Meaning_Air_Force_Symbol.pdf?ver=2016-03-30-001043-347","title":"4.4 Branding/Logo"},{"location":"ux/4-4-branding/#44-brandinglogo","text":"","title":"4.4 Branding/Logo"},{"location":"ux/4-4-branding/#our-identification","text":"The Air Force Symbol is the official symbol of the United States Air Force. It honors the heritage of our past and represents the promise of our future. Furthermore, it retains the core elements of our Air Corps heritage, the \"Arnold\" wings and star with circle, and modernizes them to reflect our air and space force of today and tomorrow. The USAF symbol was thoughtfully crafted to represent the Air Force\u2019s history as well as the promise of the future of the armed forces branch. The symbol can take on two different forms depending on how you choose to see it: a medal of valor in service or our nation\u2019s emblem of freedom, an eagle.","title":"Our Identification"},{"location":"ux/4-4-branding/#some-rules-around-the-usaf-signature","text":"A 15% stand-off space around the Symbol and/or signature is required. The stand-off space takes the shape of a square, not the outline of the Symbol. The U.S. Air Force signature consists of the Air Force Symbol and the logotype (U.S. Air Force) The Symbol can be used with or without the logotype If used with the logotype, the two elements are in a fixed relationship and cannot be altered. The only alternate words permitted \u201cdirectly\u201d under the Symbol are those approved by the Chief of Staff of the Air Force (see Formats)","title":"Some Rules Around the USAF Signature"},{"location":"ux/4-4-branding/#logo-application","text":"The spacing and positioning for the logomark has been carefully considered and optimized to create an ideal optical balance. Because the shape is comprised of sharp, angular points, in tandem with the illusion of a 3-dimensional shape, a sense of symmetry is achieved through subtle repositioning of the mathematical center.","title":"Logo Application"},{"location":"ux/4-4-branding/#tips-on-usage","text":"White logo on photographic/textural background White or color logo on solid color background Avoid using USAF color logo on full-color photographic image background","title":"Tips on usage"},{"location":"ux/4-4-branding/#spacing","text":"The spacing and positioning for the logomark has been carefully considered and optimized to create an ideal optical balance. Because the shape is comprised of sharp, angular points, in tandem with the illusion of a 3-dimensional shape, a sense of symmetry is achieved through subtle repositioning of the mathematical center.","title":"Spacing"},{"location":"ux/4-4-branding/#sizing","text":"The shape of the mark should maintain its integrity at relatively small sizes. Designers should use discretion and consider the output-media when utilizing the mark at a smaller size.","title":"Sizing"},{"location":"ux/4-4-branding/#references","text":"The Official Website of the Air Force Trademark and Licensing Program https://www.trademark.af.mil/About-Us/The-Air-Force-Symbol/Display-guidelines Meaning behind the mark https://www.airman.af.mil/Portals/17/002%20All%20Products/003%20PACEsetters/Meaning_Air_Force_Symbol.pdf?ver=2016-03-30-001043-347","title":"References"},{"location":"ux/4-5-headers/","text":"4.5 Global Headers & Footers Headers & Footers In a web page layout, the header is the upper (topmost) part of the page, immediately beneath the browser chrome (where the URL, search field, and bookmarks are located). The footer is the lower (bottommost) part of the page, where scrolling terminates. Most mobile devices remove the chrome from the browser in order to allow more space for the header and footer elements. Headers Headers can include a variety of meaningful elements: Basic elements of brand identity: logo, brand / application name, tagline or brand statement, application colors, corporate colors, etc. Navigation to primary site or application sections Language-switching functionality (if applicable) Search field Not all of the mentioned elements should be included in one web page header: in this case, the risk is high that the header section would be overloaded with information. The more objects that attract a user\u2019s attention, the harder it is to concentrate on the vital ones. As with all design decisions, apply UX best practices to focus on user orientation (\u201cwhere am I and what does this do?\u201d) and task completion (\u201chow do I do it?\u201d). Desktop Navigation Search is Intertwined with Navigation Search is a form of navigation. In many situations, the reader will use a combination of the \u201ccontent gatherers\u201d. They will use search to bring them to the subject area or product type they are interested in. Then the navigation should kick in, giving them the context for their search. Footers The footer is the lower (bottommost) part of the page, where scrolling terminates. Footers can support wide varieties and large volumes of content, and often contain universal content (contact details and forms), expanded navigation options, and links outside of the given site or application. Contemporary users have demonstrated comfort scrolling long pages and engaging with large page footers, provided both follow layout best practices. It is reasonable to use the page footer as a repository for all content that is both deprioritized but should also be universally accessible. Visual Hierarchy Given that the footer is often more visually dense than other page components, a visual hierarchy is critical. Adequate spacing helps improve both focus and legibility. Thoughtful typography helps improve scanning and the overall layout. Consider \u201creversing\u201d the colors of the main page in the page\u2019s footer in order to communicate that they\u2019ve reached the end of the page. That is, if the body of the page is on a light background, put the footer on a dark background. Given that this is a common technique, note that \u201creversing\u201d the colors of a component in the page body can sometimes create a \u201cfalse bottom,\u201d causing users to believe they\u2019ve hit the footer, and possibly miss the rest of the page. Anchoring Both headers and footers can \u201canchor\u201d to the top or bottom of the page (though rarely should both page elements should behave this way simultaneously). An anchored \u2013 or \u201csticky\u201d \u2013 element stays on screen as the user scrolls, no matter the scroll depth. This is particularly useful for bringing persistent information or interaction along with the user, such as a form submission, call to action, or mode toggle (edit / review). References U.S. Web Design System https://designsystem.digital.gov/page-templates/#documentation-page UX Planet uxplanet.org","title":"4.5 Global Headers & Footers"},{"location":"ux/4-5-headers/#45-global-headers-footers","text":"","title":"4.5 Global Headers &amp; Footers"},{"location":"ux/4-5-headers/#headers-footers","text":"In a web page layout, the header is the upper (topmost) part of the page, immediately beneath the browser chrome (where the URL, search field, and bookmarks are located). The footer is the lower (bottommost) part of the page, where scrolling terminates. Most mobile devices remove the chrome from the browser in order to allow more space for the header and footer elements.","title":"Headers &amp; Footers"},{"location":"ux/4-5-headers/#headers","text":"Headers can include a variety of meaningful elements: Basic elements of brand identity: logo, brand / application name, tagline or brand statement, application colors, corporate colors, etc. Navigation to primary site or application sections Language-switching functionality (if applicable) Search field Not all of the mentioned elements should be included in one web page header: in this case, the risk is high that the header section would be overloaded with information. The more objects that attract a user\u2019s attention, the harder it is to concentrate on the vital ones. As with all design decisions, apply UX best practices to focus on user orientation (\u201cwhere am I and what does this do?\u201d) and task completion (\u201chow do I do it?\u201d).","title":"Headers"},{"location":"ux/4-5-headers/#desktop-navigation","text":"","title":"Desktop Navigation"},{"location":"ux/4-5-headers/#search-is-intertwined-with-navigation","text":"Search is a form of navigation. In many situations, the reader will use a combination of the \u201ccontent gatherers\u201d. They will use search to bring them to the subject area or product type they are interested in. Then the navigation should kick in, giving them the context for their search.","title":"Search is Intertwined with Navigation"},{"location":"ux/4-5-headers/#footers","text":"The footer is the lower (bottommost) part of the page, where scrolling terminates. Footers can support wide varieties and large volumes of content, and often contain universal content (contact details and forms), expanded navigation options, and links outside of the given site or application. Contemporary users have demonstrated comfort scrolling long pages and engaging with large page footers, provided both follow layout best practices. It is reasonable to use the page footer as a repository for all content that is both deprioritized but should also be universally accessible.","title":"Footers"},{"location":"ux/4-5-headers/#visual-hierarchy","text":"Given that the footer is often more visually dense than other page components, a visual hierarchy is critical. Adequate spacing helps improve both focus and legibility. Thoughtful typography helps improve scanning and the overall layout. Consider \u201creversing\u201d the colors of the main page in the page\u2019s footer in order to communicate that they\u2019ve reached the end of the page. That is, if the body of the page is on a light background, put the footer on a dark background. Given that this is a common technique, note that \u201creversing\u201d the colors of a component in the page body can sometimes create a \u201cfalse bottom,\u201d causing users to believe they\u2019ve hit the footer, and possibly miss the rest of the page.","title":"Visual Hierarchy"},{"location":"ux/4-5-headers/#anchoring","text":"Both headers and footers can \u201canchor\u201d to the top or bottom of the page (though rarely should both page elements should behave this way simultaneously). An anchored \u2013 or \u201csticky\u201d \u2013 element stays on screen as the user scrolls, no matter the scroll depth. This is particularly useful for bringing persistent information or interaction along with the user, such as a form submission, call to action, or mode toggle (edit / review).","title":"Anchoring"},{"location":"ux/4-5-headers/#references","text":"U.S. Web Design System https://designsystem.digital.gov/page-templates/#documentation-page UX Planet uxplanet.org","title":"References"},{"location":"ux/4-6-layouts/","text":"4.6 Page Layouts Layouts Generally speaking, the layout of a page should prioritize content importance from top to bottom, providing orientation (wayfinding and labels) to components throughout. Layouts should also follow the natural flow of task completion; users completing a number of form fields on a long vertical page might encounter the \u201csubmit\u201d button \u2013 the primary call-to-action \u2013 beneath the original viewport and offscreen. This is permissible because the form fields are the prerequisite for completing the task of the page, and should thus be prioritized in the visual hierarchy. F Pattern Studies have found that, in cultures that read from left to right, most people scan screens in an \u201cF\u201d pattern, with preference given to elements that are positioned at the top and left of the screen. Consider placing the most important elements at the top and left of your layout. The upper right of the page is considered secondary in importance due to its positioning at the top of the page, but the importance of the right side rapidly diminishes as you move further down the page. Other layout patterns support different densities of content and types of tasks to complete. They all acknowledge the way the user scans the page, tending to favor the top and left. References NN/g Nielsen Norma Group : Pattern of Reading https://www.nngroup.com/articles/f-shaped-pattern-reading-web-content/ Vanseo Design : Design Layouts : Gutenberg Diagram, Z-OPattern and F-Pattern http://vanseodesign.com/web-design/3-design-layouts U.S. Web Design System https://designsystem.digital.gov","title":"4.6 Page Layouts"},{"location":"ux/4-6-layouts/#46-page-layouts","text":"","title":"4.6 Page Layouts"},{"location":"ux/4-6-layouts/#layouts","text":"Generally speaking, the layout of a page should prioritize content importance from top to bottom, providing orientation (wayfinding and labels) to components throughout. Layouts should also follow the natural flow of task completion; users completing a number of form fields on a long vertical page might encounter the \u201csubmit\u201d button \u2013 the primary call-to-action \u2013 beneath the original viewport and offscreen. This is permissible because the form fields are the prerequisite for completing the task of the page, and should thus be prioritized in the visual hierarchy.","title":"Layouts"},{"location":"ux/4-6-layouts/#f-pattern","text":"Studies have found that, in cultures that read from left to right, most people scan screens in an \u201cF\u201d pattern, with preference given to elements that are positioned at the top and left of the screen. Consider placing the most important elements at the top and left of your layout. The upper right of the page is considered secondary in importance due to its positioning at the top of the page, but the importance of the right side rapidly diminishes as you move further down the page. Other layout patterns support different densities of content and types of tasks to complete. They all acknowledge the way the user scans the page, tending to favor the top and left.","title":"F Pattern"},{"location":"ux/4-6-layouts/#references","text":"NN/g Nielsen Norma Group : Pattern of Reading https://www.nngroup.com/articles/f-shaped-pattern-reading-web-content/ Vanseo Design : Design Layouts : Gutenberg Diagram, Z-OPattern and F-Pattern http://vanseodesign.com/web-design/3-design-layouts U.S. Web Design System https://designsystem.digital.gov","title":"References"},{"location":"ux/4-7-typography/","text":"4.7 Typography Reinforcing the Goals of Typography More than 95% of information on the web is in the form of written language. If typography is done well, it goes unnoticed. If it\u2019s not, it sticks out like a sore thumb and moreover disrupts the user experience. Typography holds an important place in any form of design that features text, and this is especially true for web and application design. Three Fundamental Aspects of Typography Although in a non-technical sense \u201clegible\u201d and \u201creadable\u201d are often used synonymously, typographically they are separate but related concepts. Legibility is the ease with which a reader can recognize individual characters in text. Readability is the ease with which a reader can recognize words, sentences, and paragraphs comprised of characters. Aesthetics is the emotional property of the text, communicating everything from precision to speed to humor. Typography as an art is nuanced; our primary concern is functional \u2013 the fluid communication of information by the written word. As such, we focus on core concepts for achieving legibility, readability, and context-appropriate aesthetics. Font Type Of the many font categorizations, the two largest (and most common categories) are \u201cserif\u201d and \u201csans-serif\u201d fonts (literally translating to \u201cfeet\u201d and \u201cwithout feet\u201d). The \u201cfeet\u201d to which they refer are the small visual elements that branch out at the end of a stroke. Due to their clean-line, technical appearance, sans-serif fonts are often considered a more modern aesthetic. They are appropriate for both headline and body copy; when used as headlines they often pair well with serif body copy. Due to their origins in mechanical letterpress, serif fonts are often considered a more timeless aesthetic. Studies have shown larger blocks of copy to be more readable when printed in serif type, which is why most books and long-form copy employ serif fonts to this day. Font Size Font size is another major determinant of text readability. A font size of about 16px is optimal for easy online reading of long-form content. To support overall visual hierarchy, consider using three or fewer different font sizes in any given component, Rules on minimum font size can be found on the Accessibility section. Contrast Color contrast ultimately determines whether or not text is readable. Black text on a white background, for example, is very high contrast, but for optimum readability, place very dark grey text on a white background; it will maximize contrast while lowering eye strain. The rules of contrast are outlined specifically in the Web Content Accessibility Guidelines (WCAG) 2.0, addressed elsewhere in this playbook. Depending upon accessibility requirements for the visually impaired, contrast compliance may need to be more or less strictly followed. Hierarchy Text hierarchy becomes especially important in web and application design because it helps users navigate through a site quickly and scan text easily. If all type was the same size and weight, it would be difficult to know which was the most important information on the page. In order to orient the user to critical layout components, headings are usually the most dominant, sub-headings less so, and body type even less so. This hierarchy is accomplished through a combination of font size (px), contrast (color), and weight (thin, bold). Line Height Line height, or leading, refers to the space between lines in a body of text. A general rule for readable text is that your leading value should be 125\u2013150% of the font size. Frustratingly, the \u201csingle spacing\u201d default is typically too tight for text on a web page while \u201cdouble spacing\u201d can be so loose that the text no longer looks as though it is part of one unit. Consider adjusting your line height to satisfy this 125\u2013150% rule. Line Length Line length defines the number of characters in a single line of text, until the reader returns to the beginning of the following line. In large viewports, long lines of body text can be cumbersome to consume. A best practice for body text is a single line character length of 50-60 characters, beyond which diminishes the readability of text. White Space Your layout\u2019s \u201cwhite space\u201d \u2013 visual areas unoccupied by text or images \u2013 is essential for offsetting large amounts of text, and providing focus and orientation for users. It should offer separation between different elements of the text layout, such as text and images, and body text and headers. References Tubik Blog https://tubikstudio.com/20-wise-thoughts-by-typography-master-erik-spiekermann Smashing Design https://www.smashingmagazine.com/2011/01/guidelines-for-responsive-web-design","title":"4.7 Typography"},{"location":"ux/4-7-typography/#47-typography","text":"","title":"4.7 Typography"},{"location":"ux/4-7-typography/#reinforcing-the-goals-of-typography","text":"More than 95% of information on the web is in the form of written language. If typography is done well, it goes unnoticed. If it\u2019s not, it sticks out like a sore thumb and moreover disrupts the user experience. Typography holds an important place in any form of design that features text, and this is especially true for web and application design.","title":"Reinforcing the Goals of Typography"},{"location":"ux/4-7-typography/#three-fundamental-aspects-of-typography","text":"Although in a non-technical sense \u201clegible\u201d and \u201creadable\u201d are often used synonymously, typographically they are separate but related concepts. Legibility is the ease with which a reader can recognize individual characters in text. Readability is the ease with which a reader can recognize words, sentences, and paragraphs comprised of characters. Aesthetics is the emotional property of the text, communicating everything from precision to speed to humor. Typography as an art is nuanced; our primary concern is functional \u2013 the fluid communication of information by the written word. As such, we focus on core concepts for achieving legibility, readability, and context-appropriate aesthetics.","title":"Three Fundamental Aspects of Typography"},{"location":"ux/4-7-typography/#font-type","text":"Of the many font categorizations, the two largest (and most common categories) are \u201cserif\u201d and \u201csans-serif\u201d fonts (literally translating to \u201cfeet\u201d and \u201cwithout feet\u201d). The \u201cfeet\u201d to which they refer are the small visual elements that branch out at the end of a stroke. Due to their clean-line, technical appearance, sans-serif fonts are often considered a more modern aesthetic. They are appropriate for both headline and body copy; when used as headlines they often pair well with serif body copy. Due to their origins in mechanical letterpress, serif fonts are often considered a more timeless aesthetic. Studies have shown larger blocks of copy to be more readable when printed in serif type, which is why most books and long-form copy employ serif fonts to this day.","title":"Font Type"},{"location":"ux/4-7-typography/#font-size","text":"Font size is another major determinant of text readability. A font size of about 16px is optimal for easy online reading of long-form content. To support overall visual hierarchy, consider using three or fewer different font sizes in any given component, Rules on minimum font size can be found on the Accessibility section.","title":"Font Size"},{"location":"ux/4-7-typography/#contrast","text":"Color contrast ultimately determines whether or not text is readable. Black text on a white background, for example, is very high contrast, but for optimum readability, place very dark grey text on a white background; it will maximize contrast while lowering eye strain. The rules of contrast are outlined specifically in the Web Content Accessibility Guidelines (WCAG) 2.0, addressed elsewhere in this playbook. Depending upon accessibility requirements for the visually impaired, contrast compliance may need to be more or less strictly followed.","title":"Contrast"},{"location":"ux/4-7-typography/#hierarchy","text":"Text hierarchy becomes especially important in web and application design because it helps users navigate through a site quickly and scan text easily. If all type was the same size and weight, it would be difficult to know which was the most important information on the page. In order to orient the user to critical layout components, headings are usually the most dominant, sub-headings less so, and body type even less so. This hierarchy is accomplished through a combination of font size (px), contrast (color), and weight (thin, bold).","title":"Hierarchy"},{"location":"ux/4-7-typography/#line-height","text":"Line height, or leading, refers to the space between lines in a body of text. A general rule for readable text is that your leading value should be 125\u2013150% of the font size. Frustratingly, the \u201csingle spacing\u201d default is typically too tight for text on a web page while \u201cdouble spacing\u201d can be so loose that the text no longer looks as though it is part of one unit. Consider adjusting your line height to satisfy this 125\u2013150% rule.","title":"Line Height"},{"location":"ux/4-7-typography/#line-length","text":"Line length defines the number of characters in a single line of text, until the reader returns to the beginning of the following line. In large viewports, long lines of body text can be cumbersome to consume. A best practice for body text is a single line character length of 50-60 characters, beyond which diminishes the readability of text.","title":"Line Length"},{"location":"ux/4-7-typography/#white-space","text":"Your layout\u2019s \u201cwhite space\u201d \u2013 visual areas unoccupied by text or images \u2013 is essential for offsetting large amounts of text, and providing focus and orientation for users. It should offer separation between different elements of the text layout, such as text and images, and body text and headers.","title":"White Space"},{"location":"ux/4-7-typography/#references","text":"Tubik Blog https://tubikstudio.com/20-wise-thoughts-by-typography-master-erik-spiekermann Smashing Design https://www.smashingmagazine.com/2011/01/guidelines-for-responsive-web-design","title":"References"},{"location":"ux/4-8-color/","text":"4.8 Color About Colors Color theory hypothesizes that humans react to color in visceral ways. The heat we feel from reds, the chill from blues \u2013 these reactions to color are ingrained in us. It gives color the power to spark emotions and be a sort of shorthand for categorical types. When leveraging the tool of color, remember that not everyone sees the colors the way you do and usability trumps beauty. Colors Color is one of the visual designer\u2019s most powerful tools, reinforcing both the brand and user experience. In addition to supporting brand recognition, color is particularly useful for: Creating contrast Grouping elements Encoding additional meaning Communicating interactivity Primary Colors The primary palette should be applied in marketing communications and application design. The darker palette lends sophistication and polish has been designed to give a bold and exciting direction to the brand. Percentage tints can be used in any of these colors. Secondary Colors The secondary palette extends the original colors to support new contexts, such as color-coded categorization, complex data visualization, and interactive color cues. Colors for Accessibility WCAG (Web Content Accessibility Guidelines) ensure that content is accessible by everyone, regardless of disability or user device. To meet the highest standards, text and interactive elements should have a color contrast ratio of at least 4.5:1. The contrast ratio is the comparison of luminance between two adjacent colors, one darker and one lighter. An appropriate ensures that viewers who cannot see the full color spectrum are able to read the text. The options below offer color palette combinations that fall within the range of Section 508 compliant foreground/background color contrast ratios. To ensure that text remains readable and accessible, use only these permitted color combinations. If you choose to customize beyond this palette, https://webaim.org/resources/contrastchecker/ is a useful resource for testing the compliance of any color combination. Colors should be used to denote the type of component or content being displayed. Large blocks of passive, read-only text should be colored to achieve maximum readability, whereas interactive links within that text should be colored with a more eye-catching \u201ccall-to-action\" (CTA) standard. Reds and greens should be used sparingly, as they are often reserved for \u201calert colors,\u201d or stylized system notifications (for when a form field is filled incorrectly, for instance). A color that indicates a disabled state could be the same as one that indicates active, but applied at 50% opacity. While your particular USAF application may adhere to its own style guide, all color palettes should comply with the USAF brand and digital best practices. As a matter of example, the following color palettes have been designed and optimized specifically for USAF experiences. Careful consideration has been made with regards to content, data, accessibility, mobile form factors, and eye fatigue. In addition, hues have been strategically chosen to either compliment or reinforce the existing USAF parent brand palettes. In this way, the following schemes should be seen as an extension of the USAF brand, with additions and minor modifications included to create an optimal digital experience. First consult your application\u2019s style guide, should one exist. References From AF Branding & Trademark Licensing https://www.trademark.af.mil/About-Us/The-Air-Force-Symbol/Display-guidelines From CCE https://company-123432.frontify.com/d/YQiReCF6Sab5/usaf-cce-style-guide#/brand-design/colors","title":"4.8 Color"},{"location":"ux/4-8-color/#48-color","text":"","title":"4.8 Color"},{"location":"ux/4-8-color/#about-colors","text":"Color theory hypothesizes that humans react to color in visceral ways. The heat we feel from reds, the chill from blues \u2013 these reactions to color are ingrained in us. It gives color the power to spark emotions and be a sort of shorthand for categorical types. When leveraging the tool of color, remember that not everyone sees the colors the way you do and usability trumps beauty.","title":"About Colors"},{"location":"ux/4-8-color/#colors","text":"Color is one of the visual designer\u2019s most powerful tools, reinforcing both the brand and user experience. In addition to supporting brand recognition, color is particularly useful for: Creating contrast Grouping elements Encoding additional meaning Communicating interactivity","title":"Colors"},{"location":"ux/4-8-color/#primary-colors","text":"The primary palette should be applied in marketing communications and application design. The darker palette lends sophistication and polish has been designed to give a bold and exciting direction to the brand. Percentage tints can be used in any of these colors.","title":"Primary Colors"},{"location":"ux/4-8-color/#secondary-colors","text":"The secondary palette extends the original colors to support new contexts, such as color-coded categorization, complex data visualization, and interactive color cues.","title":"Secondary Colors"},{"location":"ux/4-8-color/#colors-for-accessibility","text":"WCAG (Web Content Accessibility Guidelines) ensure that content is accessible by everyone, regardless of disability or user device. To meet the highest standards, text and interactive elements should have a color contrast ratio of at least 4.5:1. The contrast ratio is the comparison of luminance between two adjacent colors, one darker and one lighter. An appropriate ensures that viewers who cannot see the full color spectrum are able to read the text. The options below offer color palette combinations that fall within the range of Section 508 compliant foreground/background color contrast ratios. To ensure that text remains readable and accessible, use only these permitted color combinations. If you choose to customize beyond this palette, https://webaim.org/resources/contrastchecker/ is a useful resource for testing the compliance of any color combination. Colors should be used to denote the type of component or content being displayed. Large blocks of passive, read-only text should be colored to achieve maximum readability, whereas interactive links within that text should be colored with a more eye-catching \u201ccall-to-action\" (CTA) standard. Reds and greens should be used sparingly, as they are often reserved for \u201calert colors,\u201d or stylized system notifications (for when a form field is filled incorrectly, for instance). A color that indicates a disabled state could be the same as one that indicates active, but applied at 50% opacity. While your particular USAF application may adhere to its own style guide, all color palettes should comply with the USAF brand and digital best practices. As a matter of example, the following color palettes have been designed and optimized specifically for USAF experiences. Careful consideration has been made with regards to content, data, accessibility, mobile form factors, and eye fatigue. In addition, hues have been strategically chosen to either compliment or reinforce the existing USAF parent brand palettes. In this way, the following schemes should be seen as an extension of the USAF brand, with additions and minor modifications included to create an optimal digital experience. First consult your application\u2019s style guide, should one exist.","title":"Colors for Accessibility"},{"location":"ux/4-8-color/#references","text":"From AF Branding & Trademark Licensing https://www.trademark.af.mil/About-Us/The-Air-Force-Symbol/Display-guidelines From CCE https://company-123432.frontify.com/d/YQiReCF6Sab5/usaf-cce-style-guide#/brand-design/colors","title":"References"},{"location":"ux/4-9-icons/","text":"4.9 Iconography Icons An icon\u2019s first job is orientation. Icons are an essential part of many user interfaces, visually expressing objects, actions and ideas. When done correctly, they communicate the core idea and intent of an element or action, save screen real estate, and enhance aesthetic appeal. Best Practices Icons are highly context-specific, but general best practices include: Use a platform or system standard icon should one exist; rarely should an application design require you to create a new icon Pull your icons from a single icon family / library to simplify the overall composition; these families will have similar line weights, levels of fidelity, repeating elements, etc. Employ icon fonts or open-source font families when possible , use .SVG (scalable vector graphics) when possible for optimal versatility and filesize If you must design custom icons and they can\u2019t be vector / SVG, design first in the largest viewport (desktop), giving your icon the appropriate level of detail. Subsequently, scale down for smaller viewports and remove details in order to keep icons scannable and understandable. Keep icons simple and schematic. Focusing on the basic characteristics of the object. Test them with neutral users for usability, recognizability, and memorability. Icons and Button Labels Icons accompanied by labels make information easier to find and scan, as long as they\u2019re placed in the right spot. Place icons according to the natural reading order. There are two important factors in an icon\u2019s location: In order for icons to serve as a visual scanning aid, users need to see them before they see the accompanying label. Place icons to the left of their labels so that users see them first. Align the icon with the label\u2019s heading, instead of centering it with the heading and body. Seeing the icon first will help users to scan the page more easily. Icons in Data Tables Icons to the left of a number usually indicate the intent of the data, whereas icons to the right usually indicate the quality of the data. As with icons with button labels, the placement of icons should follow the natural reading order. There are two possibilities for icon placement: Status icons would appear at the end of the line. As seen in the example below, the user will see the subject first, then the value associated with the subject and, finally, the status of the value. If the icons themselves are the subject, then they would appear at the start of the line, and everything else would follow thereafter. References Smashing Magazine; perspectives on icons https://www.smashingmagazine.com/2016/10/icons-as-part-of-a-great-user-experience The Noun Project; a comprehensive source of free and attributed icons https://thenounproject.com","title":"4.9 Iconography"},{"location":"ux/4-9-icons/#49-iconography","text":"","title":"4.9 Iconography"},{"location":"ux/4-9-icons/#icons","text":"An icon\u2019s first job is orientation. Icons are an essential part of many user interfaces, visually expressing objects, actions and ideas. When done correctly, they communicate the core idea and intent of an element or action, save screen real estate, and enhance aesthetic appeal.","title":"Icons"},{"location":"ux/4-9-icons/#best-practices","text":"Icons are highly context-specific, but general best practices include: Use a platform or system standard icon should one exist; rarely should an application design require you to create a new icon Pull your icons from a single icon family / library to simplify the overall composition; these families will have similar line weights, levels of fidelity, repeating elements, etc. Employ icon fonts or open-source font families when possible , use .SVG (scalable vector graphics) when possible for optimal versatility and filesize If you must design custom icons and they can\u2019t be vector / SVG, design first in the largest viewport (desktop), giving your icon the appropriate level of detail. Subsequently, scale down for smaller viewports and remove details in order to keep icons scannable and understandable. Keep icons simple and schematic. Focusing on the basic characteristics of the object. Test them with neutral users for usability, recognizability, and memorability.","title":"Best Practices"},{"location":"ux/4-9-icons/#icons-and-button-labels","text":"Icons accompanied by labels make information easier to find and scan, as long as they\u2019re placed in the right spot. Place icons according to the natural reading order. There are two important factors in an icon\u2019s location: In order for icons to serve as a visual scanning aid, users need to see them before they see the accompanying label. Place icons to the left of their labels so that users see them first. Align the icon with the label\u2019s heading, instead of centering it with the heading and body. Seeing the icon first will help users to scan the page more easily.","title":"Icons and Button Labels"},{"location":"ux/4-9-icons/#icons-in-data-tables","text":"Icons to the left of a number usually indicate the intent of the data, whereas icons to the right usually indicate the quality of the data. As with icons with button labels, the placement of icons should follow the natural reading order. There are two possibilities for icon placement: Status icons would appear at the end of the line. As seen in the example below, the user will see the subject first, then the value associated with the subject and, finally, the status of the value. If the icons themselves are the subject, then they would appear at the start of the line, and everything else would follow thereafter.","title":"Icons in Data Tables"},{"location":"ux/4-9-icons/#references","text":"Smashing Magazine; perspectives on icons https://www.smashingmagazine.com/2016/10/icons-as-part-of-a-great-user-experience The Noun Project; a comprehensive source of free and attributed icons https://thenounproject.com","title":"References"},{"location":"ux/5-1-intro/","text":"5.1 Introduction Design is the Little Things Elements. Components. Modules. We often use these words interchangeably, but all of them refer to the pieces of varying complexity that make up a design system. From the single pixel to the entire page, their consistency contributes to both usability and visual appeal. How to use this Component Library As previously noted, this playbook is not a style guide; it is intended to complement and fill gaps in style guides. As such, please: Refer to your application\u2019s style guide for your specific design standards, Work with your design and front-end teams to determine an appropriate pre-existing solution Confirm any relevant standards within the USAF brand or US government guidelines. In the absence of that standards documentation, this library provides general design guidance. Style Specificity Given the differences between specific application style guides, this component library frequently refers to page elements by the purpose they serve or their front-end (HTML) shorthand. So, instead of defining a block of copy by its specific typographic properties, for instance (font name, size, hex color, etc.), we label it as \u201cBody\u201d or \u201cH1 - Headline\u201d in reference to your specific application\u2019s styles. In the case of entirely new components, enterprise-wide updates, or emergent devices / contexts / interaction patterns not accounted for in foundational style guides, this library provides a more prescriptive design: \u201cAlpha Standard.\u201d Again, these styles should only be applied if you\u2019ve exhausted all other standards, and are primarily to demonstrate the best practices outlined in Web Design Standards. Desktop / Tablet / Mobile This component library places an increased focus on mobile (small viewport) use cases. When possible, desktop (and landscape tablet) components are shown with their behavior at the mobile breakpoint. USAF Application Style Guides USAF Branding & Trademark Licensing https://www.trademark.af.mil/About-Us/The-Air-Force-Symbol/Display-guidelines US Web Design System https://designsystem.digital.gov","title":"5.1 Introduction"},{"location":"ux/5-1-intro/#51-introduction","text":"","title":"5.1 Introduction"},{"location":"ux/5-1-intro/#design-is-the-little-things","text":"Elements. Components. Modules. We often use these words interchangeably, but all of them refer to the pieces of varying complexity that make up a design system. From the single pixel to the entire page, their consistency contributes to both usability and visual appeal.","title":"Design is the Little Things"},{"location":"ux/5-1-intro/#how-to-use-this-component-library","text":"As previously noted, this playbook is not a style guide; it is intended to complement and fill gaps in style guides. As such, please: Refer to your application\u2019s style guide for your specific design standards, Work with your design and front-end teams to determine an appropriate pre-existing solution Confirm any relevant standards within the USAF brand or US government guidelines. In the absence of that standards documentation, this library provides general design guidance.","title":"How to use this Component Library"},{"location":"ux/5-1-intro/#style-specificity","text":"Given the differences between specific application style guides, this component library frequently refers to page elements by the purpose they serve or their front-end (HTML) shorthand. So, instead of defining a block of copy by its specific typographic properties, for instance (font name, size, hex color, etc.), we label it as \u201cBody\u201d or \u201cH1 - Headline\u201d in reference to your specific application\u2019s styles. In the case of entirely new components, enterprise-wide updates, or emergent devices / contexts / interaction patterns not accounted for in foundational style guides, this library provides a more prescriptive design: \u201cAlpha Standard.\u201d Again, these styles should only be applied if you\u2019ve exhausted all other standards, and are primarily to demonstrate the best practices outlined in Web Design Standards.","title":"Style Specificity"},{"location":"ux/5-1-intro/#desktop-tablet-mobile","text":"This component library places an increased focus on mobile (small viewport) use cases. When possible, desktop (and landscape tablet) components are shown with their behavior at the mobile breakpoint.","title":"Desktop / Tablet / Mobile"},{"location":"ux/5-1-intro/#references","text":"USAF Branding & Trademark Licensing https://www.trademark.af.mil/About-Us/The-Air-Force-Symbol/Display-guidelines US Web Design System https://designsystem.digital.gov","title":"USAF Application Style Guides"},{"location":"ux/5-2-nav/","text":"5.2 Global Navigation About Global Navigation Any element that, upon interaction, moves a user through an application or site is technically a navigation element. Our focus is on those items that persist regardless of the user\u2019s location within the application: global navigation. The vast majority of global navigation exists at the top of the screen, either within or adjacent to the page header. Some global navigation exists at the left or right side of the screen, and may also behave like a \u201cdrawer\u201d of other user options. Alpha Standard The following example component illustrates the web standards outlined previously, with the practical choices that make it so. Note that the Alpha Standard below is a stylistic proposal only. The inclusion / exclusion of particular navigation items (including utility nav, search, and functional elements) is usually determined by UX designers in accordance with user needs. Task Completion . In content and layout, it first considers the user\u2019s intention. Hierarchy . The component features prioritized items that must be universally accessible, with interactivity and a search component to access everything else. Status . Design elements communicate interactivity, as well as the active state of nav items (I.e. the user\u2019s location). As reflected in the web standards, the following should be considered: a hover state to indicate interactivity, a highlighted state to indicate the currently selected option, a disabled state where applicable to indicate something is disabled. Contrast . The combination of color ratios and the spacing between elements assures scannability and distinction of critical items. Legibility . The combination of color contrast and font characteristics (font type, size, line weight) meet visual accessibility standards. Disclaimer : Please default to your application\u2019s and USAF styles; the following component standards are to be used only if those assets are not applicable or not available. Desktop Navigation Tablet Navigation Phablet Navigation Mobile Navigation References U.S. Web Design System https://designsystem.digital.gov/page-templates/#documentation-page UX Planet uxplanet.org","title":"5.2 Global Navigation"},{"location":"ux/5-2-nav/#52-global-navigation","text":"","title":"5.2 Global Navigation"},{"location":"ux/5-2-nav/#about-global-navigation","text":"Any element that, upon interaction, moves a user through an application or site is technically a navigation element. Our focus is on those items that persist regardless of the user\u2019s location within the application: global navigation. The vast majority of global navigation exists at the top of the screen, either within or adjacent to the page header. Some global navigation exists at the left or right side of the screen, and may also behave like a \u201cdrawer\u201d of other user options.","title":"About Global Navigation"},{"location":"ux/5-2-nav/#alpha-standard","text":"The following example component illustrates the web standards outlined previously, with the practical choices that make it so. Note that the Alpha Standard below is a stylistic proposal only. The inclusion / exclusion of particular navigation items (including utility nav, search, and functional elements) is usually determined by UX designers in accordance with user needs. Task Completion . In content and layout, it first considers the user\u2019s intention. Hierarchy . The component features prioritized items that must be universally accessible, with interactivity and a search component to access everything else. Status . Design elements communicate interactivity, as well as the active state of nav items (I.e. the user\u2019s location). As reflected in the web standards, the following should be considered: a hover state to indicate interactivity, a highlighted state to indicate the currently selected option, a disabled state where applicable to indicate something is disabled. Contrast . The combination of color ratios and the spacing between elements assures scannability and distinction of critical items. Legibility . The combination of color contrast and font characteristics (font type, size, line weight) meet visual accessibility standards. Disclaimer : Please default to your application\u2019s and USAF styles; the following component standards are to be used only if those assets are not applicable or not available.","title":"Alpha Standard"},{"location":"ux/5-2-nav/#desktop-navigation","text":"","title":"Desktop Navigation"},{"location":"ux/5-2-nav/#tablet-navigation","text":"","title":"Tablet Navigation"},{"location":"ux/5-2-nav/#phablet-navigation","text":"","title":"Phablet Navigation"},{"location":"ux/5-2-nav/#mobile-navigation","text":"","title":"Mobile Navigation"},{"location":"ux/5-2-nav/#references","text":"U.S. Web Design System https://designsystem.digital.gov/page-templates/#documentation-page UX Planet uxplanet.org","title":"References"},{"location":"ux/5-3-icons/","text":"5.3 Icons USAF Application Styles The following icons are an aggregate of USAF applications and labeled accordingly. When selecting new icons, choose only from a single \u201cfamily\u201d to retain consistency, or match the characteristics of your pre-existing icon set. An icon family is usually created by the same designer, and each icon will have similar complexity, line weight, repeating elements, and themes. When selecting new icons, choose only from a single family to retain consistency, or match the characteristics of your pre-existing icon set. A good practice is ensuring the icons fit within the same square, having matching line weight / complexity when compared at the same size. Alpha Standard The following example component illustrates the best practices outlined previously, with the practical choices that make it so. Clear meaning . Icon designs favor obvious over clever, communicating one- or two-word concepts in culturally established standards. Simple design . Limited visual complexity improves scannability, plus the ability to scale up and down as contexts require. Standardization . Stylistic similarities between icons (complexity, line weight, repeating elements) improve the coherence of the design system. These styles are made consistent within an icon family. Example Icon Classifications Disclaimer : Please default to USAF application styles; these component standards are to be used only if those assets are not applicable or not available.","title":"5.3 Icons"},{"location":"ux/5-3-icons/#53-icons","text":"","title":"5.3 Icons"},{"location":"ux/5-3-icons/#usaf-application-styles","text":"The following icons are an aggregate of USAF applications and labeled accordingly. When selecting new icons, choose only from a single \u201cfamily\u201d to retain consistency, or match the characteristics of your pre-existing icon set. An icon family is usually created by the same designer, and each icon will have similar complexity, line weight, repeating elements, and themes. When selecting new icons, choose only from a single family to retain consistency, or match the characteristics of your pre-existing icon set. A good practice is ensuring the icons fit within the same square, having matching line weight / complexity when compared at the same size.","title":"USAF Application Styles"},{"location":"ux/5-3-icons/#alpha-standard","text":"The following example component illustrates the best practices outlined previously, with the practical choices that make it so. Clear meaning . Icon designs favor obvious over clever, communicating one- or two-word concepts in culturally established standards. Simple design . Limited visual complexity improves scannability, plus the ability to scale up and down as contexts require. Standardization . Stylistic similarities between icons (complexity, line weight, repeating elements) improve the coherence of the design system. These styles are made consistent within an icon family.","title":"Alpha Standard"},{"location":"ux/5-3-icons/#example-icon-classifications","text":"Disclaimer : Please default to USAF application styles; these component standards are to be used only if those assets are not applicable or not available.","title":"Example Icon Classifications"},{"location":"ux/5-4-buttons/","text":"5.4 Buttons & Labels USAF Application Styles Buttons styles are specific to applications, though button behaviors should follow best practices laid out in Buttons & Controls (4.11). Machine Learning Engines Health Positive Inventory Control Alpha Standard The following example component illustrates the best practices outlined previously, with the practical choices that make it so. Obvious . By placing active text into a containing element (usually a rectangle or rounded rectangle), they follow the design standard of the web and real-world. Clear . Button labels like \u201cSubmit\u201d and \u201cLearn More\u201d communicate the action that the user is taking upon click. Feedback . Interacting with the button changes its style, and clicking it provides other feedback (like success / error messages) to clarity the user experience. Contrast . The spacing and color ratios of the element provide adequate legibility and scannability. Disclaimer : Please default to your application\u2019s and USAF styles; the following component standards are to be used only if those assets are not applicable or not available.","title":"5.4 Buttons & Labels"},{"location":"ux/5-4-buttons/#54-buttons-labels","text":"","title":"5.4 Buttons &amp; Labels"},{"location":"ux/5-4-buttons/#usaf-application-styles","text":"Buttons styles are specific to applications, though button behaviors should follow best practices laid out in Buttons & Controls (4.11).","title":"USAF Application Styles"},{"location":"ux/5-4-buttons/#machine-learning","text":"","title":"Machine Learning"},{"location":"ux/5-4-buttons/#engines-health","text":"","title":"Engines Health"},{"location":"ux/5-4-buttons/#positive-inventory-control","text":"","title":"Positive Inventory Control"},{"location":"ux/5-4-buttons/#alpha-standard","text":"The following example component illustrates the best practices outlined previously, with the practical choices that make it so. Obvious . By placing active text into a containing element (usually a rectangle or rounded rectangle), they follow the design standard of the web and real-world. Clear . Button labels like \u201cSubmit\u201d and \u201cLearn More\u201d communicate the action that the user is taking upon click. Feedback . Interacting with the button changes its style, and clicking it provides other feedback (like success / error messages) to clarity the user experience. Contrast . The spacing and color ratios of the element provide adequate legibility and scannability. Disclaimer : Please default to your application\u2019s and USAF styles; the following component standards are to be used only if those assets are not applicable or not available.","title":"Alpha Standard"},{"location":"ux/5-5-modules/","text":"5.5 Content Modules Designing for Editorial Content When layouts are particularly content-heavy (such as in the case of an article page), they may require components that support copy, images, and other editorial elements. These components are called \u201ccontent modules,\u201d and can be used individually or stacked to create long-form content. Your application will likely feature content modules specific to the style of your platform. For the sake of consistency and efficiency, first try re-purposing an existing design solution. Should one not exist within your style guide or front-end code, the following examples act as a guide for designing new components. Full-Width Content Modules Full-width content modules span the width of the viewport at any breakpoint, split and stack as the layout responds down. Particularly in the mobile viewport, these modules should follow best practices of content reordering (section 4.6) and degradation of individual elements like images, nav elements, and longer copy blocks (section 4.12) . Global Navigation (See section 5.2) Branded Header A header for major content categories (generally those items featured in top-level navigation); generally reserved for more editorial (and less functional) pages. Can support large calls-to-action that drive to elsewhere on the page, or other pages entirely. Inline Introduction / Search Combining both introductory text and a search element, this component is useful for guiding the user through long-form content (articles, FAQs, etc.) via search. Content Block: Short Form Copy A simple inline content block supporting short-form copy, branded images, and a call-to-action. Content Block: Video A simple inline content block supporting quoted text and a video player; the video can play either inline or within a modal. Content Block: Brand Call Out A simple inline content block supporting brand taglines, images, and a call-to-action. Content Block: Headline and Image This variant on Editorial Block A introduces a headline element; useful for linking to articles and major content sections, or employing near the top of the layout in lieu of a branded header. Content Block: Number/Bullet List Another variant on Editorial Block A refactors the copy to support a numbered or bulleted list. Inline links provide additional details without disrupting the sequence of content. Content Block: Image Carousel Appropriate for galleries of five or fewer images, this inline carousel can rotate automatically after a few seconds, or based on user click. Content Block: Data Visualization A chart, table, or graph \u2013 in part or whole \u2013 featuring either inline interactivity or a call-to-action linking to the interactive data. Dependent upon your application, this module may exist within a summary article or on an \u201cdashboard\u201d view. Content Block: Data Callout \u201cBig idea\u201d callouts focus on important, singular data points and support headlines, body copy, and branded images; useful for providing an executive view into data without overwhelming with data visualizations. Global Footer (see section 5.2) Half-Width (2-Up) Content Modules Half-width (or 2-up) content modules split the viewport down the middle, and are useful for shorter-form content, callouts, and links to other pages / sections. They are to be avoided in the mobile viewport, were instead they span full-width. Copy 2-Up This simple copy module supports short-form text and a call-to action; given its visual dominance it is appropriate for content of lower priority. Image 2-Up A simple static element supporting an image, caption / body copy, and call-to-action. Can also support inline animation or video. Data 2-Up So named because of its half-width nature in desktop view, this component supports a data visualization preview and caption, though most often links to a full scale data display.","title":"5.5 Content Modules"},{"location":"ux/5-5-modules/#55-content-modules","text":"","title":"5.5 Content Modules"},{"location":"ux/5-5-modules/#designing-for-editorial-content","text":"When layouts are particularly content-heavy (such as in the case of an article page), they may require components that support copy, images, and other editorial elements. These components are called \u201ccontent modules,\u201d and can be used individually or stacked to create long-form content. Your application will likely feature content modules specific to the style of your platform. For the sake of consistency and efficiency, first try re-purposing an existing design solution. Should one not exist within your style guide or front-end code, the following examples act as a guide for designing new components.","title":"Designing for Editorial Content"},{"location":"ux/5-5-modules/#full-width-content-modules","text":"Full-width content modules span the width of the viewport at any breakpoint, split and stack as the layout responds down. Particularly in the mobile viewport, these modules should follow best practices of content reordering (section 4.6) and degradation of individual elements like images, nav elements, and longer copy blocks (section 4.12) .","title":"Full-Width Content Modules"},{"location":"ux/5-5-modules/#global-navigation-see-section-52","text":"","title":"Global Navigation (See section 5.2)"},{"location":"ux/5-5-modules/#branded-header","text":"A header for major content categories (generally those items featured in top-level navigation); generally reserved for more editorial (and less functional) pages. Can support large calls-to-action that drive to elsewhere on the page, or other pages entirely.","title":"Branded Header"},{"location":"ux/5-5-modules/#inline-introduction-search","text":"Combining both introductory text and a search element, this component is useful for guiding the user through long-form content (articles, FAQs, etc.) via search.","title":"Inline Introduction / Search"},{"location":"ux/5-5-modules/#content-block-short-form-copy","text":"A simple inline content block supporting short-form copy, branded images, and a call-to-action.","title":"Content Block: Short Form Copy"},{"location":"ux/5-5-modules/#content-block-video","text":"A simple inline content block supporting quoted text and a video player; the video can play either inline or within a modal.","title":"Content Block: Video"},{"location":"ux/5-5-modules/#content-block-brand-call-out","text":"A simple inline content block supporting brand taglines, images, and a call-to-action.","title":"Content Block: Brand Call Out"},{"location":"ux/5-5-modules/#content-block-headline-and-image","text":"This variant on Editorial Block A introduces a headline element; useful for linking to articles and major content sections, or employing near the top of the layout in lieu of a branded header.","title":"Content Block: Headline and Image"},{"location":"ux/5-5-modules/#content-block-numberbullet-list","text":"Another variant on Editorial Block A refactors the copy to support a numbered or bulleted list. Inline links provide additional details without disrupting the sequence of content.","title":"Content Block: Number/Bullet List"},{"location":"ux/5-5-modules/#content-block-image-carousel","text":"Appropriate for galleries of five or fewer images, this inline carousel can rotate automatically after a few seconds, or based on user click.","title":"Content Block: Image Carousel"},{"location":"ux/5-5-modules/#content-block-data-visualization","text":"A chart, table, or graph \u2013 in part or whole \u2013 featuring either inline interactivity or a call-to-action linking to the interactive data. Dependent upon your application, this module may exist within a summary article or on an \u201cdashboard\u201d view.","title":"Content Block: Data Visualization"},{"location":"ux/5-5-modules/#content-block-data-callout","text":"\u201cBig idea\u201d callouts focus on important, singular data points and support headlines, body copy, and branded images; useful for providing an executive view into data without overwhelming with data visualizations.","title":"Content Block: Data Callout"},{"location":"ux/5-5-modules/#global-footer-see-section-52","text":"","title":"Global Footer (see section 5.2)"},{"location":"ux/5-5-modules/#half-width-2-up-content-modules","text":"Half-width (or 2-up) content modules split the viewport down the middle, and are useful for shorter-form content, callouts, and links to other pages / sections. They are to be avoided in the mobile viewport, were instead they span full-width.","title":"Half-Width (2-Up) Content Modules"},{"location":"ux/5-5-modules/#copy-2-up","text":"This simple copy module supports short-form text and a call-to action; given its visual dominance it is appropriate for content of lower priority.","title":"Copy 2-Up"},{"location":"ux/5-5-modules/#image-2-up","text":"A simple static element supporting an image, caption / body copy, and call-to-action. Can also support inline animation or video.","title":"Image 2-Up"},{"location":"ux/5-5-modules/#data-2-up","text":"So named because of its half-width nature in desktop view, this component supports a data visualization preview and caption, though most often links to a full scale data display.","title":"Data 2-Up"},{"location":"ux/5-6-alerts/","text":"5.6 Alerts & Messaging USAF Application Styles Alerts & messaging are specific to applications, though their triggers, appearance, and behaviors should follow best practices. Machine Learning Engines Health Management Log Common Operating Picture Positive Inventory Control Alpha Standard The following example component illustrates the best practices outlined previously, with the practical choices that make it so. Clear . Alerts state in clear language, and with obvious color cues that they are positive / negative / neutral, the reason for the alert, and the action required of the user in order to resolve it. Consistent . Alerts follow web or application-standard patterns that are repeating and predictable. If a user were to perform the same triggering action twice, they would receive the same alert twice. Actionable . Alerts & messages offer clear next steps to resolve or address them. When appropriate to the context, alter text offers inline links, or anchor links to the offending component. Disclaimer : Please default to your application\u2019s and USAF styles; the following component standards are to be used only if those assets are not applicable or not available Reference Alerts U.S. Web Design System https://designsystem.digital.gov/components/alerts","title":"5.6 Alerts & Messaging"},{"location":"ux/5-6-alerts/#56-alerts-messaging","text":"","title":"5.6 Alerts &amp; Messaging"},{"location":"ux/5-6-alerts/#usaf-application-styles","text":"Alerts & messaging are specific to applications, though their triggers, appearance, and behaviors should follow best practices.","title":"USAF Application Styles"},{"location":"ux/5-6-alerts/#machine-learning","text":"","title":"Machine Learning"},{"location":"ux/5-6-alerts/#engines-health-management","text":"","title":"Engines Health Management"},{"location":"ux/5-6-alerts/#log-common-operating-picture","text":"","title":"Log Common Operating Picture"},{"location":"ux/5-6-alerts/#positive-inventory-control","text":"","title":"Positive Inventory Control"},{"location":"ux/5-6-alerts/#alpha-standard","text":"The following example component illustrates the best practices outlined previously, with the practical choices that make it so. Clear . Alerts state in clear language, and with obvious color cues that they are positive / negative / neutral, the reason for the alert, and the action required of the user in order to resolve it. Consistent . Alerts follow web or application-standard patterns that are repeating and predictable. If a user were to perform the same triggering action twice, they would receive the same alert twice. Actionable . Alerts & messages offer clear next steps to resolve or address them. When appropriate to the context, alter text offers inline links, or anchor links to the offending component. Disclaimer : Please default to your application\u2019s and USAF styles; the following component standards are to be used only if those assets are not applicable or not available","title":"Alpha Standard"},{"location":"ux/5-6-alerts/#reference","text":"Alerts U.S. Web Design System https://designsystem.digital.gov/components/alerts","title":"Reference"},{"location":"ux/5-7-forms/","text":"5.7 Forms & Controls USAF Application Styles Form controls are specific to applications, though their triggers, appearance, and behaviors should follow best practices ( see Buttons & Controls in section 4.11 ). Note that many form controls are defined by the user\u2019s browser, and thus should be presumed to default to that design. Engines Health Management Positive Inventory Control Alpha Standard The following example component illustrates the best practices outlined previously, with the practical choices that make it so. Looks interactive . Controls are differentiated from \u201cpassive\u201d content by their label, form factor, and/or use of the design system\u2019s call-to-action color. Proximity . Controls are placed logically near the form or element they are intended to affect. For instance, a \u201cSUBMIT\u201d button lives at the bottom of the related text field. Predictable . Context, form label, and control label give the user a clear understanding of what will occur when they\u2019ve interacted with the component. Disclaimer : Please default to your application\u2019s and USAF styles; the following component standards are to be used only if those assets are not applicable or not available","title":"5.7 Forms & Controls"},{"location":"ux/5-7-forms/#57-forms-controls","text":"","title":"5.7 Forms &amp; Controls"},{"location":"ux/5-7-forms/#usaf-application-styles","text":"Form controls are specific to applications, though their triggers, appearance, and behaviors should follow best practices ( see Buttons & Controls in section 4.11 ). Note that many form controls are defined by the user\u2019s browser, and thus should be presumed to default to that design.","title":"USAF Application Styles"},{"location":"ux/5-7-forms/#engines-health-management","text":"","title":"Engines Health Management"},{"location":"ux/5-7-forms/#positive-inventory-control","text":"","title":"Positive Inventory Control"},{"location":"ux/5-7-forms/#alpha-standard","text":"The following example component illustrates the best practices outlined previously, with the practical choices that make it so. Looks interactive . Controls are differentiated from \u201cpassive\u201d content by their label, form factor, and/or use of the design system\u2019s call-to-action color. Proximity . Controls are placed logically near the form or element they are intended to affect. For instance, a \u201cSUBMIT\u201d button lives at the bottom of the related text field. Predictable . Context, form label, and control label give the user a clear understanding of what will occur when they\u2019ve interacted with the component. Disclaimer : Please default to your application\u2019s and USAF styles; the following component standards are to be used only if those assets are not applicable or not available","title":"Alpha Standard"},{"location":"ux/5-8-visualizations/","text":"5.8 Visualizations USAF Application Styles The styles of visualizations are specific to applications, though their appearance and behaviors should follow best practices. In the absence of a style guide, see the bottom of this section for common data visualization types, plus application-agnostic styles. Log Common Operating Picture Positive Inventory Control Machine Learning Alpha Standard Data visualizations should, in their very design, do most of the work of making sense of the data being displayed. Dominant sizes, color cues, and labels should provide the user the orientation and clarity they need to understand data at a glance. Fundamentals of Data Visualization Simplify the data . The intention of a data visualization is to reduce the overall complexity of what is being shared. The visualization should be selected and executed in such a way that does the majority of the mental legwork for the audience. Focus on a single story . The visualization should allow the viewer to clearly understanding the situation: \u201cthings are trending well,\u201d or \u201cthis inventory item is low,\u201d for instance. Sometimes, this story is user-selected, such as when they sort columns by different data characteristcs (status, low-to-high, etc.). Express data accurately . In telling the single story, it can be tempting to artificially increase the size of a data element \u2013 which actually distorts the real picture of it. Try to employ techniques that draw the user\u2019s eye without changing the visual area of the data on display. Data Visualizations As the most common data visualization type in USAF applications, tables warrant particular attention. Even as relatively dense displays, certain visual design choices can aid the user in making sense of the table, and focusing on particular elements. Bold and emphasize key data points and outliers, or highlight these particular cells. This improves the scannability of the table, and draws attention to information that may require action. Use standard color conventions to communicate additional meaning. In financial data, for instance, black numbers indicate a gain, and red numbers indicate a loss. Generally, green indicates a positive trend, and red indicates a negative one. Minimize \u201cchart junk.\u201d Chart junk refers to the design elements that are everything except the data \u2013 separating lines, containing elements, grids, etc. They should be applied as light-handedly as possible, to create an obvious contrast between the subject of the visualization and the container in which it lives. Caption: You can place copy here to add further context to your charts or graphs. References Catalog of Data Visualizations https://datavizcatalogue.com Data Visualization Best Practices https://www.promptcloud.com/blog/design-principles-for-effective-data-visualization","title":"5.8 Visualizations"},{"location":"ux/5-8-visualizations/#58-visualizations","text":"","title":"5.8 Visualizations"},{"location":"ux/5-8-visualizations/#usaf-application-styles","text":"The styles of visualizations are specific to applications, though their appearance and behaviors should follow best practices. In the absence of a style guide, see the bottom of this section for common data visualization types, plus application-agnostic styles.","title":"USAF Application Styles"},{"location":"ux/5-8-visualizations/#log-common-operating-picture","text":"","title":"Log Common Operating Picture"},{"location":"ux/5-8-visualizations/#positive-inventory-control","text":"","title":"Positive Inventory Control"},{"location":"ux/5-8-visualizations/#machine-learning","text":"","title":"Machine Learning"},{"location":"ux/5-8-visualizations/#alpha-standard","text":"Data visualizations should, in their very design, do most of the work of making sense of the data being displayed. Dominant sizes, color cues, and labels should provide the user the orientation and clarity they need to understand data at a glance.","title":"Alpha Standard"},{"location":"ux/5-8-visualizations/#fundamentals-of-data-visualization","text":"Simplify the data . The intention of a data visualization is to reduce the overall complexity of what is being shared. The visualization should be selected and executed in such a way that does the majority of the mental legwork for the audience. Focus on a single story . The visualization should allow the viewer to clearly understanding the situation: \u201cthings are trending well,\u201d or \u201cthis inventory item is low,\u201d for instance. Sometimes, this story is user-selected, such as when they sort columns by different data characteristcs (status, low-to-high, etc.). Express data accurately . In telling the single story, it can be tempting to artificially increase the size of a data element \u2013 which actually distorts the real picture of it. Try to employ techniques that draw the user\u2019s eye without changing the visual area of the data on display.","title":"Fundamentals of Data Visualization"},{"location":"ux/5-8-visualizations/#data-visualizations","text":"As the most common data visualization type in USAF applications, tables warrant particular attention. Even as relatively dense displays, certain visual design choices can aid the user in making sense of the table, and focusing on particular elements. Bold and emphasize key data points and outliers, or highlight these particular cells. This improves the scannability of the table, and draws attention to information that may require action. Use standard color conventions to communicate additional meaning. In financial data, for instance, black numbers indicate a gain, and red numbers indicate a loss. Generally, green indicates a positive trend, and red indicates a negative one. Minimize \u201cchart junk.\u201d Chart junk refers to the design elements that are everything except the data \u2013 separating lines, containing elements, grids, etc. They should be applied as light-handedly as possible, to create an obvious contrast between the subject of the visualization and the container in which it lives. Caption: You can place copy here to add further context to your charts or graphs.","title":"Data Visualizations"},{"location":"ux/5-8-visualizations/#references","text":"Catalog of Data Visualizations https://datavizcatalogue.com Data Visualization Best Practices https://www.promptcloud.com/blog/design-principles-for-effective-data-visualization","title":"References"},{"location":"ux/5-9-mobile/","text":"5.9 Key Mobile Components Mobile Requires Unique UI The small viewport and native operating systems of mobile devices require unique design considerations and, accordingly, unique UI elements. This section outlines the key components that differ from tablet / desktop, and their most common applications. For additional guidance on the theory and practice of designing for mobile, refer to elsewhere in this playbook: Key Mobile Standards and Content Modules . Considerations & Best Practices As a reminder, the core consideration of the mobile context is that the device is in-hand and the user is on-the-go . When translating desktop designs to mobile, don\u2019t just miniaturize it \u2013 consider how the device and context should influence changes. Additionally, mobile designs should Hide and reveal content. Remove unnecessary ornamentation. Embrace scrolling. Be tested vigorously! Navigation One of the most common differences in mobile UI is the navigation, which is most commonly collapsed into a three-line icon nicknamed the \u201chamburger menu.\u201d While the hamburger treatment may wane in popularity, its intent to minimize the navigation into an interactive icon remains an important mobile requirement. Other Mobile-Specific Components Certain mobile-specific elements satisfy the best practices of small viewport design, which other leverage the functionality of the phone itself. Below are common examples. Expand and Collapse More Button Call Button Share Full-screen Components Given the small viewport, it is more common in mobile designs to completely overtake the screen with modals, forms, alerts, and error messages that must be dismissed or completed by the user. These should be used sparingly, but are useful for focusing the user\u2019s attention. Accommodating OS Native Elements When designing components, consider any native OS elements that may be triggered by interacting with the component. For instance, an open text field will trigger the appearance of the native keyboard, potentially obscuring the component and resulting in a frustrating user experience. Alpha Standard The following mobile components illustrate the best practices outlined previously, with the practical choices that make it so. Disclaimer : Please default to USAF application styles; these component standards are to be used only if those assets are not applicable or not available. Closed and Open Hamburger Menu Headers Content Block and Images Media Charts and Graphs Alerts & Messaging Tables Translated from Desktop to Mobile References iOS Design Standards https://developer.apple.com/design/human-interface-guidelines/ios/overview/themes Android Design Standards https://developer.android.com/design/handhelds","title":"5.9 Key Mobile Components"},{"location":"ux/5-9-mobile/#59-key-mobile-components","text":"","title":"5.9 Key Mobile Components"},{"location":"ux/5-9-mobile/#mobile-requires-unique-ui","text":"The small viewport and native operating systems of mobile devices require unique design considerations and, accordingly, unique UI elements. This section outlines the key components that differ from tablet / desktop, and their most common applications. For additional guidance on the theory and practice of designing for mobile, refer to elsewhere in this playbook: Key Mobile Standards and Content Modules .","title":"Mobile Requires Unique UI"},{"location":"ux/5-9-mobile/#considerations-best-practices","text":"As a reminder, the core consideration of the mobile context is that the device is in-hand and the user is on-the-go . When translating desktop designs to mobile, don\u2019t just miniaturize it \u2013 consider how the device and context should influence changes. Additionally, mobile designs should Hide and reveal content. Remove unnecessary ornamentation. Embrace scrolling. Be tested vigorously!","title":"Considerations &amp; Best Practices"},{"location":"ux/5-9-mobile/#navigation","text":"One of the most common differences in mobile UI is the navigation, which is most commonly collapsed into a three-line icon nicknamed the \u201chamburger menu.\u201d While the hamburger treatment may wane in popularity, its intent to minimize the navigation into an interactive icon remains an important mobile requirement.","title":"Navigation"},{"location":"ux/5-9-mobile/#other-mobile-specific-components","text":"Certain mobile-specific elements satisfy the best practices of small viewport design, which other leverage the functionality of the phone itself. Below are common examples.","title":"Other Mobile-Specific Components"},{"location":"ux/5-9-mobile/#expand-and-collapse","text":"","title":"Expand and Collapse"},{"location":"ux/5-9-mobile/#more-button","text":"","title":"More Button"},{"location":"ux/5-9-mobile/#call-button","text":"","title":"Call Button"},{"location":"ux/5-9-mobile/#share","text":"","title":"Share"},{"location":"ux/5-9-mobile/#full-screen-components","text":"Given the small viewport, it is more common in mobile designs to completely overtake the screen with modals, forms, alerts, and error messages that must be dismissed or completed by the user. These should be used sparingly, but are useful for focusing the user\u2019s attention.","title":"Full-screen Components"},{"location":"ux/5-9-mobile/#accommodating-os-native-elements","text":"When designing components, consider any native OS elements that may be triggered by interacting with the component. For instance, an open text field will trigger the appearance of the native keyboard, potentially obscuring the component and resulting in a frustrating user experience.","title":"Accommodating OS Native Elements"},{"location":"ux/5-9-mobile/#alpha-standard","text":"The following mobile components illustrate the best practices outlined previously, with the practical choices that make it so. Disclaimer : Please default to USAF application styles; these component standards are to be used only if those assets are not applicable or not available.","title":"Alpha Standard"},{"location":"ux/5-9-mobile/#closed-and-open-hamburger-menu","text":"","title":"Closed and Open Hamburger Menu"},{"location":"ux/5-9-mobile/#headers","text":"","title":"Headers"},{"location":"ux/5-9-mobile/#content-block-and-images","text":"","title":"Content Block and Images"},{"location":"ux/5-9-mobile/#media","text":"","title":"Media"},{"location":"ux/5-9-mobile/#charts-and-graphs","text":"","title":"Charts and Graphs"},{"location":"ux/5-9-mobile/#alerts-messaging","text":"","title":"Alerts &amp; Messaging"},{"location":"ux/5-9-mobile/#tables-translated-from-desktop-to-mobile","text":"","title":"Tables Translated from Desktop to Mobile"},{"location":"ux/5-9-mobile/#references","text":"iOS Design Standards https://developer.apple.com/design/human-interface-guidelines/ios/overview/themes Android Design Standards https://developer.android.com/design/handhelds","title":"References"},{"location":"ux/6-1-appendix/","text":"6.1 Additional Resources The following is a list of additional excellent sources for learning more about user experience. Online resources usability.gov Nielsen Norman Group The Interaction Design Foundation UXRESOURCES.DESIGN Inside Design by InVision Courses Interaction Design Specialization \u2013 Coursera User Experience (UX): The Ultimate Guide to Usability and UX \u2013 Udemy Books The Elements of User Experience: User-Centered Design for the Web and Beyond About Face: The Essentials of Interaction Design Information Architecture: For the Web and Beyond Don\u2019t Make Me Think Rocket Surgery Made Easy UX for Beginners: A Crash Course in 100 Short Lessons Observing the User Experience: A Practitioner's Guide to User Research Communicating Design: Developing Web Site Documentation for Design and Planning The User Experience Team of One: A Research and Design Survival Guide","title":"6.1 Additional Resources"},{"location":"ux/6-1-appendix/#61-additional-resources","text":"The following is a list of additional excellent sources for learning more about user experience.","title":"6.1 Additional Resources"},{"location":"ux/6-1-appendix/#online-resources","text":"usability.gov Nielsen Norman Group The Interaction Design Foundation UXRESOURCES.DESIGN Inside Design by InVision","title":"Online resources"},{"location":"ux/6-1-appendix/#courses","text":"Interaction Design Specialization \u2013 Coursera User Experience (UX): The Ultimate Guide to Usability and UX \u2013 Udemy","title":"Courses"},{"location":"ux/6-1-appendix/#books","text":"The Elements of User Experience: User-Centered Design for the Web and Beyond About Face: The Essentials of Interaction Design Information Architecture: For the Web and Beyond Don\u2019t Make Me Think Rocket Surgery Made Easy UX for Beginners: A Crash Course in 100 Short Lessons Observing the User Experience: A Practitioner's Guide to User Research Communicating Design: Developing Web Site Documentation for Design and Planning The User Experience Team of One: A Research and Design Survival Guide","title":"Books"},{"location":"ux/6-2-layouts/","text":"6.2 Additional Standard Layouts Bringing Components Together The following examples show full-page desktop and mobile layouts of the Alpha Standard design system. This is intended to further demonstrate design best practices, as well as provide a tangible example of the stacking relationship of responsive components. As with all examples in this playbook, please first refer to your specific application\u2019s style guide","title":"6.2 Alpha Standard Layouts"},{"location":"ux/6-2-layouts/#62-additional-standard-layouts","text":"","title":"6.2 Additional Standard Layouts"},{"location":"ux/6-2-layouts/#bringing-components-together","text":"The following examples show full-page desktop and mobile layouts of the Alpha Standard design system. This is intended to further demonstrate design best practices, as well as provide a tangible example of the stacking relationship of responsive components. As with all examples in this playbook, please first refer to your specific application\u2019s style guide","title":"Bringing Components Together"}]}